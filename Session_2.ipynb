{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Session 2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahmadhajmosa/Machine-learning-labs/blob/probst/Session_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmSUExzC-ZBV",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgWZldyi-Zef",
        "colab_type": "text"
      },
      "source": [
        "# Lab on Machine Learning and Applications in Intelligent Vehicles\n",
        "## Session 1: Introduction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a75NLOuwKXSs",
        "colab_type": "text"
      },
      "source": [
        "#Session 2: 05.06 - 13:00 - 14:30 :\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciUtGM-W_ehP",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "## Intro:\n",
        "\n",
        "Tensorflow is a powerful framework for implementing and deploying large-scale deep learning models. Recently, it has been widely used in both reasearch and production. TF objective is to combine scale and flexibility.\n",
        "\n",
        "In the past session, we will learning the following:\n",
        "\n",
        "1. TF programming stack\n",
        "2. TF programming concepts including computatoin graphs, operations and sessions. \n",
        "3. Implementation of linear regression\n",
        "4. Implementation of feed-forward neural networks\n",
        "\n",
        "## TF stack:\n",
        "\n",
        "TensorFlow is a framework composed of two core building blocks — a library for defining computational graphs and a runtime for executing such graphs on a variety of different hardware\n",
        "\n",
        "\n",
        "![alt text](https://www.tensorflow.org/images/layers.png)\n",
        "\n",
        "\n",
        "Before goining into details about the stack, let us talk about computational graphs.\n",
        "\n",
        "### Computational Graphs\n",
        "\n",
        "A directed graph is a data structure consisting of nodes (vertices) and edges. It’s a set of vertices connected pairwise by directed edges.\n",
        "\n",
        "Graphs come in many shapes and sizes and are used to solve many real-life problems, such as representing networks including telephone networks, circuit networks, road networks, and even social networks. \n",
        "![alt text](https://cdn-images-1.medium.com/max/800/1*V6aYjD3AxDbEKYahkGqVQw.png)\n",
        "\n",
        "TensorFlow uses directed graphs internally to represent computations, and they call this data flow graphs (or computational graphs).\n",
        "\n",
        "The nodes in TF data flow graph mostly represents operations, variables and placeholders.\n",
        "\n",
        "Take for example the following operation:\n",
        "![alt text](https://cdn-images-1.medium.com/max/800/1*6E3sfit6DCeJ9mOz17g4bA.png)\n",
        "\n",
        "To create a computational graph out of this program, we create nodes for each of the operations in our program, along with the input variables a and b. In fact, a and b could be constants if they don’t change. If one node is used as the input to another operation we draw a directed arrow that goes from one node to another.\n",
        "\n",
        "The computational graph for this program might look like this:\n",
        "![alt text](https://cdn-images-1.medium.com/max/800/1*vPb9E0Yd1QUAD0oFmAgaOw.png)\n",
        "\n",
        "Operations create or manipulate data according to specific rules. In TensorFlow those rules are called Ops, short for operations. Variables on the other hand represent shared, persistent state that can be manipulated by running Ops on those variables.\n",
        "\n",
        "The questions now what are the advantages of representing operations as directed graphs: The main advantage of using directed graphs is the ability to do **parallelism** and what is called **dependency driving scheduling**. \n",
        "For example, consider again the follwoing code:\n",
        "![alt text](https://cdn-images-1.medium.com/max/800/1*6E3sfit6DCeJ9mOz17g4bA.png)\n",
        "At the most fundamental level, most computer programs are mainly composed of two things — primitive operations and an order in which these operations are executed, often sequentially, line by line. This means we would first multiply a and b and only when this expression was evaluated we would take their sum. Computational graphs on the otherhand, exclusively specify the dependencies across the operations.\n",
        "If we look at our computational graph we see that we could execute the multiplication and addition in parallel. That’s because these two operations do not depend on each other.\n",
        " So we can use the topology of the graph to drive the scheduling of operations and execute them in the most efficient manner, e.g. using multiple GPUs on a single machine or even distribute the execution across multiple machines.\n",
        " Another key advantage is portability. The graph is a language-independent representation of our code. So we can build the graph in Python, save the model (TensorFlow uses protocol buffers), and restore the model in a different language, say C++, if you want to go really fast.\n",
        " \n",
        " \n",
        "\n",
        "--------------------------------\n",
        "# References:\n",
        "\n",
        "https://medium.com/@d3lm/understand-tensorflow-by-mimicking-its-api-from-scratch-faa55787170d\n",
        "\n",
        "https://www.tensorflow.org/guide/extend/architecture\n",
        "\n",
        "https://www.tensorflow.org/guide/low_level_intro\n",
        "\n",
        "  \n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-GFJPVDnEwx",
        "colab_type": "text"
      },
      "source": [
        "# placeholder: tensors are feeded externaly for example inputs tensors + output tensors\n",
        "\n",
        "# variables : tensors represent the parameters of the network/graph ie. nn weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlmSCbhtoJBs",
        "colab_type": "code",
        "outputId": "02947132-347b-4386-b498-48a2c7444518",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.001\n",
        "training_iters = 2000\n",
        "batch_size = 128\n",
        "\n",
        "# Network Parameters\n",
        "\n",
        "num_inputs = 3\n",
        "num_outputs = 4\n",
        "num_samples= 10\n",
        "# Training data\n",
        "x_gr = np.random.rand(num_samples,num_inputs)\n",
        "y_gr = np.random.rand(num_samples,num_outputs)\n",
        "\n",
        "\n",
        "# tf Graph input\n",
        "x = tf.placeholder(tf.float32, [None, num_inputs])\n",
        "y = tf.placeholder(tf.float32, [None, num_outputs])\n",
        "\n",
        "\n",
        "# weights \n",
        "w_1 = tf.Variable(tf.random_normal([num_inputs,num_outputs ]))\n",
        "\n",
        "# model\n",
        "y_p = tf.matmul(x, w_1)\n",
        "\n",
        "\n",
        "# cost\n",
        "\n",
        "cost = tf.reduce_mean(tf.pow(y-y_p,2)) # \n",
        "\n",
        "# optimisation \n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# initalizing the graph and the weights\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Launch the graph\n",
        "with tf.Session() as sess:\n",
        "    \n",
        "    sess.run(init)\n",
        "    \n",
        "    for i in range(10):\n",
        "    \n",
        "        sess.run(optimizer, feed_dict={x: x_gr, y: y_gr}) \n",
        "\n",
        "        pr_cost = sess.run(cost, feed_dict={x: x_gr,y: y_gr})\n",
        "    \n",
        "        print('iter: ',i, 'cost: ', pr_cost)\n",
        "    \n",
        "    y_p_p = sess.run(y_p, feed_dict={x: x_gr, y: y_gr})\n",
        "    \n",
        "    print('predicted ', y_p_p)\n",
        "    print('real ', y_gr)\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter:  0 cost:  1.123856\n",
            "iter:  1 cost:  1.1211833\n",
            "iter:  2 cost:  1.1185161\n",
            "iter:  3 cost:  1.1158546\n",
            "iter:  4 cost:  1.1131986\n",
            "iter:  5 cost:  1.1105487\n",
            "iter:  6 cost:  1.1079047\n",
            "iter:  7 cost:  1.1052667\n",
            "iter:  8 cost:  1.1026348\n",
            "iter:  9 cost:  1.1000088\n",
            "predicted  [[ 0.9050338   0.1602141   0.92013043  0.35228956]\n",
            " [ 1.0046937  -0.46938652  0.9352531  -0.28297973]\n",
            " [-0.03045141  0.7378717   0.94228226  0.84887326]\n",
            " [ 1.6736385  -0.07725339  1.8665017   0.28374815]\n",
            " [ 1.9989486  -1.0733619   1.1903814  -0.7728812 ]\n",
            " [ 0.5502916   0.84981495  1.4021975   1.0713874 ]\n",
            " [ 2.1071057  -1.4316987   0.89470595 -1.1591331 ]\n",
            " [ 2.2422628   0.18256232  1.5306542   0.5781349 ]\n",
            " [ 0.5619546  -0.7266482   0.43879023 -0.6438483 ]\n",
            " [ 1.8578552  -0.09937002  2.063054    0.30010092]]\n",
            "real  [[0.21474705 0.47051972 0.67244097 0.54962198]\n",
            " [0.79322732 0.24853241 0.53264926 0.04893323]\n",
            " [0.98697024 0.00239486 0.35794472 0.84164742]\n",
            " [0.58980382 0.80074009 0.36212481 0.53885776]\n",
            " [0.5726852  0.80401436 0.56580598 0.43837744]\n",
            " [0.24685803 0.91090296 0.35907364 0.37994194]\n",
            " [0.31951455 0.99643121 0.17688305 0.16410542]\n",
            " [0.05786095 0.9075506  0.11253557 0.06750927]\n",
            " [0.123738   0.41311002 0.40737943 0.79637428]\n",
            " [0.46130208 0.35435602 0.60221288 0.96520144]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fv2aqi3Fu-AJ",
        "colab_type": "code",
        "outputId": "8d9b4eea-a9f3-4a68-ea45-b8c1c7f7925b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        }
      },
      "source": [
        "sess = tf.Session() \n",
        "sess.run(init)\n",
        "    \n",
        "for i in range(10):\n",
        "    \n",
        "        sess.run(optimizer, feed_dict={x: x_gr, y: y_gr}) \n",
        "\n",
        "        pr_cost = sess.run(cost, feed_dict={x: x_gr,y: y_gr})\n",
        "    \n",
        "        print('iter: ',i, 'cost: ', pr_cost)\n",
        "    \n",
        "y_p_p = sess.run(y_p, feed_dict={x: x_gr, y: y_gr})\n",
        "    \n",
        "print('predicted ', y_p_p)\n",
        "print('real ', y_gr)\n",
        "\n",
        "#sess.close()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter:  0 cost:  0.98726064\n",
            "iter:  1 cost:  0.9855059\n",
            "iter:  2 cost:  0.98375356\n",
            "iter:  3 cost:  0.9820038\n",
            "iter:  4 cost:  0.9802567\n",
            "iter:  5 cost:  0.97851217\n",
            "iter:  6 cost:  0.9767703\n",
            "iter:  7 cost:  0.9750311\n",
            "iter:  8 cost:  0.97329473\n",
            "iter:  9 cost:  0.9715611\n",
            "predicted  [[ 0.26064053 -0.25670126 -0.5003677  -0.5513109 ]\n",
            " [ 0.2602894   0.6473293  -0.44281128  0.48309955]\n",
            " [-0.37220675 -0.5429424  -0.1174845  -1.1071638 ]\n",
            " [ 0.36778173  0.19079013 -0.86150897 -0.35512745]\n",
            " [ 0.8074629   1.1018417  -0.8530548   1.0988331 ]\n",
            " [-0.1447592  -0.79902995 -0.43940505 -1.4962587 ]\n",
            " [ 0.9829569   1.4039248  -0.84513634  1.6180394 ]\n",
            " [ 0.963123   -0.7549219  -1.1982899  -1.1118507 ]\n",
            " [ 0.13497426  1.0172094  -0.16676992  1.0520728 ]\n",
            " [ 0.41086552  0.22719614 -0.9539254  -0.37225053]]\n",
            "real  [[0.21474705 0.47051972 0.67244097 0.54962198]\n",
            " [0.79322732 0.24853241 0.53264926 0.04893323]\n",
            " [0.98697024 0.00239486 0.35794472 0.84164742]\n",
            " [0.58980382 0.80074009 0.36212481 0.53885776]\n",
            " [0.5726852  0.80401436 0.56580598 0.43837744]\n",
            " [0.24685803 0.91090296 0.35907364 0.37994194]\n",
            " [0.31951455 0.99643121 0.17688305 0.16410542]\n",
            " [0.05786095 0.9075506  0.11253557 0.06750927]\n",
            " [0.123738   0.41311002 0.40737943 0.79637428]\n",
            " [0.46130208 0.35435602 0.60221288 0.96520144]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUPSS03avw5D",
        "colab_type": "code",
        "outputId": "e070da16-7ab6-4fc2-f0fb-81cd7eaa23f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.001\n",
        "training_iters = 2000\n",
        "batch_size = 128\n",
        "\n",
        "# Network Parameters\n",
        "\n",
        "num_inputs = 3\n",
        "num_h1_n = 4\n",
        "num_h2_n = 10\n",
        "num_outputs = 4\n",
        "\n",
        "num_samples= 10\n",
        "\n",
        "# Training data\n",
        "x_gr = np.random.rand(num_samples,num_inputs)\n",
        "y_gr = np.random.rand(num_samples,num_outputs)\n",
        "\n",
        "\n",
        "# tf Graph input\n",
        "x = tf.placeholder(tf.float32, [None, num_inputs])\n",
        "y = tf.placeholder(tf.float32, [None, num_outputs])\n",
        "\n",
        "\n",
        "# weights \n",
        "w_1 = tf.Variable(tf.random_normal([num_inputs,num_h1_n ]))\n",
        "w_2 = tf.Variable(tf.random_normal([num_h1_n,num_h2_n ]))\n",
        "w_3 = tf.Variable(tf.random_normal([num_h2_n,num_outputs ]))\n",
        "\n",
        "# bias \n",
        "b_1 = tf.Variable(tf.random_normal([num_h1_n]))\n",
        "b_2 = tf.Variable(tf.random_normal([num_h2_n]))\n",
        "b_3 = tf.Variable(tf.random_normal([num_outputs]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# model\n",
        "\n",
        "h1 = tf.nn.sigmoid(tf.add(tf.matmul(x, w_1),b_1)) # model of hidden layer 1\n",
        "h2 = tf.nn.sigmoid(tf.add(tf.matmul(h1, w_2),b_2)) # model of hidden layer 2\n",
        "y_p = tf.add(tf.matmul(h2, w_3),b_3) # model of the output layer\n",
        "\n",
        "\n",
        "\n",
        "# cost\n",
        "\n",
        "cost = tf.reduce_mean(tf.pow(y-y_p,2)) # \n",
        "\n",
        "# optimisation \n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# initalizing the graph and the weights\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Launch the graph\n",
        "with tf.Session() as sess:\n",
        "    \n",
        "    sess.run(init)\n",
        "    \n",
        "    for i in range(10):\n",
        "    \n",
        "        sess.run(optimizer, feed_dict={x: x_gr, y: y_gr}) \n",
        "\n",
        "        pr_cost = sess.run(cost, feed_dict={x: x_gr,y: y_gr})\n",
        "    \n",
        "        print('iter: ',i, 'cost: ', pr_cost)\n",
        "    \n",
        "    y_p_p = sess.run(y_p, feed_dict={x: x_gr, y: y_gr})\n",
        "    \n",
        "    print('predicted ', y_p_p)\n",
        "    print('real ', y_gr)\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter:  0 cost:  9.055705\n",
            "iter:  1 cost:  9.014727\n",
            "iter:  2 cost:  8.9738865\n",
            "iter:  3 cost:  8.93319\n",
            "iter:  4 cost:  8.892634\n",
            "iter:  5 cost:  8.852217\n",
            "iter:  6 cost:  8.811934\n",
            "iter:  7 cost:  8.771787\n",
            "iter:  8 cost:  8.731775\n",
            "iter:  9 cost:  8.691897\n",
            "predicted  [[-1.2477537   5.9756837   0.6207233   1.423921  ]\n",
            " [-1.4750469   5.6989446   0.5577158   1.2042632 ]\n",
            " [-1.4107713   5.7938414   0.5821939   1.2413826 ]\n",
            " [-1.4418812   5.7458324   0.56632453  1.2346345 ]\n",
            " [-1.2363346   5.9920564   0.65204597  1.4277827 ]\n",
            " [-1.227309    5.9981165   0.6149899   1.4424824 ]\n",
            " [-1.0931115   6.1276073   0.6602364   1.6102186 ]\n",
            " [-1.4714556   5.741755    0.5315476   1.1362072 ]\n",
            " [-1.2333767   6.011944    0.6158439   1.3924607 ]\n",
            " [-1.3408425   5.8636127   0.61586833  1.347872  ]]\n",
            "real  [[0.32463124 0.73313092 0.47488059 0.30511154]\n",
            " [0.7638626  0.63017407 0.09125469 0.53216256]\n",
            " [0.26815479 0.06324862 0.71566913 0.62970509]\n",
            " [0.8064397  0.15393563 0.48375146 0.74241536]\n",
            " [0.80525852 0.38296255 0.97285628 0.65525673]\n",
            " [0.17314031 0.18034827 0.33841957 0.10681907]\n",
            " [0.83897411 0.57246599 0.93045043 0.32084221]\n",
            " [0.55663316 0.55168506 0.48916313 0.84250453]\n",
            " [0.68276726 0.72478758 0.5224834  0.9933377 ]\n",
            " [0.74340984 0.06693573 0.89536741 0.26781805]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwhCMk9VyVj-",
        "colab_type": "code",
        "outputId": "03632301-88af-40a5-a1e2-2383173edfa2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
            "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaeRp0T10834",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 18712
        },
        "outputId": "49f10c8e-abac-403f-ef6d-62f96d164665"
      },
      "source": [
        "# training data\n",
        "X_train = mnist.train.images\n",
        "Y_train = mnist.train.labels\n",
        "\n",
        "# training data\n",
        "X_test = mnist.test.images\n",
        "Y_test = mnist.test.labels\n",
        "\n",
        "# training data\n",
        "X_val = mnist.validation.images\n",
        "Y_val = mnist.validation.labels\n",
        "\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.001\n",
        "training_iters = 2000\n",
        "batch_size = 128\n",
        "\n",
        "# Network Parameters\n",
        "\n",
        "num_inputs = 784\n",
        "num_h1_n = 100\n",
        "num_h2_n = 100\n",
        "num_outputs = 10\n",
        "\n",
        "\n",
        "\n",
        "# tf Graph input\n",
        "x = tf.placeholder(tf.float32, [None, num_inputs])\n",
        "y = tf.placeholder(tf.float32, [None, num_outputs])\n",
        "\n",
        "\n",
        "# weights \n",
        "w_1 = tf.Variable(tf.random_normal([num_inputs,num_h1_n ]))\n",
        "w_2 = tf.Variable(tf.random_normal([num_h1_n,num_h2_n ]))\n",
        "w_3 = tf.Variable(tf.random_normal([num_h2_n,num_outputs ]))\n",
        "\n",
        "# bias \n",
        "b_1 = tf.Variable(tf.random_normal([num_h1_n]))\n",
        "b_2 = tf.Variable(tf.random_normal([num_h2_n]))\n",
        "b_3 = tf.Variable(tf.random_normal([num_outputs]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# model\n",
        "\n",
        "h1 = tf.nn.sigmoid(tf.add(tf.matmul(x, w_1),b_1)) # model of hidden layer 1\n",
        "h2 = tf.nn.sigmoid(tf.add(tf.matmul(h1, w_2),b_2)) # model of hidden layer 2\n",
        "y_p = tf.add(tf.matmul(h2, w_3),b_3) # model of the output layer\n",
        "\n",
        "\n",
        "\n",
        "# cost\n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_p, labels=y)) # cross entropy cost\n",
        "\n",
        "\n",
        "# Evaluate model\n",
        "correct_pred = tf.equal(tf.argmax(y_p, 1), tf.argmax(y, 1))\n",
        "\n",
        "## 3 images, y_p=[[0.1,0.0,0,0.9],[0.9,0.1,0,0.],[0,0.9,0,0.1]] \n",
        "\n",
        "# tf.argmax(y_p, 1) [3,0,1] \n",
        "\n",
        "# 3 images, y=[[0,0.0,0,1],[0,1,0,0],[0,1,0,0]] \n",
        "\n",
        "# tf.argmax(y, 1) [3,1,1]\n",
        "\n",
        "# tf.equal [True,False,True]--[1,0,1]--- 2/3 \n",
        "\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "#\n",
        "\n",
        "# optimisation \n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# initalizing the graph and the weights\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Launch the graph\n",
        "with tf.Session() as sess:\n",
        "    \n",
        "    sess.run(init)\n",
        "    \n",
        "    for i in range(1000):\n",
        "        \n",
        "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
        "        \n",
        "        # Run optimization op (backprop)\n",
        "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
        "    \n",
        "\n",
        "        train_cost, train_acc  = sess.run([cost,accuracy], feed_dict={x: batch_x,y: batch_y})\n",
        "    \n",
        "        \n",
        "        test_batch_x, test_batch_y = mnist.test.next_batch(batch_size)\n",
        "\n",
        "        test_cost, test_acc  = sess.run([cost,accuracy], feed_dict={x: test_batch_x,y: test_batch_y})\n",
        "        print('iter: ',i, 'train_cost: ', train_cost, 'train_acc: ', train_acc,'test_cost: ', test_cost, 'test_acc: ', test_acc )\n",
        "\n",
        "    \n",
        "    #y_p_p = sess.run(y_p, feed_dict={x: x_gr, y: y_gr})\n",
        "    \n",
        "    print('predicted ', y_p_p)\n",
        "    print('real ', y_gr)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-16-0e7a1690510e>:55: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "iter:  0 train_cost:  11.325241 train_acc:  0.0625 test_cost:  11.599855 test_acc:  0.0390625\n",
            "iter:  1 train_cost:  11.090834 train_acc:  0.09375 test_cost:  10.510571 test_acc:  0.078125\n",
            "iter:  2 train_cost:  10.87133 train_acc:  0.078125 test_cost:  10.276623 test_acc:  0.078125\n",
            "iter:  3 train_cost:  10.766141 train_acc:  0.0390625 test_cost:  9.33554 test_acc:  0.078125\n",
            "iter:  4 train_cost:  9.203386 train_acc:  0.03125 test_cost:  10.173584 test_acc:  0.0859375\n",
            "iter:  5 train_cost:  9.306101 train_acc:  0.078125 test_cost:  9.957951 test_acc:  0.078125\n",
            "iter:  6 train_cost:  9.559373 train_acc:  0.0625 test_cost:  9.849054 test_acc:  0.0625\n",
            "iter:  7 train_cost:  9.270992 train_acc:  0.0859375 test_cost:  9.251337 test_acc:  0.0703125\n",
            "iter:  8 train_cost:  9.532303 train_acc:  0.078125 test_cost:  9.165961 test_acc:  0.0859375\n",
            "iter:  9 train_cost:  8.422884 train_acc:  0.1171875 test_cost:  9.479416 test_acc:  0.046875\n",
            "iter:  10 train_cost:  8.475318 train_acc:  0.1015625 test_cost:  8.4319315 test_acc:  0.1015625\n",
            "iter:  11 train_cost:  8.611632 train_acc:  0.0859375 test_cost:  7.693922 test_acc:  0.1171875\n",
            "iter:  12 train_cost:  9.197084 train_acc:  0.0546875 test_cost:  8.442195 test_acc:  0.0703125\n",
            "iter:  13 train_cost:  8.020605 train_acc:  0.1328125 test_cost:  8.193332 test_acc:  0.1015625\n",
            "iter:  14 train_cost:  7.7950854 train_acc:  0.109375 test_cost:  7.83311 test_acc:  0.1171875\n",
            "iter:  15 train_cost:  7.9716496 train_acc:  0.0859375 test_cost:  7.7261415 test_acc:  0.0625\n",
            "iter:  16 train_cost:  7.815846 train_acc:  0.1015625 test_cost:  6.96389 test_acc:  0.1328125\n",
            "iter:  17 train_cost:  7.302273 train_acc:  0.140625 test_cost:  6.522566 test_acc:  0.140625\n",
            "iter:  18 train_cost:  7.684971 train_acc:  0.1171875 test_cost:  6.9208884 test_acc:  0.09375\n",
            "iter:  19 train_cost:  6.6455 train_acc:  0.1328125 test_cost:  7.0764017 test_acc:  0.1015625\n",
            "iter:  20 train_cost:  6.9969654 train_acc:  0.1484375 test_cost:  7.6884937 test_acc:  0.1015625\n",
            "iter:  21 train_cost:  6.8994026 train_acc:  0.125 test_cost:  7.1214094 test_acc:  0.1015625\n",
            "iter:  22 train_cost:  6.240708 train_acc:  0.09375 test_cost:  7.4716983 test_acc:  0.1015625\n",
            "iter:  23 train_cost:  6.7492085 train_acc:  0.109375 test_cost:  6.6301537 test_acc:  0.140625\n",
            "iter:  24 train_cost:  6.4928827 train_acc:  0.1015625 test_cost:  6.179052 test_acc:  0.171875\n",
            "iter:  25 train_cost:  5.912449 train_acc:  0.09375 test_cost:  5.584438 test_acc:  0.15625\n",
            "iter:  26 train_cost:  6.128538 train_acc:  0.1484375 test_cost:  5.9884167 test_acc:  0.1171875\n",
            "iter:  27 train_cost:  6.0038033 train_acc:  0.140625 test_cost:  5.603956 test_acc:  0.15625\n",
            "iter:  28 train_cost:  5.704858 train_acc:  0.171875 test_cost:  6.508305 test_acc:  0.0859375\n",
            "iter:  29 train_cost:  5.61732 train_acc:  0.1875 test_cost:  5.769058 test_acc:  0.1484375\n",
            "iter:  30 train_cost:  5.690478 train_acc:  0.109375 test_cost:  5.8272524 test_acc:  0.1640625\n",
            "iter:  31 train_cost:  4.7788467 train_acc:  0.171875 test_cost:  6.043307 test_acc:  0.140625\n",
            "iter:  32 train_cost:  5.974927 train_acc:  0.1015625 test_cost:  5.199155 test_acc:  0.1484375\n",
            "iter:  33 train_cost:  4.7057657 train_acc:  0.203125 test_cost:  5.6572256 test_acc:  0.1875\n",
            "iter:  34 train_cost:  4.8987617 train_acc:  0.1484375 test_cost:  5.1837435 test_acc:  0.125\n",
            "iter:  35 train_cost:  5.2107887 train_acc:  0.171875 test_cost:  5.2728596 test_acc:  0.140625\n",
            "iter:  36 train_cost:  4.5804844 train_acc:  0.15625 test_cost:  4.8676987 test_acc:  0.171875\n",
            "iter:  37 train_cost:  5.350156 train_acc:  0.1796875 test_cost:  4.841822 test_acc:  0.1640625\n",
            "iter:  38 train_cost:  4.5590997 train_acc:  0.15625 test_cost:  4.6989384 test_acc:  0.1796875\n",
            "iter:  39 train_cost:  4.790849 train_acc:  0.140625 test_cost:  5.031988 test_acc:  0.1171875\n",
            "iter:  40 train_cost:  3.962636 train_acc:  0.1875 test_cost:  4.5019965 test_acc:  0.2578125\n",
            "iter:  41 train_cost:  4.710135 train_acc:  0.140625 test_cost:  4.4042134 test_acc:  0.140625\n",
            "iter:  42 train_cost:  4.670552 train_acc:  0.140625 test_cost:  4.3606796 test_acc:  0.1875\n",
            "iter:  43 train_cost:  4.1889715 train_acc:  0.1328125 test_cost:  4.129484 test_acc:  0.203125\n",
            "iter:  44 train_cost:  3.9446683 train_acc:  0.2265625 test_cost:  4.211569 test_acc:  0.1640625\n",
            "iter:  45 train_cost:  4.213802 train_acc:  0.203125 test_cost:  4.0337305 test_acc:  0.1171875\n",
            "iter:  46 train_cost:  3.7211618 train_acc:  0.234375 test_cost:  3.9688067 test_acc:  0.1796875\n",
            "iter:  47 train_cost:  4.3812895 train_acc:  0.171875 test_cost:  3.9443955 test_acc:  0.1796875\n",
            "iter:  48 train_cost:  4.2456856 train_acc:  0.171875 test_cost:  3.4270859 test_acc:  0.25\n",
            "iter:  49 train_cost:  3.8208566 train_acc:  0.203125 test_cost:  3.7057822 test_acc:  0.1875\n",
            "iter:  50 train_cost:  3.781742 train_acc:  0.21875 test_cost:  3.8633726 test_acc:  0.1875\n",
            "iter:  51 train_cost:  3.6296668 train_acc:  0.25 test_cost:  3.7768333 test_acc:  0.1953125\n",
            "iter:  52 train_cost:  3.5437465 train_acc:  0.2109375 test_cost:  3.4811883 test_acc:  0.1640625\n",
            "iter:  53 train_cost:  3.3894 train_acc:  0.21875 test_cost:  3.42027 test_acc:  0.1875\n",
            "iter:  54 train_cost:  3.465367 train_acc:  0.171875 test_cost:  3.1103492 test_acc:  0.203125\n",
            "iter:  55 train_cost:  3.6454482 train_acc:  0.203125 test_cost:  3.80252 test_acc:  0.1640625\n",
            "iter:  56 train_cost:  3.4634717 train_acc:  0.171875 test_cost:  3.3507318 test_acc:  0.1875\n",
            "iter:  57 train_cost:  3.2171605 train_acc:  0.1953125 test_cost:  3.2974768 test_acc:  0.15625\n",
            "iter:  58 train_cost:  3.2855756 train_acc:  0.21875 test_cost:  3.2580752 test_acc:  0.1640625\n",
            "iter:  59 train_cost:  3.131641 train_acc:  0.25 test_cost:  3.1031556 test_acc:  0.28125\n",
            "iter:  60 train_cost:  3.1812234 train_acc:  0.1640625 test_cost:  3.2278717 test_acc:  0.203125\n",
            "iter:  61 train_cost:  3.1080294 train_acc:  0.2109375 test_cost:  2.809993 test_acc:  0.21875\n",
            "iter:  62 train_cost:  3.069429 train_acc:  0.203125 test_cost:  3.0958087 test_acc:  0.2421875\n",
            "iter:  63 train_cost:  2.8313231 train_acc:  0.2578125 test_cost:  2.9498847 test_acc:  0.2421875\n",
            "iter:  64 train_cost:  2.8277647 train_acc:  0.2578125 test_cost:  2.933188 test_acc:  0.21875\n",
            "iter:  65 train_cost:  2.8715534 train_acc:  0.2421875 test_cost:  2.4681945 test_acc:  0.2734375\n",
            "iter:  66 train_cost:  2.8157368 train_acc:  0.28125 test_cost:  3.094571 test_acc:  0.234375\n",
            "iter:  67 train_cost:  2.690106 train_acc:  0.296875 test_cost:  2.877099 test_acc:  0.2421875\n",
            "iter:  68 train_cost:  2.4915442 train_acc:  0.265625 test_cost:  2.654192 test_acc:  0.296875\n",
            "iter:  69 train_cost:  2.8821316 train_acc:  0.328125 test_cost:  2.5637717 test_acc:  0.2734375\n",
            "iter:  70 train_cost:  2.915025 train_acc:  0.2421875 test_cost:  3.0113463 test_acc:  0.2421875\n",
            "iter:  71 train_cost:  2.6711621 train_acc:  0.3125 test_cost:  2.4371657 test_acc:  0.3359375\n",
            "iter:  72 train_cost:  2.7096 train_acc:  0.296875 test_cost:  2.7328143 test_acc:  0.3046875\n",
            "iter:  73 train_cost:  2.46384 train_acc:  0.296875 test_cost:  2.5072036 test_acc:  0.3125\n",
            "iter:  74 train_cost:  2.431603 train_acc:  0.3125 test_cost:  2.451752 test_acc:  0.3203125\n",
            "iter:  75 train_cost:  2.3996792 train_acc:  0.390625 test_cost:  2.3616328 test_acc:  0.328125\n",
            "iter:  76 train_cost:  2.8773923 train_acc:  0.203125 test_cost:  2.6055899 test_acc:  0.296875\n",
            "iter:  77 train_cost:  2.4410858 train_acc:  0.3359375 test_cost:  2.8599946 test_acc:  0.2578125\n",
            "iter:  78 train_cost:  2.1861284 train_acc:  0.359375 test_cost:  2.6389427 test_acc:  0.28125\n",
            "iter:  79 train_cost:  2.7450318 train_acc:  0.3046875 test_cost:  2.1637173 test_acc:  0.390625\n",
            "iter:  80 train_cost:  2.4376163 train_acc:  0.28125 test_cost:  2.6054823 test_acc:  0.25\n",
            "iter:  81 train_cost:  2.4359846 train_acc:  0.2890625 test_cost:  2.2982166 test_acc:  0.3359375\n",
            "iter:  82 train_cost:  2.287765 train_acc:  0.34375 test_cost:  2.676503 test_acc:  0.25\n",
            "iter:  83 train_cost:  2.3514438 train_acc:  0.3515625 test_cost:  2.335253 test_acc:  0.3046875\n",
            "iter:  84 train_cost:  2.2219164 train_acc:  0.40625 test_cost:  2.3755412 test_acc:  0.296875\n",
            "iter:  85 train_cost:  2.4058425 train_acc:  0.3828125 test_cost:  2.3140886 test_acc:  0.4453125\n",
            "iter:  86 train_cost:  2.478873 train_acc:  0.3828125 test_cost:  2.4746337 test_acc:  0.3046875\n",
            "iter:  87 train_cost:  2.7607532 train_acc:  0.3359375 test_cost:  2.2613823 test_acc:  0.390625\n",
            "iter:  88 train_cost:  2.2859263 train_acc:  0.3671875 test_cost:  2.2856686 test_acc:  0.3671875\n",
            "iter:  89 train_cost:  1.9072611 train_acc:  0.4375 test_cost:  2.0622325 test_acc:  0.3671875\n",
            "iter:  90 train_cost:  2.2109084 train_acc:  0.3671875 test_cost:  2.1541436 test_acc:  0.40625\n",
            "iter:  91 train_cost:  2.2108645 train_acc:  0.34375 test_cost:  2.3426914 test_acc:  0.40625\n",
            "iter:  92 train_cost:  2.165006 train_acc:  0.390625 test_cost:  2.3945951 test_acc:  0.375\n",
            "iter:  93 train_cost:  2.2833982 train_acc:  0.3046875 test_cost:  1.9163321 test_acc:  0.4140625\n",
            "iter:  94 train_cost:  2.1546917 train_acc:  0.4296875 test_cost:  2.2089493 test_acc:  0.328125\n",
            "iter:  95 train_cost:  2.289095 train_acc:  0.390625 test_cost:  2.0211635 test_acc:  0.421875\n",
            "iter:  96 train_cost:  1.9876102 train_acc:  0.421875 test_cost:  1.9134766 test_acc:  0.4375\n",
            "iter:  97 train_cost:  2.0346682 train_acc:  0.40625 test_cost:  2.3630948 test_acc:  0.3515625\n",
            "iter:  98 train_cost:  2.08934 train_acc:  0.3984375 test_cost:  2.1567345 test_acc:  0.40625\n",
            "iter:  99 train_cost:  2.422478 train_acc:  0.328125 test_cost:  1.9721005 test_acc:  0.421875\n",
            "iter:  100 train_cost:  2.2321215 train_acc:  0.453125 test_cost:  1.9825478 test_acc:  0.4375\n",
            "iter:  101 train_cost:  1.8411319 train_acc:  0.453125 test_cost:  1.8713646 test_acc:  0.4375\n",
            "iter:  102 train_cost:  1.8157315 train_acc:  0.4765625 test_cost:  1.8015527 test_acc:  0.46875\n",
            "iter:  103 train_cost:  1.908359 train_acc:  0.4375 test_cost:  2.2076998 test_acc:  0.328125\n",
            "iter:  104 train_cost:  2.0496325 train_acc:  0.4296875 test_cost:  2.3791192 test_acc:  0.359375\n",
            "iter:  105 train_cost:  2.2126124 train_acc:  0.421875 test_cost:  1.8871986 test_acc:  0.390625\n",
            "iter:  106 train_cost:  1.9213979 train_acc:  0.4375 test_cost:  1.8301206 test_acc:  0.421875\n",
            "iter:  107 train_cost:  1.9462152 train_acc:  0.453125 test_cost:  1.9073638 test_acc:  0.4375\n",
            "iter:  108 train_cost:  2.0421245 train_acc:  0.3984375 test_cost:  2.0689611 test_acc:  0.3984375\n",
            "iter:  109 train_cost:  1.851419 train_acc:  0.4453125 test_cost:  1.7260416 test_acc:  0.4609375\n",
            "iter:  110 train_cost:  2.0447905 train_acc:  0.3984375 test_cost:  2.315452 test_acc:  0.3515625\n",
            "iter:  111 train_cost:  1.8196557 train_acc:  0.4609375 test_cost:  1.6760768 test_acc:  0.5\n",
            "iter:  112 train_cost:  1.8445207 train_acc:  0.484375 test_cost:  1.9166986 test_acc:  0.421875\n",
            "iter:  113 train_cost:  2.0139248 train_acc:  0.453125 test_cost:  1.9928229 test_acc:  0.4296875\n",
            "iter:  114 train_cost:  1.9059391 train_acc:  0.4609375 test_cost:  1.8311145 test_acc:  0.4375\n",
            "iter:  115 train_cost:  2.1448708 train_acc:  0.3828125 test_cost:  1.6977241 test_acc:  0.4921875\n",
            "iter:  116 train_cost:  1.750062 train_acc:  0.4921875 test_cost:  1.9715434 test_acc:  0.453125\n",
            "iter:  117 train_cost:  2.127418 train_acc:  0.359375 test_cost:  1.6906384 test_acc:  0.453125\n",
            "iter:  118 train_cost:  1.5429239 train_acc:  0.5 test_cost:  2.019518 test_acc:  0.40625\n",
            "iter:  119 train_cost:  2.0305052 train_acc:  0.40625 test_cost:  1.7577367 test_acc:  0.4921875\n",
            "iter:  120 train_cost:  1.9824079 train_acc:  0.421875 test_cost:  1.9553149 test_acc:  0.4375\n",
            "iter:  121 train_cost:  1.8574276 train_acc:  0.46875 test_cost:  1.6187465 test_acc:  0.515625\n",
            "iter:  122 train_cost:  1.6349919 train_acc:  0.5 test_cost:  1.7825634 test_acc:  0.453125\n",
            "iter:  123 train_cost:  1.9133757 train_acc:  0.484375 test_cost:  1.7489636 test_acc:  0.5\n",
            "iter:  124 train_cost:  1.4309623 train_acc:  0.5859375 test_cost:  1.6085719 test_acc:  0.515625\n",
            "iter:  125 train_cost:  1.9317431 train_acc:  0.3984375 test_cost:  1.918474 test_acc:  0.4296875\n",
            "iter:  126 train_cost:  1.6871204 train_acc:  0.546875 test_cost:  1.6818953 test_acc:  0.515625\n",
            "iter:  127 train_cost:  1.7337857 train_acc:  0.421875 test_cost:  2.0259073 test_acc:  0.3984375\n",
            "iter:  128 train_cost:  1.7075622 train_acc:  0.4765625 test_cost:  1.6630605 test_acc:  0.515625\n",
            "iter:  129 train_cost:  1.5813863 train_acc:  0.5859375 test_cost:  1.8810427 test_acc:  0.5\n",
            "iter:  130 train_cost:  1.8770031 train_acc:  0.484375 test_cost:  1.6472075 test_acc:  0.5390625\n",
            "iter:  131 train_cost:  1.7335107 train_acc:  0.4375 test_cost:  1.989059 test_acc:  0.4296875\n",
            "iter:  132 train_cost:  1.7799182 train_acc:  0.4453125 test_cost:  1.8128697 test_acc:  0.4296875\n",
            "iter:  133 train_cost:  1.628039 train_acc:  0.46875 test_cost:  1.6785907 test_acc:  0.546875\n",
            "iter:  134 train_cost:  1.6259148 train_acc:  0.484375 test_cost:  1.5221194 test_acc:  0.4921875\n",
            "iter:  135 train_cost:  1.67823 train_acc:  0.5078125 test_cost:  1.7563789 test_acc:  0.4375\n",
            "iter:  136 train_cost:  2.0112183 train_acc:  0.4296875 test_cost:  2.022879 test_acc:  0.375\n",
            "iter:  137 train_cost:  1.7213461 train_acc:  0.4765625 test_cost:  1.602705 test_acc:  0.5234375\n",
            "iter:  138 train_cost:  1.5690203 train_acc:  0.578125 test_cost:  1.756552 test_acc:  0.46875\n",
            "iter:  139 train_cost:  1.6694269 train_acc:  0.53125 test_cost:  1.7993498 test_acc:  0.5078125\n",
            "iter:  140 train_cost:  1.9041126 train_acc:  0.5 test_cost:  1.8922797 test_acc:  0.515625\n",
            "iter:  141 train_cost:  1.7285962 train_acc:  0.4296875 test_cost:  1.5395873 test_acc:  0.5390625\n",
            "iter:  142 train_cost:  1.7856485 train_acc:  0.4609375 test_cost:  1.5614663 test_acc:  0.421875\n",
            "iter:  143 train_cost:  1.6940479 train_acc:  0.453125 test_cost:  1.4920568 test_acc:  0.5234375\n",
            "iter:  144 train_cost:  1.5530202 train_acc:  0.5625 test_cost:  1.3951447 test_acc:  0.5078125\n",
            "iter:  145 train_cost:  1.6117957 train_acc:  0.5 test_cost:  1.2367787 test_acc:  0.59375\n",
            "iter:  146 train_cost:  1.6452626 train_acc:  0.546875 test_cost:  1.5717485 test_acc:  0.5390625\n",
            "iter:  147 train_cost:  1.5680488 train_acc:  0.5625 test_cost:  1.5292757 test_acc:  0.6015625\n",
            "iter:  148 train_cost:  1.4753201 train_acc:  0.5390625 test_cost:  1.4541212 test_acc:  0.5703125\n",
            "iter:  149 train_cost:  1.5432495 train_acc:  0.5703125 test_cost:  1.677464 test_acc:  0.4921875\n",
            "iter:  150 train_cost:  1.7415159 train_acc:  0.4765625 test_cost:  1.8416685 test_acc:  0.421875\n",
            "iter:  151 train_cost:  1.6268897 train_acc:  0.53125 test_cost:  1.5106766 test_acc:  0.5\n",
            "iter:  152 train_cost:  1.8708249 train_acc:  0.4921875 test_cost:  1.4722052 test_acc:  0.546875\n",
            "iter:  153 train_cost:  1.4967461 train_acc:  0.5390625 test_cost:  1.5278764 test_acc:  0.5546875\n",
            "iter:  154 train_cost:  1.6667596 train_acc:  0.4453125 test_cost:  1.4042434 test_acc:  0.5546875\n",
            "iter:  155 train_cost:  1.6357079 train_acc:  0.5546875 test_cost:  1.7438931 test_acc:  0.515625\n",
            "iter:  156 train_cost:  1.4853395 train_acc:  0.5546875 test_cost:  1.3521509 test_acc:  0.5859375\n",
            "iter:  157 train_cost:  1.77976 train_acc:  0.5078125 test_cost:  1.6738516 test_acc:  0.4921875\n",
            "iter:  158 train_cost:  1.567307 train_acc:  0.46875 test_cost:  1.7662028 test_acc:  0.421875\n",
            "iter:  159 train_cost:  1.6808841 train_acc:  0.53125 test_cost:  1.5247136 test_acc:  0.515625\n",
            "iter:  160 train_cost:  1.5405647 train_acc:  0.5234375 test_cost:  1.603928 test_acc:  0.5\n",
            "iter:  161 train_cost:  1.5176135 train_acc:  0.546875 test_cost:  1.5139623 test_acc:  0.546875\n",
            "iter:  162 train_cost:  1.3167479 train_acc:  0.59375 test_cost:  1.3616881 test_acc:  0.5625\n",
            "iter:  163 train_cost:  1.3530267 train_acc:  0.5546875 test_cost:  1.3882943 test_acc:  0.5625\n",
            "iter:  164 train_cost:  1.425977 train_acc:  0.5703125 test_cost:  1.5251371 test_acc:  0.53125\n",
            "iter:  165 train_cost:  1.476636 train_acc:  0.5546875 test_cost:  1.6059513 test_acc:  0.5390625\n",
            "iter:  166 train_cost:  1.542686 train_acc:  0.5234375 test_cost:  1.4314668 test_acc:  0.578125\n",
            "iter:  167 train_cost:  1.5896201 train_acc:  0.5 test_cost:  1.4476881 test_acc:  0.5390625\n",
            "iter:  168 train_cost:  1.46949 train_acc:  0.5390625 test_cost:  1.2430656 test_acc:  0.6328125\n",
            "iter:  169 train_cost:  1.4065846 train_acc:  0.5859375 test_cost:  1.4328288 test_acc:  0.515625\n",
            "iter:  170 train_cost:  1.6430955 train_acc:  0.53125 test_cost:  1.274181 test_acc:  0.5703125\n",
            "iter:  171 train_cost:  1.507533 train_acc:  0.546875 test_cost:  1.326813 test_acc:  0.5703125\n",
            "iter:  172 train_cost:  1.3902614 train_acc:  0.5390625 test_cost:  1.5181816 test_acc:  0.5859375\n",
            "iter:  173 train_cost:  1.6200402 train_acc:  0.515625 test_cost:  1.3044947 test_acc:  0.5625\n",
            "iter:  174 train_cost:  1.3504384 train_acc:  0.5390625 test_cost:  1.4281675 test_acc:  0.609375\n",
            "iter:  175 train_cost:  1.3297957 train_acc:  0.640625 test_cost:  1.3077224 test_acc:  0.5859375\n",
            "iter:  176 train_cost:  1.3771372 train_acc:  0.6171875 test_cost:  1.2697258 test_acc:  0.6328125\n",
            "iter:  177 train_cost:  1.5481014 train_acc:  0.5625 test_cost:  1.4025655 test_acc:  0.578125\n",
            "iter:  178 train_cost:  1.4472151 train_acc:  0.5703125 test_cost:  1.2823653 test_acc:  0.5625\n",
            "iter:  179 train_cost:  1.4056184 train_acc:  0.5625 test_cost:  1.2365046 test_acc:  0.6171875\n",
            "iter:  180 train_cost:  1.4752734 train_acc:  0.5234375 test_cost:  1.5263429 test_acc:  0.484375\n",
            "iter:  181 train_cost:  1.5619868 train_acc:  0.5 test_cost:  1.2499524 test_acc:  0.578125\n",
            "iter:  182 train_cost:  1.5405114 train_acc:  0.5078125 test_cost:  1.4837998 test_acc:  0.5625\n",
            "iter:  183 train_cost:  1.2412972 train_acc:  0.625 test_cost:  1.3362073 test_acc:  0.5703125\n",
            "iter:  184 train_cost:  1.4724932 train_acc:  0.515625 test_cost:  1.2430761 test_acc:  0.625\n",
            "iter:  185 train_cost:  1.5158156 train_acc:  0.546875 test_cost:  1.3527496 test_acc:  0.578125\n",
            "iter:  186 train_cost:  1.3686228 train_acc:  0.578125 test_cost:  1.3140757 test_acc:  0.6171875\n",
            "iter:  187 train_cost:  1.4270545 train_acc:  0.5546875 test_cost:  1.1512749 test_acc:  0.640625\n",
            "iter:  188 train_cost:  1.3084049 train_acc:  0.59375 test_cost:  1.0279273 test_acc:  0.6328125\n",
            "iter:  189 train_cost:  1.2144091 train_acc:  0.625 test_cost:  1.4658253 test_acc:  0.5703125\n",
            "iter:  190 train_cost:  1.1872262 train_acc:  0.609375 test_cost:  1.252837 test_acc:  0.609375\n",
            "iter:  191 train_cost:  1.3537754 train_acc:  0.53125 test_cost:  1.0381043 test_acc:  0.6796875\n",
            "iter:  192 train_cost:  1.4815029 train_acc:  0.5625 test_cost:  1.0755931 test_acc:  0.578125\n",
            "iter:  193 train_cost:  1.3584049 train_acc:  0.6171875 test_cost:  1.297127 test_acc:  0.5859375\n",
            "iter:  194 train_cost:  1.370295 train_acc:  0.578125 test_cost:  1.6091086 test_acc:  0.515625\n",
            "iter:  195 train_cost:  1.4405243 train_acc:  0.5703125 test_cost:  1.1597848 test_acc:  0.6484375\n",
            "iter:  196 train_cost:  1.5219213 train_acc:  0.5703125 test_cost:  1.2223197 test_acc:  0.6015625\n",
            "iter:  197 train_cost:  1.4096364 train_acc:  0.578125 test_cost:  1.3564835 test_acc:  0.578125\n",
            "iter:  198 train_cost:  1.1901464 train_acc:  0.59375 test_cost:  1.3896372 test_acc:  0.5546875\n",
            "iter:  199 train_cost:  1.5110068 train_acc:  0.5546875 test_cost:  1.3522277 test_acc:  0.5625\n",
            "iter:  200 train_cost:  1.3444362 train_acc:  0.5859375 test_cost:  1.2838919 test_acc:  0.625\n",
            "iter:  201 train_cost:  1.423107 train_acc:  0.578125 test_cost:  1.2307731 test_acc:  0.5859375\n",
            "iter:  202 train_cost:  1.3913264 train_acc:  0.5703125 test_cost:  1.2638204 test_acc:  0.6484375\n",
            "iter:  203 train_cost:  0.95701563 train_acc:  0.6796875 test_cost:  1.0755359 test_acc:  0.6328125\n",
            "iter:  204 train_cost:  1.1505497 train_acc:  0.6640625 test_cost:  1.1822777 test_acc:  0.6640625\n",
            "iter:  205 train_cost:  1.1157894 train_acc:  0.6484375 test_cost:  1.3193971 test_acc:  0.578125\n",
            "iter:  206 train_cost:  1.3232838 train_acc:  0.640625 test_cost:  1.4423577 test_acc:  0.53125\n",
            "iter:  207 train_cost:  1.2632097 train_acc:  0.6484375 test_cost:  1.4733009 test_acc:  0.53125\n",
            "iter:  208 train_cost:  1.0715114 train_acc:  0.6484375 test_cost:  1.1366401 test_acc:  0.609375\n",
            "iter:  209 train_cost:  1.0762335 train_acc:  0.65625 test_cost:  1.1119792 test_acc:  0.6328125\n",
            "iter:  210 train_cost:  1.3430324 train_acc:  0.5546875 test_cost:  1.310405 test_acc:  0.5703125\n",
            "iter:  211 train_cost:  1.4069777 train_acc:  0.6171875 test_cost:  1.3416754 test_acc:  0.6484375\n",
            "iter:  212 train_cost:  1.2366526 train_acc:  0.625 test_cost:  1.3332776 test_acc:  0.6328125\n",
            "iter:  213 train_cost:  1.0338936 train_acc:  0.6875 test_cost:  1.1941063 test_acc:  0.5859375\n",
            "iter:  214 train_cost:  1.4812725 train_acc:  0.515625 test_cost:  1.2532325 test_acc:  0.6328125\n",
            "iter:  215 train_cost:  1.1563485 train_acc:  0.6328125 test_cost:  1.2251691 test_acc:  0.5859375\n",
            "iter:  216 train_cost:  1.3632513 train_acc:  0.5546875 test_cost:  1.323786 test_acc:  0.5859375\n",
            "iter:  217 train_cost:  1.1300035 train_acc:  0.6171875 test_cost:  1.2859254 test_acc:  0.6171875\n",
            "iter:  218 train_cost:  1.3966558 train_acc:  0.578125 test_cost:  1.4144435 test_acc:  0.5390625\n",
            "iter:  219 train_cost:  1.1631413 train_acc:  0.6640625 test_cost:  1.2939645 test_acc:  0.59375\n",
            "iter:  220 train_cost:  1.0774347 train_acc:  0.6640625 test_cost:  1.1903806 test_acc:  0.625\n",
            "iter:  221 train_cost:  1.2365234 train_acc:  0.6328125 test_cost:  1.1967652 test_acc:  0.640625\n",
            "iter:  222 train_cost:  1.470716 train_acc:  0.5859375 test_cost:  1.2360508 test_acc:  0.6171875\n",
            "iter:  223 train_cost:  1.2136054 train_acc:  0.6484375 test_cost:  1.0077219 test_acc:  0.640625\n",
            "iter:  224 train_cost:  1.2265075 train_acc:  0.5625 test_cost:  0.8340876 test_acc:  0.6953125\n",
            "iter:  225 train_cost:  1.5138822 train_acc:  0.5390625 test_cost:  1.0479562 test_acc:  0.703125\n",
            "iter:  226 train_cost:  1.1514602 train_acc:  0.609375 test_cost:  0.91800773 test_acc:  0.71875\n",
            "iter:  227 train_cost:  1.2333797 train_acc:  0.625 test_cost:  1.1851511 test_acc:  0.6328125\n",
            "iter:  228 train_cost:  1.0082798 train_acc:  0.6796875 test_cost:  1.4339966 test_acc:  0.5703125\n",
            "iter:  229 train_cost:  1.0568289 train_acc:  0.65625 test_cost:  1.3362255 test_acc:  0.6328125\n",
            "iter:  230 train_cost:  1.1982586 train_acc:  0.5859375 test_cost:  1.2841227 test_acc:  0.5859375\n",
            "iter:  231 train_cost:  1.2257723 train_acc:  0.609375 test_cost:  1.1524217 test_acc:  0.6328125\n",
            "iter:  232 train_cost:  1.1244314 train_acc:  0.625 test_cost:  1.2867628 test_acc:  0.59375\n",
            "iter:  233 train_cost:  1.0214602 train_acc:  0.6796875 test_cost:  1.2108674 test_acc:  0.6328125\n",
            "iter:  234 train_cost:  1.1682683 train_acc:  0.6171875 test_cost:  0.9032008 test_acc:  0.6796875\n",
            "iter:  235 train_cost:  1.3483378 train_acc:  0.625 test_cost:  1.0395932 test_acc:  0.609375\n",
            "iter:  236 train_cost:  1.1267182 train_acc:  0.625 test_cost:  1.1579099 test_acc:  0.625\n",
            "iter:  237 train_cost:  1.242899 train_acc:  0.6015625 test_cost:  1.0127338 test_acc:  0.6796875\n",
            "iter:  238 train_cost:  1.3725739 train_acc:  0.546875 test_cost:  1.0557718 test_acc:  0.6796875\n",
            "iter:  239 train_cost:  1.1838665 train_acc:  0.6015625 test_cost:  1.1469537 test_acc:  0.703125\n",
            "iter:  240 train_cost:  1.0643617 train_acc:  0.6328125 test_cost:  1.049016 test_acc:  0.6640625\n",
            "iter:  241 train_cost:  1.3511869 train_acc:  0.6015625 test_cost:  1.1830249 test_acc:  0.640625\n",
            "iter:  242 train_cost:  0.9742783 train_acc:  0.671875 test_cost:  1.1684254 test_acc:  0.625\n",
            "iter:  243 train_cost:  0.9919368 train_acc:  0.6640625 test_cost:  1.013558 test_acc:  0.65625\n",
            "iter:  244 train_cost:  0.95964766 train_acc:  0.7265625 test_cost:  1.0923051 test_acc:  0.6640625\n",
            "iter:  245 train_cost:  0.94626516 train_acc:  0.7265625 test_cost:  1.0590115 test_acc:  0.609375\n",
            "iter:  246 train_cost:  1.3712676 train_acc:  0.609375 test_cost:  1.2030897 test_acc:  0.59375\n",
            "iter:  247 train_cost:  1.2003707 train_acc:  0.625 test_cost:  1.1945 test_acc:  0.5703125\n",
            "iter:  248 train_cost:  0.9905077 train_acc:  0.640625 test_cost:  1.0332987 test_acc:  0.6796875\n",
            "iter:  249 train_cost:  1.065504 train_acc:  0.6640625 test_cost:  1.1133814 test_acc:  0.6171875\n",
            "iter:  250 train_cost:  1.0861461 train_acc:  0.6953125 test_cost:  1.1928979 test_acc:  0.6015625\n",
            "iter:  251 train_cost:  1.1908633 train_acc:  0.625 test_cost:  1.0582078 test_acc:  0.671875\n",
            "iter:  252 train_cost:  0.9666475 train_acc:  0.65625 test_cost:  1.0083547 test_acc:  0.625\n",
            "iter:  253 train_cost:  0.95091116 train_acc:  0.7265625 test_cost:  1.155246 test_acc:  0.625\n",
            "iter:  254 train_cost:  0.9926094 train_acc:  0.75 test_cost:  1.1604543 test_acc:  0.6171875\n",
            "iter:  255 train_cost:  0.97678 train_acc:  0.7265625 test_cost:  1.0786177 test_acc:  0.6484375\n",
            "iter:  256 train_cost:  0.96038675 train_acc:  0.65625 test_cost:  1.2512205 test_acc:  0.6640625\n",
            "iter:  257 train_cost:  1.5450876 train_acc:  0.515625 test_cost:  1.1464509 test_acc:  0.609375\n",
            "iter:  258 train_cost:  1.2276689 train_acc:  0.6796875 test_cost:  0.87163514 test_acc:  0.734375\n",
            "iter:  259 train_cost:  0.8943603 train_acc:  0.671875 test_cost:  1.0812836 test_acc:  0.6640625\n",
            "iter:  260 train_cost:  0.77755904 train_acc:  0.7265625 test_cost:  0.9480662 test_acc:  0.6875\n",
            "iter:  261 train_cost:  0.9607141 train_acc:  0.703125 test_cost:  1.123697 test_acc:  0.6484375\n",
            "iter:  262 train_cost:  1.0930185 train_acc:  0.6796875 test_cost:  1.19916 test_acc:  0.640625\n",
            "iter:  263 train_cost:  1.1873622 train_acc:  0.6953125 test_cost:  0.9953975 test_acc:  0.6640625\n",
            "iter:  264 train_cost:  0.95652163 train_acc:  0.671875 test_cost:  1.1369877 test_acc:  0.703125\n",
            "iter:  265 train_cost:  0.8021868 train_acc:  0.71875 test_cost:  1.0739322 test_acc:  0.6796875\n",
            "iter:  266 train_cost:  1.0258614 train_acc:  0.6953125 test_cost:  1.0717167 test_acc:  0.703125\n",
            "iter:  267 train_cost:  0.94596183 train_acc:  0.7421875 test_cost:  1.0807313 test_acc:  0.671875\n",
            "iter:  268 train_cost:  1.0410995 train_acc:  0.6484375 test_cost:  1.0273062 test_acc:  0.6640625\n",
            "iter:  269 train_cost:  1.2303569 train_acc:  0.6328125 test_cost:  1.1744194 test_acc:  0.6015625\n",
            "iter:  270 train_cost:  1.0259591 train_acc:  0.6796875 test_cost:  1.180938 test_acc:  0.671875\n",
            "iter:  271 train_cost:  1.1002024 train_acc:  0.65625 test_cost:  1.0122489 test_acc:  0.6953125\n",
            "iter:  272 train_cost:  1.0960267 train_acc:  0.65625 test_cost:  0.8782556 test_acc:  0.703125\n",
            "iter:  273 train_cost:  1.0303868 train_acc:  0.71875 test_cost:  1.0273737 test_acc:  0.640625\n",
            "iter:  274 train_cost:  1.0468059 train_acc:  0.6328125 test_cost:  0.960342 test_acc:  0.6953125\n",
            "iter:  275 train_cost:  1.2209345 train_acc:  0.6484375 test_cost:  1.1025953 test_acc:  0.7109375\n",
            "iter:  276 train_cost:  1.0605308 train_acc:  0.6953125 test_cost:  0.85788083 test_acc:  0.7109375\n",
            "iter:  277 train_cost:  1.180213 train_acc:  0.625 test_cost:  1.0358016 test_acc:  0.65625\n",
            "iter:  278 train_cost:  1.0873111 train_acc:  0.6640625 test_cost:  1.1756905 test_acc:  0.671875\n",
            "iter:  279 train_cost:  0.8550531 train_acc:  0.7265625 test_cost:  1.1542772 test_acc:  0.609375\n",
            "iter:  280 train_cost:  1.0125172 train_acc:  0.6796875 test_cost:  0.79326075 test_acc:  0.734375\n",
            "iter:  281 train_cost:  0.8725528 train_acc:  0.7421875 test_cost:  0.80138624 test_acc:  0.7578125\n",
            "iter:  282 train_cost:  1.2220062 train_acc:  0.6640625 test_cost:  0.9137284 test_acc:  0.71875\n",
            "iter:  283 train_cost:  0.8962743 train_acc:  0.6953125 test_cost:  0.77530026 test_acc:  0.7265625\n",
            "iter:  284 train_cost:  1.3327197 train_acc:  0.6640625 test_cost:  1.1457345 test_acc:  0.6171875\n",
            "iter:  285 train_cost:  0.9601226 train_acc:  0.6640625 test_cost:  0.9629418 test_acc:  0.671875\n",
            "iter:  286 train_cost:  0.8361831 train_acc:  0.7421875 test_cost:  0.96449614 test_acc:  0.6640625\n",
            "iter:  287 train_cost:  1.0371981 train_acc:  0.71875 test_cost:  1.1648927 test_acc:  0.6640625\n",
            "iter:  288 train_cost:  0.89597386 train_acc:  0.7265625 test_cost:  1.0555887 test_acc:  0.71875\n",
            "iter:  289 train_cost:  1.0218792 train_acc:  0.7421875 test_cost:  1.1265048 test_acc:  0.6328125\n",
            "iter:  290 train_cost:  1.0683099 train_acc:  0.703125 test_cost:  1.0091437 test_acc:  0.671875\n",
            "iter:  291 train_cost:  0.7542572 train_acc:  0.765625 test_cost:  0.6808607 test_acc:  0.765625\n",
            "iter:  292 train_cost:  0.88588727 train_acc:  0.6953125 test_cost:  1.0853907 test_acc:  0.65625\n",
            "iter:  293 train_cost:  1.0004503 train_acc:  0.6484375 test_cost:  0.8410275 test_acc:  0.6875\n",
            "iter:  294 train_cost:  0.9936731 train_acc:  0.703125 test_cost:  1.0898869 test_acc:  0.7265625\n",
            "iter:  295 train_cost:  0.9269556 train_acc:  0.734375 test_cost:  1.1138399 test_acc:  0.640625\n",
            "iter:  296 train_cost:  1.160096 train_acc:  0.6796875 test_cost:  0.99124897 test_acc:  0.671875\n",
            "iter:  297 train_cost:  0.8871211 train_acc:  0.7265625 test_cost:  1.0348806 test_acc:  0.703125\n",
            "iter:  298 train_cost:  0.8803921 train_acc:  0.671875 test_cost:  0.94960654 test_acc:  0.671875\n",
            "iter:  299 train_cost:  1.1069138 train_acc:  0.640625 test_cost:  1.1141479 test_acc:  0.671875\n",
            "iter:  300 train_cost:  0.9960637 train_acc:  0.7421875 test_cost:  1.0209291 test_acc:  0.6796875\n",
            "iter:  301 train_cost:  0.9792253 train_acc:  0.7265625 test_cost:  1.0431815 test_acc:  0.625\n",
            "iter:  302 train_cost:  0.8697498 train_acc:  0.7109375 test_cost:  0.8241681 test_acc:  0.7265625\n",
            "iter:  303 train_cost:  0.869043 train_acc:  0.7578125 test_cost:  0.9726685 test_acc:  0.734375\n",
            "iter:  304 train_cost:  1.1318734 train_acc:  0.6875 test_cost:  0.8730501 test_acc:  0.7265625\n",
            "iter:  305 train_cost:  0.8977362 train_acc:  0.7265625 test_cost:  0.8605453 test_acc:  0.7265625\n",
            "iter:  306 train_cost:  1.012039 train_acc:  0.6640625 test_cost:  0.97329646 test_acc:  0.703125\n",
            "iter:  307 train_cost:  1.0611331 train_acc:  0.703125 test_cost:  0.9293803 test_acc:  0.703125\n",
            "iter:  308 train_cost:  0.9767349 train_acc:  0.671875 test_cost:  0.89142793 test_acc:  0.7421875\n",
            "iter:  309 train_cost:  1.0451247 train_acc:  0.6875 test_cost:  1.0776489 test_acc:  0.6796875\n",
            "iter:  310 train_cost:  0.85687745 train_acc:  0.7109375 test_cost:  0.81655073 test_acc:  0.7265625\n",
            "iter:  311 train_cost:  0.89056593 train_acc:  0.703125 test_cost:  0.88060313 test_acc:  0.7109375\n",
            "iter:  312 train_cost:  0.88374764 train_acc:  0.7421875 test_cost:  0.93979555 test_acc:  0.6484375\n",
            "iter:  313 train_cost:  0.79387915 train_acc:  0.703125 test_cost:  0.80121446 test_acc:  0.7109375\n",
            "iter:  314 train_cost:  0.9760963 train_acc:  0.6640625 test_cost:  0.94505024 test_acc:  0.6875\n",
            "iter:  315 train_cost:  0.8428001 train_acc:  0.671875 test_cost:  1.0298202 test_acc:  0.71875\n",
            "iter:  316 train_cost:  1.1508327 train_acc:  0.65625 test_cost:  0.8377403 test_acc:  0.75\n",
            "iter:  317 train_cost:  0.97645986 train_acc:  0.6640625 test_cost:  0.9642604 test_acc:  0.6875\n",
            "iter:  318 train_cost:  0.7337512 train_acc:  0.7421875 test_cost:  0.71070725 test_acc:  0.75\n",
            "iter:  319 train_cost:  0.95638096 train_acc:  0.71875 test_cost:  1.087337 test_acc:  0.65625\n",
            "iter:  320 train_cost:  0.95232755 train_acc:  0.6875 test_cost:  1.06623 test_acc:  0.640625\n",
            "iter:  321 train_cost:  0.77404714 train_acc:  0.7421875 test_cost:  1.0761642 test_acc:  0.6796875\n",
            "iter:  322 train_cost:  0.9555762 train_acc:  0.6796875 test_cost:  0.8545177 test_acc:  0.6796875\n",
            "iter:  323 train_cost:  0.9877746 train_acc:  0.6796875 test_cost:  1.1677669 test_acc:  0.6796875\n",
            "iter:  324 train_cost:  0.9133779 train_acc:  0.7265625 test_cost:  0.9627169 test_acc:  0.7265625\n",
            "iter:  325 train_cost:  0.96628356 train_acc:  0.6640625 test_cost:  0.7551621 test_acc:  0.7265625\n",
            "iter:  326 train_cost:  0.9413429 train_acc:  0.7109375 test_cost:  0.9513266 test_acc:  0.6953125\n",
            "iter:  327 train_cost:  0.7806425 train_acc:  0.7578125 test_cost:  0.9667751 test_acc:  0.71875\n",
            "iter:  328 train_cost:  0.93881726 train_acc:  0.7109375 test_cost:  0.8568286 test_acc:  0.6796875\n",
            "iter:  329 train_cost:  0.9356934 train_acc:  0.7109375 test_cost:  0.90116346 test_acc:  0.7265625\n",
            "iter:  330 train_cost:  0.91609716 train_acc:  0.6796875 test_cost:  0.88631207 test_acc:  0.7109375\n",
            "iter:  331 train_cost:  1.0666192 train_acc:  0.640625 test_cost:  0.8282491 test_acc:  0.765625\n",
            "iter:  332 train_cost:  0.71600235 train_acc:  0.7578125 test_cost:  1.1073847 test_acc:  0.6328125\n",
            "iter:  333 train_cost:  0.8055456 train_acc:  0.765625 test_cost:  0.76577693 test_acc:  0.734375\n",
            "iter:  334 train_cost:  0.93263125 train_acc:  0.6875 test_cost:  1.0083923 test_acc:  0.6875\n",
            "iter:  335 train_cost:  0.94158816 train_acc:  0.7734375 test_cost:  0.8074963 test_acc:  0.75\n",
            "iter:  336 train_cost:  1.1898875 train_acc:  0.59375 test_cost:  0.9636494 test_acc:  0.6875\n",
            "iter:  337 train_cost:  0.9320129 train_acc:  0.71875 test_cost:  0.7164811 test_acc:  0.71875\n",
            "iter:  338 train_cost:  0.843828 train_acc:  0.6953125 test_cost:  0.8566804 test_acc:  0.7265625\n",
            "iter:  339 train_cost:  0.91507226 train_acc:  0.71875 test_cost:  0.9367528 test_acc:  0.71875\n",
            "iter:  340 train_cost:  0.98287374 train_acc:  0.7109375 test_cost:  0.9043604 test_acc:  0.65625\n",
            "iter:  341 train_cost:  0.8515603 train_acc:  0.75 test_cost:  0.8396583 test_acc:  0.734375\n",
            "iter:  342 train_cost:  1.0215921 train_acc:  0.6875 test_cost:  0.8592275 test_acc:  0.703125\n",
            "iter:  343 train_cost:  0.8981303 train_acc:  0.7265625 test_cost:  0.7658174 test_acc:  0.765625\n",
            "iter:  344 train_cost:  1.1433713 train_acc:  0.6875 test_cost:  1.308487 test_acc:  0.640625\n",
            "iter:  345 train_cost:  0.92599916 train_acc:  0.703125 test_cost:  0.9613524 test_acc:  0.75\n",
            "iter:  346 train_cost:  0.90279377 train_acc:  0.6953125 test_cost:  0.81388545 test_acc:  0.703125\n",
            "iter:  347 train_cost:  0.93701696 train_acc:  0.6953125 test_cost:  0.81379914 test_acc:  0.7578125\n",
            "iter:  348 train_cost:  0.9500067 train_acc:  0.6796875 test_cost:  0.7716102 test_acc:  0.7265625\n",
            "iter:  349 train_cost:  0.9758754 train_acc:  0.65625 test_cost:  0.91966444 test_acc:  0.7734375\n",
            "iter:  350 train_cost:  1.007634 train_acc:  0.6796875 test_cost:  0.8745445 test_acc:  0.7265625\n",
            "iter:  351 train_cost:  0.82083964 train_acc:  0.734375 test_cost:  0.7560355 test_acc:  0.703125\n",
            "iter:  352 train_cost:  1.2102299 train_acc:  0.640625 test_cost:  0.9743176 test_acc:  0.6953125\n",
            "iter:  353 train_cost:  0.864457 train_acc:  0.7421875 test_cost:  0.8744928 test_acc:  0.6953125\n",
            "iter:  354 train_cost:  0.7011037 train_acc:  0.7578125 test_cost:  0.9077906 test_acc:  0.7109375\n",
            "iter:  355 train_cost:  0.86588836 train_acc:  0.75 test_cost:  0.711491 test_acc:  0.765625\n",
            "iter:  356 train_cost:  1.0187869 train_acc:  0.71875 test_cost:  0.7377787 test_acc:  0.765625\n",
            "iter:  357 train_cost:  1.0439296 train_acc:  0.703125 test_cost:  0.8826869 test_acc:  0.734375\n",
            "iter:  358 train_cost:  0.68539464 train_acc:  0.7578125 test_cost:  0.7316533 test_acc:  0.765625\n",
            "iter:  359 train_cost:  0.85932004 train_acc:  0.7421875 test_cost:  0.7783528 test_acc:  0.765625\n",
            "iter:  360 train_cost:  0.93856186 train_acc:  0.7421875 test_cost:  0.668537 test_acc:  0.8203125\n",
            "iter:  361 train_cost:  0.78530514 train_acc:  0.765625 test_cost:  0.94760805 test_acc:  0.703125\n",
            "iter:  362 train_cost:  0.9912869 train_acc:  0.6484375 test_cost:  0.8892066 test_acc:  0.7421875\n",
            "iter:  363 train_cost:  0.9575307 train_acc:  0.6796875 test_cost:  0.9470557 test_acc:  0.734375\n",
            "iter:  364 train_cost:  0.86864877 train_acc:  0.7734375 test_cost:  0.9674639 test_acc:  0.71875\n",
            "iter:  365 train_cost:  0.75265574 train_acc:  0.71875 test_cost:  0.78453696 test_acc:  0.7265625\n",
            "iter:  366 train_cost:  1.0627496 train_acc:  0.671875 test_cost:  0.832828 test_acc:  0.703125\n",
            "iter:  367 train_cost:  1.0426304 train_acc:  0.7265625 test_cost:  0.7196898 test_acc:  0.7734375\n",
            "iter:  368 train_cost:  0.81144303 train_acc:  0.703125 test_cost:  0.7247081 test_acc:  0.734375\n",
            "iter:  369 train_cost:  0.76231 train_acc:  0.7421875 test_cost:  0.8140742 test_acc:  0.734375\n",
            "iter:  370 train_cost:  0.6348347 train_acc:  0.7734375 test_cost:  0.59095144 test_acc:  0.796875\n",
            "iter:  371 train_cost:  0.93936634 train_acc:  0.75 test_cost:  1.0975783 test_acc:  0.6640625\n",
            "iter:  372 train_cost:  0.85758746 train_acc:  0.6953125 test_cost:  0.7085519 test_acc:  0.7578125\n",
            "iter:  373 train_cost:  1.0708935 train_acc:  0.6796875 test_cost:  0.8782608 test_acc:  0.765625\n",
            "iter:  374 train_cost:  0.9648795 train_acc:  0.6953125 test_cost:  0.7915373 test_acc:  0.734375\n",
            "iter:  375 train_cost:  0.9763508 train_acc:  0.7265625 test_cost:  1.0575582 test_acc:  0.6796875\n",
            "iter:  376 train_cost:  0.6849138 train_acc:  0.78125 test_cost:  0.72131884 test_acc:  0.734375\n",
            "iter:  377 train_cost:  0.92727053 train_acc:  0.6953125 test_cost:  1.1214234 test_acc:  0.6484375\n",
            "iter:  378 train_cost:  0.7555158 train_acc:  0.7578125 test_cost:  0.8590577 test_acc:  0.734375\n",
            "iter:  379 train_cost:  0.66209376 train_acc:  0.75 test_cost:  0.7438818 test_acc:  0.765625\n",
            "iter:  380 train_cost:  0.8153533 train_acc:  0.7578125 test_cost:  0.9622229 test_acc:  0.6875\n",
            "iter:  381 train_cost:  0.69634914 train_acc:  0.7734375 test_cost:  0.8142813 test_acc:  0.7578125\n",
            "iter:  382 train_cost:  0.8386357 train_acc:  0.75 test_cost:  0.69690543 test_acc:  0.765625\n",
            "iter:  383 train_cost:  0.6620232 train_acc:  0.7578125 test_cost:  0.65108204 test_acc:  0.796875\n",
            "iter:  384 train_cost:  0.7038498 train_acc:  0.7890625 test_cost:  0.73018974 test_acc:  0.7421875\n",
            "iter:  385 train_cost:  0.62022924 train_acc:  0.828125 test_cost:  0.9544616 test_acc:  0.703125\n",
            "iter:  386 train_cost:  0.83916736 train_acc:  0.6953125 test_cost:  1.0071638 test_acc:  0.6953125\n",
            "iter:  387 train_cost:  0.82198805 train_acc:  0.7265625 test_cost:  0.8463352 test_acc:  0.7421875\n",
            "iter:  388 train_cost:  0.6821846 train_acc:  0.75 test_cost:  0.8573998 test_acc:  0.71875\n",
            "iter:  389 train_cost:  0.79514134 train_acc:  0.75 test_cost:  0.7020632 test_acc:  0.7734375\n",
            "iter:  390 train_cost:  0.74921775 train_acc:  0.7421875 test_cost:  0.69889414 test_acc:  0.7734375\n",
            "iter:  391 train_cost:  0.83048475 train_acc:  0.765625 test_cost:  0.76777196 test_acc:  0.75\n",
            "iter:  392 train_cost:  0.859164 train_acc:  0.7109375 test_cost:  1.0080037 test_acc:  0.6953125\n",
            "iter:  393 train_cost:  0.76851964 train_acc:  0.75 test_cost:  0.72494876 test_acc:  0.765625\n",
            "iter:  394 train_cost:  0.79998505 train_acc:  0.7421875 test_cost:  0.8158632 test_acc:  0.734375\n",
            "iter:  395 train_cost:  0.6965014 train_acc:  0.8125 test_cost:  0.896075 test_acc:  0.734375\n",
            "iter:  396 train_cost:  0.9511194 train_acc:  0.6796875 test_cost:  0.8308579 test_acc:  0.734375\n",
            "iter:  397 train_cost:  0.65119624 train_acc:  0.734375 test_cost:  0.9141835 test_acc:  0.7265625\n",
            "iter:  398 train_cost:  0.7445377 train_acc:  0.7578125 test_cost:  0.92140436 test_acc:  0.703125\n",
            "iter:  399 train_cost:  0.7773554 train_acc:  0.7890625 test_cost:  0.8062684 test_acc:  0.71875\n",
            "iter:  400 train_cost:  0.8005177 train_acc:  0.765625 test_cost:  0.8208655 test_acc:  0.71875\n",
            "iter:  401 train_cost:  0.80219185 train_acc:  0.75 test_cost:  0.7896081 test_acc:  0.71875\n",
            "iter:  402 train_cost:  0.9226996 train_acc:  0.7265625 test_cost:  0.91835165 test_acc:  0.7109375\n",
            "iter:  403 train_cost:  0.83848345 train_acc:  0.7578125 test_cost:  0.648385 test_acc:  0.796875\n",
            "iter:  404 train_cost:  0.6444637 train_acc:  0.8203125 test_cost:  0.76939505 test_acc:  0.75\n",
            "iter:  405 train_cost:  0.87269974 train_acc:  0.7265625 test_cost:  0.7498305 test_acc:  0.7578125\n",
            "iter:  406 train_cost:  0.5447296 train_acc:  0.7734375 test_cost:  0.77101326 test_acc:  0.7265625\n",
            "iter:  407 train_cost:  0.7837641 train_acc:  0.7734375 test_cost:  0.86017025 test_acc:  0.71875\n",
            "iter:  408 train_cost:  0.7099564 train_acc:  0.75 test_cost:  1.1184691 test_acc:  0.625\n",
            "iter:  409 train_cost:  0.7950867 train_acc:  0.7578125 test_cost:  0.5135163 test_acc:  0.8515625\n",
            "iter:  410 train_cost:  1.0461004 train_acc:  0.6953125 test_cost:  0.712786 test_acc:  0.7578125\n",
            "iter:  411 train_cost:  0.889362 train_acc:  0.703125 test_cost:  0.6098208 test_acc:  0.7890625\n",
            "iter:  412 train_cost:  0.85586905 train_acc:  0.6875 test_cost:  0.7306673 test_acc:  0.75\n",
            "iter:  413 train_cost:  0.9145747 train_acc:  0.7265625 test_cost:  0.86159503 test_acc:  0.75\n",
            "iter:  414 train_cost:  0.8606618 train_acc:  0.7421875 test_cost:  1.0571882 test_acc:  0.6796875\n",
            "iter:  415 train_cost:  0.9261806 train_acc:  0.6796875 test_cost:  0.87651783 test_acc:  0.7109375\n",
            "iter:  416 train_cost:  0.75832075 train_acc:  0.8046875 test_cost:  0.7578433 test_acc:  0.7734375\n",
            "iter:  417 train_cost:  0.7430783 train_acc:  0.7421875 test_cost:  0.6133267 test_acc:  0.7890625\n",
            "iter:  418 train_cost:  0.8701528 train_acc:  0.6953125 test_cost:  0.7946275 test_acc:  0.765625\n",
            "iter:  419 train_cost:  0.77204126 train_acc:  0.7578125 test_cost:  0.7385336 test_acc:  0.7890625\n",
            "iter:  420 train_cost:  0.88926274 train_acc:  0.703125 test_cost:  0.71239924 test_acc:  0.765625\n",
            "iter:  421 train_cost:  0.8315942 train_acc:  0.7421875 test_cost:  0.80725324 test_acc:  0.75\n",
            "iter:  422 train_cost:  0.9374602 train_acc:  0.703125 test_cost:  0.6249101 test_acc:  0.828125\n",
            "iter:  423 train_cost:  0.652531 train_acc:  0.8125 test_cost:  0.7954212 test_acc:  0.734375\n",
            "iter:  424 train_cost:  0.69226027 train_acc:  0.78125 test_cost:  0.74360865 test_acc:  0.765625\n",
            "iter:  425 train_cost:  0.8111261 train_acc:  0.7109375 test_cost:  0.6327583 test_acc:  0.7734375\n",
            "iter:  426 train_cost:  0.60670966 train_acc:  0.8046875 test_cost:  0.7097404 test_acc:  0.7578125\n",
            "iter:  427 train_cost:  0.7370244 train_acc:  0.7734375 test_cost:  0.7436991 test_acc:  0.7734375\n",
            "iter:  428 train_cost:  0.90477794 train_acc:  0.71875 test_cost:  0.74420387 test_acc:  0.7265625\n",
            "iter:  429 train_cost:  0.9365073 train_acc:  0.71875 test_cost:  0.6449224 test_acc:  0.7890625\n",
            "iter:  430 train_cost:  0.7545645 train_acc:  0.75 test_cost:  0.7382547 test_acc:  0.7890625\n",
            "iter:  431 train_cost:  0.7640245 train_acc:  0.75 test_cost:  0.8972151 test_acc:  0.734375\n",
            "iter:  432 train_cost:  0.66162807 train_acc:  0.78125 test_cost:  0.7736838 test_acc:  0.734375\n",
            "iter:  433 train_cost:  0.91295135 train_acc:  0.7421875 test_cost:  0.76096064 test_acc:  0.7578125\n",
            "iter:  434 train_cost:  0.74637365 train_acc:  0.78125 test_cost:  0.8622426 test_acc:  0.734375\n",
            "iter:  435 train_cost:  0.795454 train_acc:  0.703125 test_cost:  0.7456982 test_acc:  0.71875\n",
            "iter:  436 train_cost:  0.70593184 train_acc:  0.7421875 test_cost:  0.6859361 test_acc:  0.8359375\n",
            "iter:  437 train_cost:  0.943906 train_acc:  0.7109375 test_cost:  0.65563273 test_acc:  0.7578125\n",
            "iter:  438 train_cost:  0.6722984 train_acc:  0.7734375 test_cost:  0.8847872 test_acc:  0.7421875\n",
            "iter:  439 train_cost:  0.64419556 train_acc:  0.7890625 test_cost:  0.89229006 test_acc:  0.71875\n",
            "iter:  440 train_cost:  1.0016917 train_acc:  0.6796875 test_cost:  0.65747136 test_acc:  0.7734375\n",
            "iter:  441 train_cost:  0.6056013 train_acc:  0.796875 test_cost:  0.54177314 test_acc:  0.828125\n",
            "iter:  442 train_cost:  0.6007868 train_acc:  0.796875 test_cost:  0.8686589 test_acc:  0.71875\n",
            "iter:  443 train_cost:  0.9595243 train_acc:  0.6796875 test_cost:  0.6722294 test_acc:  0.765625\n",
            "iter:  444 train_cost:  0.6563836 train_acc:  0.796875 test_cost:  0.6738926 test_acc:  0.78125\n",
            "iter:  445 train_cost:  0.60031736 train_acc:  0.78125 test_cost:  0.72342193 test_acc:  0.7734375\n",
            "iter:  446 train_cost:  0.7769717 train_acc:  0.765625 test_cost:  0.7059678 test_acc:  0.796875\n",
            "iter:  447 train_cost:  0.7714344 train_acc:  0.7421875 test_cost:  0.75875103 test_acc:  0.7734375\n",
            "iter:  448 train_cost:  0.7072902 train_acc:  0.8203125 test_cost:  0.7107798 test_acc:  0.7578125\n",
            "iter:  449 train_cost:  0.6587181 train_acc:  0.8125 test_cost:  0.5523739 test_acc:  0.8125\n",
            "iter:  450 train_cost:  0.8002273 train_acc:  0.6953125 test_cost:  0.6079105 test_acc:  0.7578125\n",
            "iter:  451 train_cost:  0.6091626 train_acc:  0.78125 test_cost:  0.71514225 test_acc:  0.7890625\n",
            "iter:  452 train_cost:  0.77061605 train_acc:  0.7421875 test_cost:  0.79538745 test_acc:  0.7578125\n",
            "iter:  453 train_cost:  0.6676642 train_acc:  0.765625 test_cost:  0.86852264 test_acc:  0.7734375\n",
            "iter:  454 train_cost:  0.89826316 train_acc:  0.703125 test_cost:  0.63851696 test_acc:  0.7734375\n",
            "iter:  455 train_cost:  0.6618451 train_acc:  0.8125 test_cost:  0.73177713 test_acc:  0.7734375\n",
            "iter:  456 train_cost:  0.71305615 train_acc:  0.765625 test_cost:  0.942513 test_acc:  0.734375\n",
            "iter:  457 train_cost:  0.5983943 train_acc:  0.828125 test_cost:  0.70411927 test_acc:  0.7578125\n",
            "iter:  458 train_cost:  0.55554223 train_acc:  0.8125 test_cost:  0.7298852 test_acc:  0.78125\n",
            "iter:  459 train_cost:  0.5472915 train_acc:  0.8125 test_cost:  0.79191166 test_acc:  0.75\n",
            "iter:  460 train_cost:  0.80270666 train_acc:  0.6953125 test_cost:  0.64729154 test_acc:  0.8046875\n",
            "iter:  461 train_cost:  0.6629193 train_acc:  0.7578125 test_cost:  0.6986009 test_acc:  0.78125\n",
            "iter:  462 train_cost:  0.6649853 train_acc:  0.7578125 test_cost:  0.86549383 test_acc:  0.703125\n",
            "iter:  463 train_cost:  0.7730581 train_acc:  0.71875 test_cost:  0.6374543 test_acc:  0.8046875\n",
            "iter:  464 train_cost:  0.97429025 train_acc:  0.6875 test_cost:  0.67632663 test_acc:  0.796875\n",
            "iter:  465 train_cost:  0.70396376 train_acc:  0.8125 test_cost:  0.6996966 test_acc:  0.7265625\n",
            "iter:  466 train_cost:  0.9007021 train_acc:  0.71875 test_cost:  0.68294585 test_acc:  0.7734375\n",
            "iter:  467 train_cost:  0.8251201 train_acc:  0.765625 test_cost:  0.5532571 test_acc:  0.828125\n",
            "iter:  468 train_cost:  0.59540755 train_acc:  0.765625 test_cost:  0.83560663 test_acc:  0.734375\n",
            "iter:  469 train_cost:  0.62717897 train_acc:  0.8125 test_cost:  0.79003155 test_acc:  0.7578125\n",
            "iter:  470 train_cost:  0.8040658 train_acc:  0.7734375 test_cost:  0.80934334 test_acc:  0.7578125\n",
            "iter:  471 train_cost:  0.8265238 train_acc:  0.75 test_cost:  0.6821518 test_acc:  0.78125\n",
            "iter:  472 train_cost:  0.60943925 train_acc:  0.8046875 test_cost:  1.0527492 test_acc:  0.703125\n",
            "iter:  473 train_cost:  0.76686037 train_acc:  0.75 test_cost:  0.6195514 test_acc:  0.8203125\n",
            "iter:  474 train_cost:  0.6302328 train_acc:  0.765625 test_cost:  0.83337283 test_acc:  0.7109375\n",
            "iter:  475 train_cost:  0.9695442 train_acc:  0.6953125 test_cost:  0.5198836 test_acc:  0.828125\n",
            "iter:  476 train_cost:  1.0154378 train_acc:  0.734375 test_cost:  0.8626567 test_acc:  0.734375\n",
            "iter:  477 train_cost:  0.74991703 train_acc:  0.75 test_cost:  0.81474435 test_acc:  0.7890625\n",
            "iter:  478 train_cost:  0.80693495 train_acc:  0.7421875 test_cost:  0.6066856 test_acc:  0.8046875\n",
            "iter:  479 train_cost:  0.8777968 train_acc:  0.6796875 test_cost:  0.643676 test_acc:  0.796875\n",
            "iter:  480 train_cost:  0.79344547 train_acc:  0.75 test_cost:  0.6805426 test_acc:  0.7890625\n",
            "iter:  481 train_cost:  0.6557752 train_acc:  0.734375 test_cost:  0.6120951 test_acc:  0.796875\n",
            "iter:  482 train_cost:  0.6708142 train_acc:  0.7734375 test_cost:  0.7143491 test_acc:  0.734375\n",
            "iter:  483 train_cost:  0.8179727 train_acc:  0.7109375 test_cost:  0.7206939 test_acc:  0.7578125\n",
            "iter:  484 train_cost:  0.7007496 train_acc:  0.765625 test_cost:  0.6625021 test_acc:  0.8125\n",
            "iter:  485 train_cost:  0.6729618 train_acc:  0.7890625 test_cost:  0.74417377 test_acc:  0.78125\n",
            "iter:  486 train_cost:  0.68908834 train_acc:  0.796875 test_cost:  0.6844236 test_acc:  0.8203125\n",
            "iter:  487 train_cost:  0.7629781 train_acc:  0.78125 test_cost:  0.7260057 test_acc:  0.7890625\n",
            "iter:  488 train_cost:  0.52183896 train_acc:  0.8359375 test_cost:  0.5032046 test_acc:  0.859375\n",
            "iter:  489 train_cost:  0.67628896 train_acc:  0.796875 test_cost:  0.71511936 test_acc:  0.78125\n",
            "iter:  490 train_cost:  0.76517606 train_acc:  0.7421875 test_cost:  0.64208674 test_acc:  0.7734375\n",
            "iter:  491 train_cost:  0.59991145 train_acc:  0.796875 test_cost:  0.8541236 test_acc:  0.703125\n",
            "iter:  492 train_cost:  0.788605 train_acc:  0.796875 test_cost:  0.81903577 test_acc:  0.765625\n",
            "iter:  493 train_cost:  0.5679113 train_acc:  0.8359375 test_cost:  0.7616474 test_acc:  0.78125\n",
            "iter:  494 train_cost:  0.6050601 train_acc:  0.8046875 test_cost:  0.7264462 test_acc:  0.78125\n",
            "iter:  495 train_cost:  0.4484427 train_acc:  0.8359375 test_cost:  0.74622613 test_acc:  0.75\n",
            "iter:  496 train_cost:  0.78214705 train_acc:  0.734375 test_cost:  0.69968617 test_acc:  0.7890625\n",
            "iter:  497 train_cost:  0.80191743 train_acc:  0.7109375 test_cost:  0.5995041 test_acc:  0.8359375\n",
            "iter:  498 train_cost:  0.67871165 train_acc:  0.7734375 test_cost:  0.662776 test_acc:  0.7890625\n",
            "iter:  499 train_cost:  0.8626547 train_acc:  0.7734375 test_cost:  0.6469196 test_acc:  0.8203125\n",
            "iter:  500 train_cost:  0.8577143 train_acc:  0.734375 test_cost:  0.6921216 test_acc:  0.7734375\n",
            "iter:  501 train_cost:  0.7012433 train_acc:  0.7890625 test_cost:  0.5710153 test_acc:  0.8125\n",
            "iter:  502 train_cost:  0.8219385 train_acc:  0.7578125 test_cost:  0.5983385 test_acc:  0.8125\n",
            "iter:  503 train_cost:  0.6788538 train_acc:  0.7734375 test_cost:  0.8083727 test_acc:  0.734375\n",
            "iter:  504 train_cost:  0.80738485 train_acc:  0.7421875 test_cost:  0.5454961 test_acc:  0.8203125\n",
            "iter:  505 train_cost:  0.8979847 train_acc:  0.6875 test_cost:  0.6853063 test_acc:  0.78125\n",
            "iter:  506 train_cost:  0.5981303 train_acc:  0.8203125 test_cost:  0.6234705 test_acc:  0.765625\n",
            "iter:  507 train_cost:  0.81368667 train_acc:  0.765625 test_cost:  0.60618895 test_acc:  0.8515625\n",
            "iter:  508 train_cost:  0.70139605 train_acc:  0.71875 test_cost:  0.59503776 test_acc:  0.84375\n",
            "iter:  509 train_cost:  0.72681826 train_acc:  0.78125 test_cost:  0.81888103 test_acc:  0.703125\n",
            "iter:  510 train_cost:  0.66600835 train_acc:  0.75 test_cost:  0.6197116 test_acc:  0.7890625\n",
            "iter:  511 train_cost:  0.6229735 train_acc:  0.8125 test_cost:  0.6316701 test_acc:  0.8203125\n",
            "iter:  512 train_cost:  0.77269185 train_acc:  0.75 test_cost:  0.5742122 test_acc:  0.8203125\n",
            "iter:  513 train_cost:  0.6176178 train_acc:  0.7734375 test_cost:  0.6532402 test_acc:  0.75\n",
            "iter:  514 train_cost:  0.68321645 train_acc:  0.7890625 test_cost:  0.805262 test_acc:  0.75\n",
            "iter:  515 train_cost:  0.67759675 train_acc:  0.7890625 test_cost:  0.67735946 test_acc:  0.7890625\n",
            "iter:  516 train_cost:  0.6020503 train_acc:  0.8359375 test_cost:  0.47453538 test_acc:  0.84375\n",
            "iter:  517 train_cost:  0.63696575 train_acc:  0.8125 test_cost:  0.7075636 test_acc:  0.78125\n",
            "iter:  518 train_cost:  0.9190821 train_acc:  0.6796875 test_cost:  0.5561935 test_acc:  0.8125\n",
            "iter:  519 train_cost:  0.5139046 train_acc:  0.8359375 test_cost:  0.9879352 test_acc:  0.6640625\n",
            "iter:  520 train_cost:  0.46674547 train_acc:  0.859375 test_cost:  0.71293855 test_acc:  0.765625\n",
            "iter:  521 train_cost:  0.7347925 train_acc:  0.75 test_cost:  0.7218246 test_acc:  0.796875\n",
            "iter:  522 train_cost:  0.6770207 train_acc:  0.71875 test_cost:  0.7086915 test_acc:  0.7734375\n",
            "iter:  523 train_cost:  0.53163505 train_acc:  0.828125 test_cost:  0.7923503 test_acc:  0.7265625\n",
            "iter:  524 train_cost:  0.5370207 train_acc:  0.8359375 test_cost:  0.69723916 test_acc:  0.75\n",
            "iter:  525 train_cost:  0.5881002 train_acc:  0.7890625 test_cost:  0.7276505 test_acc:  0.7578125\n",
            "iter:  526 train_cost:  0.85954285 train_acc:  0.7421875 test_cost:  0.6636814 test_acc:  0.8046875\n",
            "iter:  527 train_cost:  0.6356978 train_acc:  0.7890625 test_cost:  0.6338892 test_acc:  0.8046875\n",
            "iter:  528 train_cost:  0.5409479 train_acc:  0.8203125 test_cost:  0.711249 test_acc:  0.7578125\n",
            "iter:  529 train_cost:  0.61047745 train_acc:  0.828125 test_cost:  0.755659 test_acc:  0.71875\n",
            "iter:  530 train_cost:  0.59394276 train_acc:  0.7578125 test_cost:  0.7473501 test_acc:  0.734375\n",
            "iter:  531 train_cost:  0.5425147 train_acc:  0.8046875 test_cost:  0.56322443 test_acc:  0.7578125\n",
            "iter:  532 train_cost:  0.70066714 train_acc:  0.7421875 test_cost:  0.7911411 test_acc:  0.765625\n",
            "iter:  533 train_cost:  0.56393874 train_acc:  0.8046875 test_cost:  0.6995341 test_acc:  0.828125\n",
            "iter:  534 train_cost:  0.5359254 train_acc:  0.8125 test_cost:  0.54160595 test_acc:  0.8359375\n",
            "iter:  535 train_cost:  0.6217623 train_acc:  0.8359375 test_cost:  0.8014856 test_acc:  0.7734375\n",
            "iter:  536 train_cost:  0.6187271 train_acc:  0.8203125 test_cost:  0.69646627 test_acc:  0.734375\n",
            "iter:  537 train_cost:  0.61158276 train_acc:  0.8046875 test_cost:  0.50767946 test_acc:  0.84375\n",
            "iter:  538 train_cost:  0.6702833 train_acc:  0.8125 test_cost:  0.68931353 test_acc:  0.8046875\n",
            "iter:  539 train_cost:  0.75172645 train_acc:  0.8125 test_cost:  0.63082635 test_acc:  0.78125\n",
            "iter:  540 train_cost:  0.67589426 train_acc:  0.765625 test_cost:  0.5367778 test_acc:  0.7578125\n",
            "iter:  541 train_cost:  0.7139844 train_acc:  0.765625 test_cost:  0.49708456 test_acc:  0.8359375\n",
            "iter:  542 train_cost:  0.6811982 train_acc:  0.828125 test_cost:  0.5275676 test_acc:  0.8359375\n",
            "iter:  543 train_cost:  0.53102326 train_acc:  0.8125 test_cost:  0.4943751 test_acc:  0.8046875\n",
            "iter:  544 train_cost:  0.56660753 train_acc:  0.828125 test_cost:  0.5914271 test_acc:  0.8203125\n",
            "iter:  545 train_cost:  0.75516367 train_acc:  0.7734375 test_cost:  0.62868524 test_acc:  0.8203125\n",
            "iter:  546 train_cost:  0.6661135 train_acc:  0.78125 test_cost:  0.69180024 test_acc:  0.765625\n",
            "iter:  547 train_cost:  0.7621284 train_acc:  0.78125 test_cost:  0.8379806 test_acc:  0.765625\n",
            "iter:  548 train_cost:  0.9418289 train_acc:  0.734375 test_cost:  0.5740471 test_acc:  0.796875\n",
            "iter:  549 train_cost:  0.8190286 train_acc:  0.7890625 test_cost:  0.73232853 test_acc:  0.7890625\n",
            "iter:  550 train_cost:  0.6516535 train_acc:  0.78125 test_cost:  0.42819673 test_acc:  0.859375\n",
            "iter:  551 train_cost:  0.73173267 train_acc:  0.765625 test_cost:  0.4354838 test_acc:  0.875\n",
            "iter:  552 train_cost:  0.5356953 train_acc:  0.8125 test_cost:  0.5805149 test_acc:  0.8203125\n",
            "iter:  553 train_cost:  0.64747626 train_acc:  0.8125 test_cost:  0.7018012 test_acc:  0.7734375\n",
            "iter:  554 train_cost:  0.62418133 train_acc:  0.828125 test_cost:  0.9330132 test_acc:  0.7265625\n",
            "iter:  555 train_cost:  0.6021209 train_acc:  0.7578125 test_cost:  0.8109732 test_acc:  0.71875\n",
            "iter:  556 train_cost:  0.75032246 train_acc:  0.7890625 test_cost:  0.6391944 test_acc:  0.7734375\n",
            "iter:  557 train_cost:  0.62373847 train_acc:  0.859375 test_cost:  0.6321406 test_acc:  0.78125\n",
            "iter:  558 train_cost:  0.663695 train_acc:  0.8046875 test_cost:  0.7326913 test_acc:  0.78125\n",
            "iter:  559 train_cost:  0.6960008 train_acc:  0.734375 test_cost:  0.6748574 test_acc:  0.7734375\n",
            "iter:  560 train_cost:  0.6061431 train_acc:  0.828125 test_cost:  0.75900286 test_acc:  0.7578125\n",
            "iter:  561 train_cost:  0.66038376 train_acc:  0.8046875 test_cost:  0.59739774 test_acc:  0.8125\n",
            "iter:  562 train_cost:  0.61503136 train_acc:  0.8125 test_cost:  0.8553859 test_acc:  0.7734375\n",
            "iter:  563 train_cost:  0.7154982 train_acc:  0.7890625 test_cost:  0.44018096 test_acc:  0.859375\n",
            "iter:  564 train_cost:  0.62219214 train_acc:  0.8203125 test_cost:  0.5400884 test_acc:  0.8515625\n",
            "iter:  565 train_cost:  0.47300977 train_acc:  0.828125 test_cost:  0.49248156 test_acc:  0.84375\n",
            "iter:  566 train_cost:  0.46798325 train_acc:  0.875 test_cost:  0.7072876 test_acc:  0.7734375\n",
            "iter:  567 train_cost:  0.65996575 train_acc:  0.8125 test_cost:  0.59940326 test_acc:  0.796875\n",
            "iter:  568 train_cost:  0.41007608 train_acc:  0.8828125 test_cost:  0.6810764 test_acc:  0.7734375\n",
            "iter:  569 train_cost:  0.71573627 train_acc:  0.7890625 test_cost:  0.4650674 test_acc:  0.859375\n",
            "iter:  570 train_cost:  0.7981314 train_acc:  0.734375 test_cost:  0.80100465 test_acc:  0.7421875\n",
            "iter:  571 train_cost:  0.64644146 train_acc:  0.8203125 test_cost:  0.6675292 test_acc:  0.7890625\n",
            "iter:  572 train_cost:  0.6311111 train_acc:  0.8046875 test_cost:  0.575479 test_acc:  0.8046875\n",
            "iter:  573 train_cost:  0.8036335 train_acc:  0.7265625 test_cost:  0.7646618 test_acc:  0.7890625\n",
            "iter:  574 train_cost:  0.8298431 train_acc:  0.7578125 test_cost:  0.53895104 test_acc:  0.8203125\n",
            "iter:  575 train_cost:  0.6640081 train_acc:  0.7734375 test_cost:  0.5380064 test_acc:  0.796875\n",
            "iter:  576 train_cost:  0.7107371 train_acc:  0.765625 test_cost:  0.65215814 test_acc:  0.8125\n",
            "iter:  577 train_cost:  0.51514363 train_acc:  0.8125 test_cost:  0.7773751 test_acc:  0.765625\n",
            "iter:  578 train_cost:  0.7427844 train_acc:  0.8125 test_cost:  0.51523954 test_acc:  0.828125\n",
            "iter:  579 train_cost:  0.5288068 train_acc:  0.8359375 test_cost:  0.5020044 test_acc:  0.8515625\n",
            "iter:  580 train_cost:  0.42102575 train_acc:  0.8828125 test_cost:  0.82498896 test_acc:  0.75\n",
            "iter:  581 train_cost:  0.69013655 train_acc:  0.796875 test_cost:  0.5620627 test_acc:  0.8203125\n",
            "iter:  582 train_cost:  0.56668025 train_acc:  0.8203125 test_cost:  0.62361526 test_acc:  0.8046875\n",
            "iter:  583 train_cost:  0.7372918 train_acc:  0.78125 test_cost:  0.7741218 test_acc:  0.7734375\n",
            "iter:  584 train_cost:  0.65730244 train_acc:  0.765625 test_cost:  0.57654476 test_acc:  0.8046875\n",
            "iter:  585 train_cost:  0.6753429 train_acc:  0.796875 test_cost:  0.53702694 test_acc:  0.8046875\n",
            "iter:  586 train_cost:  0.64771616 train_acc:  0.7890625 test_cost:  0.54106224 test_acc:  0.8359375\n",
            "iter:  587 train_cost:  0.45740706 train_acc:  0.8359375 test_cost:  0.5563935 test_acc:  0.8671875\n",
            "iter:  588 train_cost:  0.53986233 train_acc:  0.8359375 test_cost:  0.53791875 test_acc:  0.8046875\n",
            "iter:  589 train_cost:  0.49990103 train_acc:  0.84375 test_cost:  0.6476641 test_acc:  0.8125\n",
            "iter:  590 train_cost:  0.62010217 train_acc:  0.8359375 test_cost:  0.573057 test_acc:  0.8203125\n",
            "iter:  591 train_cost:  0.7399211 train_acc:  0.7890625 test_cost:  0.65427035 test_acc:  0.828125\n",
            "iter:  592 train_cost:  0.5780831 train_acc:  0.765625 test_cost:  0.66633177 test_acc:  0.8125\n",
            "iter:  593 train_cost:  0.681921 train_acc:  0.7734375 test_cost:  0.72722626 test_acc:  0.78125\n",
            "iter:  594 train_cost:  0.5231993 train_acc:  0.8359375 test_cost:  0.65632313 test_acc:  0.8046875\n",
            "iter:  595 train_cost:  0.5111747 train_acc:  0.8359375 test_cost:  0.41077673 test_acc:  0.875\n",
            "iter:  596 train_cost:  0.690132 train_acc:  0.78125 test_cost:  0.58056855 test_acc:  0.859375\n",
            "iter:  597 train_cost:  0.5568799 train_acc:  0.8359375 test_cost:  0.6546154 test_acc:  0.8046875\n",
            "iter:  598 train_cost:  0.57914585 train_acc:  0.8046875 test_cost:  0.64625007 test_acc:  0.78125\n",
            "iter:  599 train_cost:  0.45279408 train_acc:  0.8359375 test_cost:  0.5286777 test_acc:  0.8515625\n",
            "iter:  600 train_cost:  0.6666474 train_acc:  0.7890625 test_cost:  0.5321656 test_acc:  0.78125\n",
            "iter:  601 train_cost:  0.669023 train_acc:  0.75 test_cost:  0.75718135 test_acc:  0.75\n",
            "iter:  602 train_cost:  0.7040932 train_acc:  0.8125 test_cost:  0.7146293 test_acc:  0.765625\n",
            "iter:  603 train_cost:  0.59787583 train_acc:  0.7890625 test_cost:  0.5800824 test_acc:  0.796875\n",
            "iter:  604 train_cost:  0.5522779 train_acc:  0.8125 test_cost:  0.5654498 test_acc:  0.8125\n",
            "iter:  605 train_cost:  0.6254893 train_acc:  0.828125 test_cost:  0.6530996 test_acc:  0.8125\n",
            "iter:  606 train_cost:  0.57581323 train_acc:  0.8203125 test_cost:  0.6048633 test_acc:  0.8359375\n",
            "iter:  607 train_cost:  0.6979473 train_acc:  0.7734375 test_cost:  0.63809866 test_acc:  0.796875\n",
            "iter:  608 train_cost:  0.4957478 train_acc:  0.875 test_cost:  0.6214019 test_acc:  0.8203125\n",
            "iter:  609 train_cost:  0.6566038 train_acc:  0.78125 test_cost:  0.61295784 test_acc:  0.78125\n",
            "iter:  610 train_cost:  0.78272814 train_acc:  0.78125 test_cost:  0.69326067 test_acc:  0.78125\n",
            "iter:  611 train_cost:  0.7314582 train_acc:  0.78125 test_cost:  0.65630555 test_acc:  0.8359375\n",
            "iter:  612 train_cost:  0.4796937 train_acc:  0.8359375 test_cost:  0.6259601 test_acc:  0.78125\n",
            "iter:  613 train_cost:  0.58317196 train_acc:  0.8203125 test_cost:  0.572381 test_acc:  0.7890625\n",
            "iter:  614 train_cost:  0.54589075 train_acc:  0.8359375 test_cost:  0.5408432 test_acc:  0.8203125\n",
            "iter:  615 train_cost:  0.48762432 train_acc:  0.875 test_cost:  0.59897935 test_acc:  0.8125\n",
            "iter:  616 train_cost:  0.7525749 train_acc:  0.765625 test_cost:  0.57974476 test_acc:  0.8203125\n",
            "iter:  617 train_cost:  0.5885287 train_acc:  0.8125 test_cost:  0.47980726 test_acc:  0.875\n",
            "iter:  618 train_cost:  0.5627657 train_acc:  0.796875 test_cost:  0.71912086 test_acc:  0.8046875\n",
            "iter:  619 train_cost:  0.4289803 train_acc:  0.84375 test_cost:  0.61349 test_acc:  0.796875\n",
            "iter:  620 train_cost:  0.73774135 train_acc:  0.8203125 test_cost:  0.48189518 test_acc:  0.8203125\n",
            "iter:  621 train_cost:  0.6191192 train_acc:  0.828125 test_cost:  0.69367397 test_acc:  0.78125\n",
            "iter:  622 train_cost:  0.6026279 train_acc:  0.796875 test_cost:  0.7224549 test_acc:  0.765625\n",
            "iter:  623 train_cost:  0.50187814 train_acc:  0.84375 test_cost:  0.44548893 test_acc:  0.859375\n",
            "iter:  624 train_cost:  0.6933642 train_acc:  0.828125 test_cost:  0.529184 test_acc:  0.8359375\n",
            "iter:  625 train_cost:  0.4719901 train_acc:  0.8046875 test_cost:  0.53639144 test_acc:  0.84375\n",
            "iter:  626 train_cost:  0.4902823 train_acc:  0.8515625 test_cost:  0.53704786 test_acc:  0.8203125\n",
            "iter:  627 train_cost:  0.7052399 train_acc:  0.78125 test_cost:  0.4535904 test_acc:  0.8515625\n",
            "iter:  628 train_cost:  0.541892 train_acc:  0.828125 test_cost:  0.5075662 test_acc:  0.8203125\n",
            "iter:  629 train_cost:  0.70478725 train_acc:  0.7890625 test_cost:  0.82475907 test_acc:  0.7578125\n",
            "iter:  630 train_cost:  0.69183815 train_acc:  0.7734375 test_cost:  0.6118394 test_acc:  0.8046875\n",
            "iter:  631 train_cost:  0.4475175 train_acc:  0.8515625 test_cost:  0.62058926 test_acc:  0.7734375\n",
            "iter:  632 train_cost:  0.5886309 train_acc:  0.7890625 test_cost:  0.5249679 test_acc:  0.8203125\n",
            "iter:  633 train_cost:  0.675794 train_acc:  0.7890625 test_cost:  0.6716395 test_acc:  0.8046875\n",
            "iter:  634 train_cost:  0.6644342 train_acc:  0.765625 test_cost:  0.74817514 test_acc:  0.828125\n",
            "iter:  635 train_cost:  0.53015566 train_acc:  0.8203125 test_cost:  0.65192354 test_acc:  0.7890625\n",
            "iter:  636 train_cost:  0.6996079 train_acc:  0.7890625 test_cost:  0.5161254 test_acc:  0.859375\n",
            "iter:  637 train_cost:  0.5143777 train_acc:  0.8203125 test_cost:  0.6520263 test_acc:  0.7890625\n",
            "iter:  638 train_cost:  0.63640624 train_acc:  0.8046875 test_cost:  0.6398592 test_acc:  0.7890625\n",
            "iter:  639 train_cost:  0.53498393 train_acc:  0.828125 test_cost:  0.7058216 test_acc:  0.765625\n",
            "iter:  640 train_cost:  0.5941514 train_acc:  0.8125 test_cost:  0.41602165 test_acc:  0.890625\n",
            "iter:  641 train_cost:  0.69279945 train_acc:  0.8359375 test_cost:  0.5483182 test_acc:  0.8125\n",
            "iter:  642 train_cost:  0.6052865 train_acc:  0.8203125 test_cost:  0.4889603 test_acc:  0.8203125\n",
            "iter:  643 train_cost:  0.5906745 train_acc:  0.8203125 test_cost:  0.5349467 test_acc:  0.8359375\n",
            "iter:  644 train_cost:  0.6838276 train_acc:  0.78125 test_cost:  0.5910698 test_acc:  0.8203125\n",
            "iter:  645 train_cost:  0.59447336 train_acc:  0.8359375 test_cost:  0.68998444 test_acc:  0.7890625\n",
            "iter:  646 train_cost:  0.52338487 train_acc:  0.84375 test_cost:  0.858524 test_acc:  0.734375\n",
            "iter:  647 train_cost:  0.54640585 train_acc:  0.8125 test_cost:  0.62721974 test_acc:  0.8125\n",
            "iter:  648 train_cost:  0.60414654 train_acc:  0.796875 test_cost:  0.515723 test_acc:  0.8359375\n",
            "iter:  649 train_cost:  0.66943526 train_acc:  0.7734375 test_cost:  0.40852582 test_acc:  0.859375\n",
            "iter:  650 train_cost:  0.79031754 train_acc:  0.7734375 test_cost:  0.67860496 test_acc:  0.7890625\n",
            "iter:  651 train_cost:  0.64288956 train_acc:  0.8125 test_cost:  0.47601795 test_acc:  0.84375\n",
            "iter:  652 train_cost:  0.59694767 train_acc:  0.796875 test_cost:  0.6337899 test_acc:  0.828125\n",
            "iter:  653 train_cost:  0.7201702 train_acc:  0.78125 test_cost:  0.476978 test_acc:  0.8671875\n",
            "iter:  654 train_cost:  0.55956596 train_acc:  0.8203125 test_cost:  0.58374846 test_acc:  0.8359375\n",
            "iter:  655 train_cost:  0.5590293 train_acc:  0.7890625 test_cost:  0.57200307 test_acc:  0.7890625\n",
            "iter:  656 train_cost:  0.51854604 train_acc:  0.8203125 test_cost:  0.65734327 test_acc:  0.796875\n",
            "iter:  657 train_cost:  0.60733116 train_acc:  0.8046875 test_cost:  0.48290393 test_acc:  0.84375\n",
            "iter:  658 train_cost:  0.5608208 train_acc:  0.8515625 test_cost:  0.4381716 test_acc:  0.8828125\n",
            "iter:  659 train_cost:  0.68626666 train_acc:  0.796875 test_cost:  0.52266836 test_acc:  0.84375\n",
            "iter:  660 train_cost:  0.5711401 train_acc:  0.84375 test_cost:  0.47632456 test_acc:  0.8359375\n",
            "iter:  661 train_cost:  0.57186615 train_acc:  0.7734375 test_cost:  0.54096115 test_acc:  0.875\n",
            "iter:  662 train_cost:  0.58744365 train_acc:  0.8203125 test_cost:  0.56869835 test_acc:  0.828125\n",
            "iter:  663 train_cost:  0.62910855 train_acc:  0.78125 test_cost:  0.5575911 test_acc:  0.8125\n",
            "iter:  664 train_cost:  0.58856356 train_acc:  0.8359375 test_cost:  0.6654841 test_acc:  0.7734375\n",
            "iter:  665 train_cost:  0.5627731 train_acc:  0.828125 test_cost:  0.44948918 test_acc:  0.84375\n",
            "iter:  666 train_cost:  0.6724234 train_acc:  0.796875 test_cost:  0.5251682 test_acc:  0.8046875\n",
            "iter:  667 train_cost:  0.71399575 train_acc:  0.796875 test_cost:  0.4742301 test_acc:  0.828125\n",
            "iter:  668 train_cost:  0.581895 train_acc:  0.828125 test_cost:  0.5461377 test_acc:  0.828125\n",
            "iter:  669 train_cost:  0.5141102 train_acc:  0.828125 test_cost:  0.55536914 test_acc:  0.828125\n",
            "iter:  670 train_cost:  0.5820698 train_acc:  0.8203125 test_cost:  0.6113926 test_acc:  0.8046875\n",
            "iter:  671 train_cost:  0.60713136 train_acc:  0.796875 test_cost:  0.41442078 test_acc:  0.8671875\n",
            "iter:  672 train_cost:  0.5455417 train_acc:  0.8046875 test_cost:  0.70931107 test_acc:  0.8046875\n",
            "iter:  673 train_cost:  0.6671533 train_acc:  0.8203125 test_cost:  0.52091485 test_acc:  0.8203125\n",
            "iter:  674 train_cost:  0.7580335 train_acc:  0.7734375 test_cost:  0.6374233 test_acc:  0.7890625\n",
            "iter:  675 train_cost:  0.6615083 train_acc:  0.7734375 test_cost:  0.53326726 test_acc:  0.8125\n",
            "iter:  676 train_cost:  0.49659577 train_acc:  0.84375 test_cost:  0.47317624 test_acc:  0.859375\n",
            "iter:  677 train_cost:  0.59028745 train_acc:  0.8359375 test_cost:  0.5547577 test_acc:  0.8359375\n",
            "iter:  678 train_cost:  0.45068878 train_acc:  0.8515625 test_cost:  0.48139608 test_acc:  0.8125\n",
            "iter:  679 train_cost:  0.46534494 train_acc:  0.8359375 test_cost:  0.39436886 test_acc:  0.8828125\n",
            "iter:  680 train_cost:  0.53819764 train_acc:  0.828125 test_cost:  0.6347785 test_acc:  0.8046875\n",
            "iter:  681 train_cost:  0.5116253 train_acc:  0.8515625 test_cost:  0.5979215 test_acc:  0.78125\n",
            "iter:  682 train_cost:  0.5587121 train_acc:  0.8125 test_cost:  0.6837082 test_acc:  0.7890625\n",
            "iter:  683 train_cost:  0.67432165 train_acc:  0.78125 test_cost:  0.4423524 test_acc:  0.828125\n",
            "iter:  684 train_cost:  0.6704331 train_acc:  0.78125 test_cost:  0.55312324 test_acc:  0.8671875\n",
            "iter:  685 train_cost:  0.5369228 train_acc:  0.8046875 test_cost:  0.65390146 test_acc:  0.78125\n",
            "iter:  686 train_cost:  0.48568845 train_acc:  0.8515625 test_cost:  0.6276797 test_acc:  0.8515625\n",
            "iter:  687 train_cost:  0.6113609 train_acc:  0.84375 test_cost:  0.72251356 test_acc:  0.7890625\n",
            "iter:  688 train_cost:  0.7201649 train_acc:  0.796875 test_cost:  0.74058884 test_acc:  0.765625\n",
            "iter:  689 train_cost:  0.59852093 train_acc:  0.8125 test_cost:  0.4811445 test_acc:  0.8671875\n",
            "iter:  690 train_cost:  0.56953835 train_acc:  0.7890625 test_cost:  0.58893293 test_acc:  0.8125\n",
            "iter:  691 train_cost:  0.70401394 train_acc:  0.7265625 test_cost:  0.540185 test_acc:  0.8515625\n",
            "iter:  692 train_cost:  0.65686196 train_acc:  0.7890625 test_cost:  0.5418439 test_acc:  0.8203125\n",
            "iter:  693 train_cost:  0.5761085 train_acc:  0.7734375 test_cost:  0.8359821 test_acc:  0.78125\n",
            "iter:  694 train_cost:  0.6837397 train_acc:  0.796875 test_cost:  0.6486469 test_acc:  0.796875\n",
            "iter:  695 train_cost:  0.65887403 train_acc:  0.765625 test_cost:  0.5462034 test_acc:  0.8359375\n",
            "iter:  696 train_cost:  0.51546645 train_acc:  0.8203125 test_cost:  0.6427059 test_acc:  0.78125\n",
            "iter:  697 train_cost:  0.6585302 train_acc:  0.8046875 test_cost:  0.5876604 test_acc:  0.7890625\n",
            "iter:  698 train_cost:  0.5609912 train_acc:  0.859375 test_cost:  0.67579037 test_acc:  0.8125\n",
            "iter:  699 train_cost:  0.84352404 train_acc:  0.734375 test_cost:  0.6377007 test_acc:  0.796875\n",
            "iter:  700 train_cost:  0.53811365 train_acc:  0.796875 test_cost:  0.55114406 test_acc:  0.765625\n",
            "iter:  701 train_cost:  0.5938574 train_acc:  0.7734375 test_cost:  0.6271812 test_acc:  0.78125\n",
            "iter:  702 train_cost:  0.6675689 train_acc:  0.7890625 test_cost:  0.56664824 test_acc:  0.8046875\n",
            "iter:  703 train_cost:  0.4575717 train_acc:  0.859375 test_cost:  0.55901265 test_acc:  0.8515625\n",
            "iter:  704 train_cost:  0.64701736 train_acc:  0.8203125 test_cost:  0.4785239 test_acc:  0.828125\n",
            "iter:  705 train_cost:  0.5258142 train_acc:  0.828125 test_cost:  0.56689024 test_acc:  0.8046875\n",
            "iter:  706 train_cost:  0.67438835 train_acc:  0.8125 test_cost:  0.4436162 test_acc:  0.8515625\n",
            "iter:  707 train_cost:  0.55403835 train_acc:  0.8203125 test_cost:  0.43347773 test_acc:  0.8359375\n",
            "iter:  708 train_cost:  0.55697143 train_acc:  0.828125 test_cost:  0.69891524 test_acc:  0.78125\n",
            "iter:  709 train_cost:  0.5271096 train_acc:  0.796875 test_cost:  0.9154203 test_acc:  0.7734375\n",
            "iter:  710 train_cost:  0.47487718 train_acc:  0.859375 test_cost:  0.53390884 test_acc:  0.84375\n",
            "iter:  711 train_cost:  0.6232481 train_acc:  0.796875 test_cost:  0.6856427 test_acc:  0.8125\n",
            "iter:  712 train_cost:  0.6111362 train_acc:  0.8203125 test_cost:  0.37036955 test_acc:  0.890625\n",
            "iter:  713 train_cost:  0.8124467 train_acc:  0.7265625 test_cost:  0.43822262 test_acc:  0.875\n",
            "iter:  714 train_cost:  0.6779365 train_acc:  0.7734375 test_cost:  0.7384573 test_acc:  0.75\n",
            "iter:  715 train_cost:  0.43717077 train_acc:  0.8515625 test_cost:  0.5639101 test_acc:  0.8046875\n",
            "iter:  716 train_cost:  0.56114364 train_acc:  0.796875 test_cost:  0.52155024 test_acc:  0.859375\n",
            "iter:  717 train_cost:  0.6102428 train_acc:  0.8203125 test_cost:  0.5257156 test_acc:  0.8359375\n",
            "iter:  718 train_cost:  0.5385712 train_acc:  0.8203125 test_cost:  0.6295536 test_acc:  0.796875\n",
            "iter:  719 train_cost:  0.63062507 train_acc:  0.8203125 test_cost:  0.5757635 test_acc:  0.8359375\n",
            "iter:  720 train_cost:  0.45635396 train_acc:  0.859375 test_cost:  0.5291689 test_acc:  0.78125\n",
            "iter:  721 train_cost:  0.559266 train_acc:  0.828125 test_cost:  0.6822109 test_acc:  0.765625\n",
            "iter:  722 train_cost:  0.43643227 train_acc:  0.8671875 test_cost:  0.4626245 test_acc:  0.84375\n",
            "iter:  723 train_cost:  0.68993175 train_acc:  0.828125 test_cost:  0.627735 test_acc:  0.8125\n",
            "iter:  724 train_cost:  0.43532825 train_acc:  0.875 test_cost:  0.6649406 test_acc:  0.7890625\n",
            "iter:  725 train_cost:  0.65764606 train_acc:  0.7421875 test_cost:  0.61687535 test_acc:  0.7734375\n",
            "iter:  726 train_cost:  0.54614276 train_acc:  0.84375 test_cost:  0.5477833 test_acc:  0.828125\n",
            "iter:  727 train_cost:  0.44507426 train_acc:  0.84375 test_cost:  0.3907061 test_acc:  0.8671875\n",
            "iter:  728 train_cost:  0.42911047 train_acc:  0.8359375 test_cost:  0.55334485 test_acc:  0.875\n",
            "iter:  729 train_cost:  0.479291 train_acc:  0.84375 test_cost:  0.47043672 test_acc:  0.859375\n",
            "iter:  730 train_cost:  0.78875303 train_acc:  0.7578125 test_cost:  0.4664923 test_acc:  0.875\n",
            "iter:  731 train_cost:  0.6556039 train_acc:  0.8046875 test_cost:  0.542741 test_acc:  0.828125\n",
            "iter:  732 train_cost:  0.5313061 train_acc:  0.8359375 test_cost:  0.5048313 test_acc:  0.8359375\n",
            "iter:  733 train_cost:  0.6188135 train_acc:  0.7578125 test_cost:  0.6176493 test_acc:  0.7578125\n",
            "iter:  734 train_cost:  0.47637412 train_acc:  0.8359375 test_cost:  0.47776455 test_acc:  0.8515625\n",
            "iter:  735 train_cost:  0.65740186 train_acc:  0.8046875 test_cost:  0.5723783 test_acc:  0.84375\n",
            "iter:  736 train_cost:  0.669229 train_acc:  0.8125 test_cost:  0.76891756 test_acc:  0.796875\n",
            "iter:  737 train_cost:  0.54061157 train_acc:  0.8203125 test_cost:  0.44315094 test_acc:  0.8359375\n",
            "iter:  738 train_cost:  0.44485497 train_acc:  0.8984375 test_cost:  0.7210154 test_acc:  0.7734375\n",
            "iter:  739 train_cost:  0.64468145 train_acc:  0.7734375 test_cost:  0.37899953 test_acc:  0.8828125\n",
            "iter:  740 train_cost:  0.69563425 train_acc:  0.765625 test_cost:  0.5081264 test_acc:  0.8359375\n",
            "iter:  741 train_cost:  0.64120185 train_acc:  0.7890625 test_cost:  0.5457687 test_acc:  0.8515625\n",
            "iter:  742 train_cost:  0.46601206 train_acc:  0.84375 test_cost:  0.46660563 test_acc:  0.8515625\n",
            "iter:  743 train_cost:  0.53510344 train_acc:  0.8203125 test_cost:  0.52837545 test_acc:  0.828125\n",
            "iter:  744 train_cost:  0.57732123 train_acc:  0.8125 test_cost:  0.46462986 test_acc:  0.8828125\n",
            "iter:  745 train_cost:  0.6222135 train_acc:  0.78125 test_cost:  0.5608476 test_acc:  0.859375\n",
            "iter:  746 train_cost:  0.5847417 train_acc:  0.875 test_cost:  0.48295528 test_acc:  0.8046875\n",
            "iter:  747 train_cost:  0.62987757 train_acc:  0.8203125 test_cost:  0.5096048 test_acc:  0.8203125\n",
            "iter:  748 train_cost:  0.6869826 train_acc:  0.78125 test_cost:  0.5311398 test_acc:  0.8515625\n",
            "iter:  749 train_cost:  0.3376008 train_acc:  0.9140625 test_cost:  0.62160385 test_acc:  0.8203125\n",
            "iter:  750 train_cost:  0.5078542 train_acc:  0.828125 test_cost:  0.36548916 test_acc:  0.8828125\n",
            "iter:  751 train_cost:  0.58696353 train_acc:  0.8046875 test_cost:  0.7073546 test_acc:  0.8203125\n",
            "iter:  752 train_cost:  0.56438804 train_acc:  0.828125 test_cost:  0.5546619 test_acc:  0.78125\n",
            "iter:  753 train_cost:  0.47198263 train_acc:  0.859375 test_cost:  0.41561902 test_acc:  0.859375\n",
            "iter:  754 train_cost:  0.56956255 train_acc:  0.7890625 test_cost:  0.5645928 test_acc:  0.828125\n",
            "iter:  755 train_cost:  0.45981383 train_acc:  0.859375 test_cost:  0.6463287 test_acc:  0.78125\n",
            "iter:  756 train_cost:  0.48027256 train_acc:  0.8671875 test_cost:  0.4069405 test_acc:  0.8359375\n",
            "iter:  757 train_cost:  0.45073527 train_acc:  0.8359375 test_cost:  0.3687717 test_acc:  0.8671875\n",
            "iter:  758 train_cost:  0.5515873 train_acc:  0.8203125 test_cost:  0.49417728 test_acc:  0.8515625\n",
            "iter:  759 train_cost:  0.4907737 train_acc:  0.8515625 test_cost:  0.62476075 test_acc:  0.7734375\n",
            "iter:  760 train_cost:  0.3342179 train_acc:  0.8828125 test_cost:  0.5735588 test_acc:  0.828125\n",
            "iter:  761 train_cost:  0.5485559 train_acc:  0.859375 test_cost:  0.5685019 test_acc:  0.8125\n",
            "iter:  762 train_cost:  0.4233228 train_acc:  0.890625 test_cost:  0.53746986 test_acc:  0.84375\n",
            "iter:  763 train_cost:  0.5506946 train_acc:  0.84375 test_cost:  0.54091454 test_acc:  0.8671875\n",
            "iter:  764 train_cost:  0.51518226 train_acc:  0.8359375 test_cost:  0.43785107 test_acc:  0.8515625\n",
            "iter:  765 train_cost:  0.599699 train_acc:  0.8046875 test_cost:  0.6276866 test_acc:  0.8125\n",
            "iter:  766 train_cost:  0.5926594 train_acc:  0.8203125 test_cost:  0.53325975 test_acc:  0.796875\n",
            "iter:  767 train_cost:  0.48360953 train_acc:  0.859375 test_cost:  0.43957868 test_acc:  0.8515625\n",
            "iter:  768 train_cost:  0.49341974 train_acc:  0.828125 test_cost:  0.7073463 test_acc:  0.765625\n",
            "iter:  769 train_cost:  0.6889211 train_acc:  0.796875 test_cost:  0.34611446 test_acc:  0.8984375\n",
            "iter:  770 train_cost:  0.3633388 train_acc:  0.890625 test_cost:  0.48682812 test_acc:  0.8125\n",
            "iter:  771 train_cost:  0.37170288 train_acc:  0.8828125 test_cost:  0.53563446 test_acc:  0.8203125\n",
            "iter:  772 train_cost:  0.50144225 train_acc:  0.84375 test_cost:  0.4809765 test_acc:  0.796875\n",
            "iter:  773 train_cost:  0.66554487 train_acc:  0.8203125 test_cost:  0.4032202 test_acc:  0.84375\n",
            "iter:  774 train_cost:  0.37211356 train_acc:  0.875 test_cost:  0.71266514 test_acc:  0.7734375\n",
            "iter:  775 train_cost:  0.6031748 train_acc:  0.8046875 test_cost:  0.6929975 test_acc:  0.8046875\n",
            "iter:  776 train_cost:  0.6234707 train_acc:  0.828125 test_cost:  0.4960281 test_acc:  0.859375\n",
            "iter:  777 train_cost:  0.49510875 train_acc:  0.796875 test_cost:  0.5779403 test_acc:  0.8359375\n",
            "iter:  778 train_cost:  0.45855373 train_acc:  0.84375 test_cost:  0.5189494 test_acc:  0.875\n",
            "iter:  779 train_cost:  0.5172024 train_acc:  0.8359375 test_cost:  0.48796022 test_acc:  0.84375\n",
            "iter:  780 train_cost:  0.5246138 train_acc:  0.8359375 test_cost:  0.54438287 test_acc:  0.8203125\n",
            "iter:  781 train_cost:  0.5257917 train_acc:  0.8203125 test_cost:  0.5005534 test_acc:  0.8671875\n",
            "iter:  782 train_cost:  0.63965654 train_acc:  0.796875 test_cost:  0.52276605 test_acc:  0.8359375\n",
            "iter:  783 train_cost:  0.4558087 train_acc:  0.8671875 test_cost:  0.32645956 test_acc:  0.8828125\n",
            "iter:  784 train_cost:  0.48535168 train_acc:  0.8046875 test_cost:  0.44813067 test_acc:  0.859375\n",
            "iter:  785 train_cost:  0.4277395 train_acc:  0.890625 test_cost:  0.5044748 test_acc:  0.84375\n",
            "iter:  786 train_cost:  0.3762944 train_acc:  0.8515625 test_cost:  0.55073154 test_acc:  0.859375\n",
            "iter:  787 train_cost:  0.49163213 train_acc:  0.8671875 test_cost:  0.3963368 test_acc:  0.890625\n",
            "iter:  788 train_cost:  0.4526583 train_acc:  0.8359375 test_cost:  0.5274137 test_acc:  0.8515625\n",
            "iter:  789 train_cost:  0.58420205 train_acc:  0.8125 test_cost:  0.50607884 test_acc:  0.859375\n",
            "iter:  790 train_cost:  0.59965193 train_acc:  0.8359375 test_cost:  0.4481597 test_acc:  0.8515625\n",
            "iter:  791 train_cost:  0.348338 train_acc:  0.8671875 test_cost:  0.5138008 test_acc:  0.8515625\n",
            "iter:  792 train_cost:  0.47749734 train_acc:  0.859375 test_cost:  0.6469709 test_acc:  0.8046875\n",
            "iter:  793 train_cost:  0.57428324 train_acc:  0.8203125 test_cost:  0.5463263 test_acc:  0.7890625\n",
            "iter:  794 train_cost:  0.5320957 train_acc:  0.8359375 test_cost:  0.55608296 test_acc:  0.859375\n",
            "iter:  795 train_cost:  0.6386827 train_acc:  0.8125 test_cost:  0.50673574 test_acc:  0.8515625\n",
            "iter:  796 train_cost:  0.5787223 train_acc:  0.8359375 test_cost:  0.4709323 test_acc:  0.8359375\n",
            "iter:  797 train_cost:  0.60072595 train_acc:  0.8359375 test_cost:  0.4774806 test_acc:  0.8203125\n",
            "iter:  798 train_cost:  0.48434475 train_acc:  0.859375 test_cost:  0.56883574 test_acc:  0.828125\n",
            "iter:  799 train_cost:  0.42705983 train_acc:  0.8671875 test_cost:  0.59682494 test_acc:  0.765625\n",
            "iter:  800 train_cost:  0.54207027 train_acc:  0.8515625 test_cost:  0.3527938 test_acc:  0.890625\n",
            "iter:  801 train_cost:  0.7527947 train_acc:  0.78125 test_cost:  0.38695887 test_acc:  0.859375\n",
            "iter:  802 train_cost:  0.5574795 train_acc:  0.8359375 test_cost:  0.41676328 test_acc:  0.859375\n",
            "iter:  803 train_cost:  0.4585979 train_acc:  0.8515625 test_cost:  0.7074368 test_acc:  0.8046875\n",
            "iter:  804 train_cost:  0.55283725 train_acc:  0.78125 test_cost:  0.50568724 test_acc:  0.828125\n",
            "iter:  805 train_cost:  0.6484338 train_acc:  0.8203125 test_cost:  0.62296057 test_acc:  0.828125\n",
            "iter:  806 train_cost:  0.5259124 train_acc:  0.8125 test_cost:  0.3888529 test_acc:  0.8359375\n",
            "iter:  807 train_cost:  0.63941306 train_acc:  0.8515625 test_cost:  0.49346504 test_acc:  0.84375\n",
            "iter:  808 train_cost:  0.49334875 train_acc:  0.828125 test_cost:  0.48710954 test_acc:  0.828125\n",
            "iter:  809 train_cost:  0.5264074 train_acc:  0.84375 test_cost:  0.54002434 test_acc:  0.7890625\n",
            "iter:  810 train_cost:  0.497713 train_acc:  0.8515625 test_cost:  0.52044797 test_acc:  0.84375\n",
            "iter:  811 train_cost:  0.5057589 train_acc:  0.8515625 test_cost:  0.6594931 test_acc:  0.84375\n",
            "iter:  812 train_cost:  0.6408953 train_acc:  0.84375 test_cost:  0.51856196 test_acc:  0.8125\n",
            "iter:  813 train_cost:  0.55466235 train_acc:  0.828125 test_cost:  0.41046178 test_acc:  0.859375\n",
            "iter:  814 train_cost:  0.5371214 train_acc:  0.8515625 test_cost:  0.6757464 test_acc:  0.8046875\n",
            "iter:  815 train_cost:  0.32259575 train_acc:  0.8984375 test_cost:  0.5164151 test_acc:  0.8359375\n",
            "iter:  816 train_cost:  0.5328138 train_acc:  0.84375 test_cost:  0.48927245 test_acc:  0.8359375\n",
            "iter:  817 train_cost:  0.43946558 train_acc:  0.859375 test_cost:  0.4273802 test_acc:  0.8828125\n",
            "iter:  818 train_cost:  0.5068941 train_acc:  0.8125 test_cost:  0.36836007 test_acc:  0.8828125\n",
            "iter:  819 train_cost:  0.62071586 train_acc:  0.796875 test_cost:  0.35741124 test_acc:  0.8828125\n",
            "iter:  820 train_cost:  0.4356352 train_acc:  0.8203125 test_cost:  0.67246747 test_acc:  0.8125\n",
            "iter:  821 train_cost:  0.45899817 train_acc:  0.875 test_cost:  0.4038394 test_acc:  0.875\n",
            "iter:  822 train_cost:  0.5497683 train_acc:  0.8359375 test_cost:  0.6293085 test_acc:  0.75\n",
            "iter:  823 train_cost:  0.5187056 train_acc:  0.84375 test_cost:  0.65262884 test_acc:  0.8046875\n",
            "iter:  824 train_cost:  0.72157365 train_acc:  0.7734375 test_cost:  0.49618518 test_acc:  0.875\n",
            "iter:  825 train_cost:  0.4968038 train_acc:  0.8671875 test_cost:  0.5572519 test_acc:  0.828125\n",
            "iter:  826 train_cost:  0.6514889 train_acc:  0.796875 test_cost:  0.5190283 test_acc:  0.84375\n",
            "iter:  827 train_cost:  0.4434644 train_acc:  0.84375 test_cost:  0.58233845 test_acc:  0.8125\n",
            "iter:  828 train_cost:  0.48791596 train_acc:  0.8359375 test_cost:  0.39066827 test_acc:  0.890625\n",
            "iter:  829 train_cost:  0.64874965 train_acc:  0.84375 test_cost:  0.50530344 test_acc:  0.84375\n",
            "iter:  830 train_cost:  0.59850883 train_acc:  0.7890625 test_cost:  0.57242614 test_acc:  0.7890625\n",
            "iter:  831 train_cost:  0.52141595 train_acc:  0.8359375 test_cost:  0.5518408 test_acc:  0.796875\n",
            "iter:  832 train_cost:  0.4676797 train_acc:  0.8671875 test_cost:  0.41826802 test_acc:  0.859375\n",
            "iter:  833 train_cost:  0.53797215 train_acc:  0.8125 test_cost:  0.51465464 test_acc:  0.828125\n",
            "iter:  834 train_cost:  0.2984525 train_acc:  0.90625 test_cost:  0.45659113 test_acc:  0.8203125\n",
            "iter:  835 train_cost:  0.4728292 train_acc:  0.8359375 test_cost:  0.4208621 test_acc:  0.84375\n",
            "iter:  836 train_cost:  0.58555484 train_acc:  0.8671875 test_cost:  0.73209465 test_acc:  0.75\n",
            "iter:  837 train_cost:  0.49290827 train_acc:  0.8359375 test_cost:  0.40008426 test_acc:  0.8828125\n",
            "iter:  838 train_cost:  0.44996306 train_acc:  0.8515625 test_cost:  0.5410117 test_acc:  0.84375\n",
            "iter:  839 train_cost:  0.40757298 train_acc:  0.890625 test_cost:  0.43505573 test_acc:  0.890625\n",
            "iter:  840 train_cost:  0.4836407 train_acc:  0.859375 test_cost:  0.7819176 test_acc:  0.8359375\n",
            "iter:  841 train_cost:  0.5331625 train_acc:  0.8515625 test_cost:  0.6378322 test_acc:  0.84375\n",
            "iter:  842 train_cost:  0.53355736 train_acc:  0.828125 test_cost:  0.492737 test_acc:  0.828125\n",
            "iter:  843 train_cost:  0.48556215 train_acc:  0.8515625 test_cost:  0.40761736 test_acc:  0.859375\n",
            "iter:  844 train_cost:  0.67616606 train_acc:  0.78125 test_cost:  0.3737991 test_acc:  0.859375\n",
            "iter:  845 train_cost:  0.5791466 train_acc:  0.8515625 test_cost:  0.5415524 test_acc:  0.828125\n",
            "iter:  846 train_cost:  0.3587432 train_acc:  0.875 test_cost:  0.44820884 test_acc:  0.8515625\n",
            "iter:  847 train_cost:  0.43007147 train_acc:  0.8984375 test_cost:  0.5902444 test_acc:  0.84375\n",
            "iter:  848 train_cost:  0.39754674 train_acc:  0.859375 test_cost:  0.5197456 test_acc:  0.8046875\n",
            "iter:  849 train_cost:  0.59075254 train_acc:  0.8203125 test_cost:  0.5353407 test_acc:  0.7890625\n",
            "iter:  850 train_cost:  0.5118567 train_acc:  0.8203125 test_cost:  0.61732644 test_acc:  0.828125\n",
            "iter:  851 train_cost:  0.47583452 train_acc:  0.84375 test_cost:  0.37687293 test_acc:  0.859375\n",
            "iter:  852 train_cost:  0.45064685 train_acc:  0.8515625 test_cost:  0.42542773 test_acc:  0.875\n",
            "iter:  853 train_cost:  0.33526665 train_acc:  0.875 test_cost:  0.45338959 test_acc:  0.8515625\n",
            "iter:  854 train_cost:  0.56354284 train_acc:  0.84375 test_cost:  0.531337 test_acc:  0.8359375\n",
            "iter:  855 train_cost:  0.5837455 train_acc:  0.828125 test_cost:  0.7891872 test_acc:  0.7265625\n",
            "iter:  856 train_cost:  0.56459534 train_acc:  0.7890625 test_cost:  0.3894158 test_acc:  0.8828125\n",
            "iter:  857 train_cost:  0.52988136 train_acc:  0.8671875 test_cost:  0.525258 test_acc:  0.8046875\n",
            "iter:  858 train_cost:  0.45128974 train_acc:  0.8359375 test_cost:  0.50377965 test_acc:  0.8203125\n",
            "iter:  859 train_cost:  0.6145582 train_acc:  0.7890625 test_cost:  0.43561158 test_acc:  0.890625\n",
            "iter:  860 train_cost:  0.48149946 train_acc:  0.84375 test_cost:  0.4940908 test_acc:  0.8203125\n",
            "iter:  861 train_cost:  0.5504825 train_acc:  0.828125 test_cost:  0.40773457 test_acc:  0.8671875\n",
            "iter:  862 train_cost:  0.52087796 train_acc:  0.796875 test_cost:  0.6944699 test_acc:  0.8125\n",
            "iter:  863 train_cost:  0.5363763 train_acc:  0.859375 test_cost:  0.6027689 test_acc:  0.8125\n",
            "iter:  864 train_cost:  0.39466125 train_acc:  0.8828125 test_cost:  0.370702 test_acc:  0.9140625\n",
            "iter:  865 train_cost:  0.36966804 train_acc:  0.8984375 test_cost:  0.54289824 test_acc:  0.8203125\n",
            "iter:  866 train_cost:  0.47284883 train_acc:  0.828125 test_cost:  0.6131115 test_acc:  0.8046875\n",
            "iter:  867 train_cost:  0.4890658 train_acc:  0.828125 test_cost:  0.38416135 test_acc:  0.8671875\n",
            "iter:  868 train_cost:  0.4020149 train_acc:  0.90625 test_cost:  0.45639646 test_acc:  0.8359375\n",
            "iter:  869 train_cost:  0.45862693 train_acc:  0.8359375 test_cost:  0.48805958 test_acc:  0.8359375\n",
            "iter:  870 train_cost:  0.65350837 train_acc:  0.8203125 test_cost:  0.36662644 test_acc:  0.8828125\n",
            "iter:  871 train_cost:  0.6689983 train_acc:  0.8203125 test_cost:  0.48868376 test_acc:  0.859375\n",
            "iter:  872 train_cost:  0.49526626 train_acc:  0.8515625 test_cost:  0.438683 test_acc:  0.84375\n",
            "iter:  873 train_cost:  0.41780478 train_acc:  0.859375 test_cost:  0.5854246 test_acc:  0.78125\n",
            "iter:  874 train_cost:  0.46925354 train_acc:  0.8671875 test_cost:  0.4105717 test_acc:  0.8828125\n",
            "iter:  875 train_cost:  0.3691408 train_acc:  0.8828125 test_cost:  0.551751 test_acc:  0.8359375\n",
            "iter:  876 train_cost:  0.5951371 train_acc:  0.8359375 test_cost:  0.5182891 test_acc:  0.8515625\n",
            "iter:  877 train_cost:  0.52494377 train_acc:  0.8203125 test_cost:  0.43708652 test_acc:  0.890625\n",
            "iter:  878 train_cost:  0.60448307 train_acc:  0.8125 test_cost:  0.47637507 test_acc:  0.8359375\n",
            "iter:  879 train_cost:  0.6477242 train_acc:  0.78125 test_cost:  0.44134408 test_acc:  0.875\n",
            "iter:  880 train_cost:  0.42411727 train_acc:  0.84375 test_cost:  0.5819948 test_acc:  0.7890625\n",
            "iter:  881 train_cost:  0.45419282 train_acc:  0.875 test_cost:  0.5511231 test_acc:  0.8125\n",
            "iter:  882 train_cost:  0.4620416 train_acc:  0.8515625 test_cost:  0.60378087 test_acc:  0.796875\n",
            "iter:  883 train_cost:  0.54989135 train_acc:  0.8203125 test_cost:  0.49273893 test_acc:  0.8671875\n",
            "iter:  884 train_cost:  0.40564138 train_acc:  0.84375 test_cost:  0.5451164 test_acc:  0.8515625\n",
            "iter:  885 train_cost:  0.46259487 train_acc:  0.8359375 test_cost:  0.42690685 test_acc:  0.8515625\n",
            "iter:  886 train_cost:  0.53802615 train_acc:  0.8203125 test_cost:  0.40193653 test_acc:  0.8828125\n",
            "iter:  887 train_cost:  0.46526277 train_acc:  0.8828125 test_cost:  0.3116674 test_acc:  0.8828125\n",
            "iter:  888 train_cost:  0.43929344 train_acc:  0.8515625 test_cost:  0.42483532 test_acc:  0.8828125\n",
            "iter:  889 train_cost:  0.60048974 train_acc:  0.8125 test_cost:  0.59862036 test_acc:  0.8125\n",
            "iter:  890 train_cost:  0.33434147 train_acc:  0.875 test_cost:  0.56096345 test_acc:  0.8046875\n",
            "iter:  891 train_cost:  0.41020054 train_acc:  0.8671875 test_cost:  0.4905281 test_acc:  0.859375\n",
            "iter:  892 train_cost:  0.3384387 train_acc:  0.8984375 test_cost:  0.4398604 test_acc:  0.8671875\n",
            "iter:  893 train_cost:  0.38990146 train_acc:  0.859375 test_cost:  0.52641356 test_acc:  0.84375\n",
            "iter:  894 train_cost:  0.4325506 train_acc:  0.859375 test_cost:  0.42523694 test_acc:  0.875\n",
            "iter:  895 train_cost:  0.5604597 train_acc:  0.8359375 test_cost:  0.5225154 test_acc:  0.8203125\n",
            "iter:  896 train_cost:  0.3165513 train_acc:  0.875 test_cost:  0.5985807 test_acc:  0.828125\n",
            "iter:  897 train_cost:  0.6089473 train_acc:  0.8203125 test_cost:  0.4759254 test_acc:  0.8359375\n",
            "iter:  898 train_cost:  0.49039802 train_acc:  0.84375 test_cost:  0.5267721 test_acc:  0.8203125\n",
            "iter:  899 train_cost:  0.65017724 train_acc:  0.8203125 test_cost:  0.25528345 test_acc:  0.9140625\n",
            "iter:  900 train_cost:  0.541041 train_acc:  0.828125 test_cost:  0.34927598 test_acc:  0.890625\n",
            "iter:  901 train_cost:  0.4676667 train_acc:  0.8359375 test_cost:  0.59452397 test_acc:  0.796875\n",
            "iter:  902 train_cost:  0.38046345 train_acc:  0.859375 test_cost:  0.65208304 test_acc:  0.8203125\n",
            "iter:  903 train_cost:  0.4115008 train_acc:  0.875 test_cost:  0.5294319 test_acc:  0.8359375\n",
            "iter:  904 train_cost:  0.5916367 train_acc:  0.859375 test_cost:  0.54129964 test_acc:  0.8515625\n",
            "iter:  905 train_cost:  0.40718818 train_acc:  0.8828125 test_cost:  0.45336124 test_acc:  0.8203125\n",
            "iter:  906 train_cost:  0.5507115 train_acc:  0.8203125 test_cost:  0.4252927 test_acc:  0.8515625\n",
            "iter:  907 train_cost:  0.36359027 train_acc:  0.890625 test_cost:  0.4434772 test_acc:  0.890625\n",
            "iter:  908 train_cost:  0.46653458 train_acc:  0.8984375 test_cost:  0.4702137 test_acc:  0.828125\n",
            "iter:  909 train_cost:  0.4878403 train_acc:  0.8203125 test_cost:  0.46845347 test_acc:  0.859375\n",
            "iter:  910 train_cost:  0.56485516 train_acc:  0.828125 test_cost:  0.4303478 test_acc:  0.859375\n",
            "iter:  911 train_cost:  0.3306078 train_acc:  0.890625 test_cost:  0.4574463 test_acc:  0.828125\n",
            "iter:  912 train_cost:  0.4640646 train_acc:  0.8203125 test_cost:  0.46964496 test_acc:  0.84375\n",
            "iter:  913 train_cost:  0.48760432 train_acc:  0.84375 test_cost:  0.4221554 test_acc:  0.8671875\n",
            "iter:  914 train_cost:  0.34581745 train_acc:  0.859375 test_cost:  0.34357816 test_acc:  0.8671875\n",
            "iter:  915 train_cost:  0.38647866 train_acc:  0.8671875 test_cost:  0.30546904 test_acc:  0.8984375\n",
            "iter:  916 train_cost:  0.4286393 train_acc:  0.8359375 test_cost:  0.4825554 test_acc:  0.828125\n",
            "iter:  917 train_cost:  0.40472242 train_acc:  0.8984375 test_cost:  0.3273818 test_acc:  0.8828125\n",
            "iter:  918 train_cost:  0.27789405 train_acc:  0.8984375 test_cost:  0.52500176 test_acc:  0.8203125\n",
            "iter:  919 train_cost:  0.44498223 train_acc:  0.8671875 test_cost:  0.6501874 test_acc:  0.84375\n",
            "iter:  920 train_cost:  0.45126954 train_acc:  0.8515625 test_cost:  0.5683819 test_acc:  0.84375\n",
            "iter:  921 train_cost:  0.40326762 train_acc:  0.875 test_cost:  0.4157226 test_acc:  0.9140625\n",
            "iter:  922 train_cost:  0.34945232 train_acc:  0.890625 test_cost:  0.544115 test_acc:  0.8046875\n",
            "iter:  923 train_cost:  0.5917167 train_acc:  0.8203125 test_cost:  0.45045224 test_acc:  0.84375\n",
            "iter:  924 train_cost:  0.48039895 train_acc:  0.8359375 test_cost:  0.4678944 test_acc:  0.84375\n",
            "iter:  925 train_cost:  0.31077638 train_acc:  0.8984375 test_cost:  0.57144475 test_acc:  0.890625\n",
            "iter:  926 train_cost:  0.505745 train_acc:  0.859375 test_cost:  0.615314 test_acc:  0.796875\n",
            "iter:  927 train_cost:  0.43959564 train_acc:  0.8671875 test_cost:  0.5626037 test_acc:  0.828125\n",
            "iter:  928 train_cost:  0.45154452 train_acc:  0.8671875 test_cost:  0.3134436 test_acc:  0.8828125\n",
            "iter:  929 train_cost:  0.29374814 train_acc:  0.90625 test_cost:  0.67330915 test_acc:  0.78125\n",
            "iter:  930 train_cost:  0.42878926 train_acc:  0.8984375 test_cost:  0.39605173 test_acc:  0.84375\n",
            "iter:  931 train_cost:  0.5572042 train_acc:  0.8671875 test_cost:  0.29775053 test_acc:  0.90625\n",
            "iter:  932 train_cost:  0.39936948 train_acc:  0.8515625 test_cost:  0.45024577 test_acc:  0.8671875\n",
            "iter:  933 train_cost:  0.50696295 train_acc:  0.8515625 test_cost:  0.623282 test_acc:  0.7890625\n",
            "iter:  934 train_cost:  0.5367706 train_acc:  0.84375 test_cost:  0.57167447 test_acc:  0.8359375\n",
            "iter:  935 train_cost:  0.4405792 train_acc:  0.828125 test_cost:  0.4876716 test_acc:  0.859375\n",
            "iter:  936 train_cost:  0.45042914 train_acc:  0.8984375 test_cost:  0.5422598 test_acc:  0.8359375\n",
            "iter:  937 train_cost:  0.50459003 train_acc:  0.828125 test_cost:  0.3966189 test_acc:  0.875\n",
            "iter:  938 train_cost:  0.5141095 train_acc:  0.8203125 test_cost:  0.52618515 test_acc:  0.796875\n",
            "iter:  939 train_cost:  0.30739084 train_acc:  0.90625 test_cost:  0.39193553 test_acc:  0.8359375\n",
            "iter:  940 train_cost:  0.40507796 train_acc:  0.859375 test_cost:  0.5241729 test_acc:  0.8125\n",
            "iter:  941 train_cost:  0.42627317 train_acc:  0.8671875 test_cost:  0.4342298 test_acc:  0.859375\n",
            "iter:  942 train_cost:  0.4278951 train_acc:  0.875 test_cost:  0.5747267 test_acc:  0.8359375\n",
            "iter:  943 train_cost:  0.41631573 train_acc:  0.8515625 test_cost:  0.58839756 test_acc:  0.8125\n",
            "iter:  944 train_cost:  0.515399 train_acc:  0.8515625 test_cost:  0.4133008 test_acc:  0.8515625\n",
            "iter:  945 train_cost:  0.4497199 train_acc:  0.8359375 test_cost:  0.47635084 test_acc:  0.859375\n",
            "iter:  946 train_cost:  0.50706315 train_acc:  0.8359375 test_cost:  0.3693599 test_acc:  0.8671875\n",
            "iter:  947 train_cost:  0.35309643 train_acc:  0.8828125 test_cost:  0.47049248 test_acc:  0.859375\n",
            "iter:  948 train_cost:  0.5405412 train_acc:  0.8203125 test_cost:  0.38150465 test_acc:  0.921875\n",
            "iter:  949 train_cost:  0.6354519 train_acc:  0.8046875 test_cost:  0.41660076 test_acc:  0.8515625\n",
            "iter:  950 train_cost:  0.6674336 train_acc:  0.765625 test_cost:  0.48905778 test_acc:  0.8359375\n",
            "iter:  951 train_cost:  0.5174365 train_acc:  0.828125 test_cost:  0.5593385 test_acc:  0.8359375\n",
            "iter:  952 train_cost:  0.4338398 train_acc:  0.8671875 test_cost:  0.32199374 test_acc:  0.9140625\n",
            "iter:  953 train_cost:  0.4543929 train_acc:  0.8671875 test_cost:  0.51078767 test_acc:  0.8203125\n",
            "iter:  954 train_cost:  0.35907954 train_acc:  0.8828125 test_cost:  0.4699883 test_acc:  0.859375\n",
            "iter:  955 train_cost:  0.4667099 train_acc:  0.8671875 test_cost:  0.463925 test_acc:  0.8515625\n",
            "iter:  956 train_cost:  0.55162126 train_acc:  0.84375 test_cost:  0.33148757 test_acc:  0.8671875\n",
            "iter:  957 train_cost:  0.42327213 train_acc:  0.875 test_cost:  0.5241126 test_acc:  0.828125\n",
            "iter:  958 train_cost:  0.4658274 train_acc:  0.8515625 test_cost:  0.4686159 test_acc:  0.875\n",
            "iter:  959 train_cost:  0.464493 train_acc:  0.8515625 test_cost:  0.5296054 test_acc:  0.8046875\n",
            "iter:  960 train_cost:  0.4134715 train_acc:  0.890625 test_cost:  0.3817991 test_acc:  0.8828125\n",
            "iter:  961 train_cost:  0.41572714 train_acc:  0.8515625 test_cost:  0.5066041 test_acc:  0.84375\n",
            "iter:  962 train_cost:  0.5012895 train_acc:  0.8515625 test_cost:  0.48065928 test_acc:  0.859375\n",
            "iter:  963 train_cost:  0.51653767 train_acc:  0.8828125 test_cost:  0.60571945 test_acc:  0.8359375\n",
            "iter:  964 train_cost:  0.3389951 train_acc:  0.890625 test_cost:  0.6403532 test_acc:  0.8125\n",
            "iter:  965 train_cost:  0.41429162 train_acc:  0.8671875 test_cost:  0.58537436 test_acc:  0.8046875\n",
            "iter:  966 train_cost:  0.53608704 train_acc:  0.859375 test_cost:  0.29615295 test_acc:  0.875\n",
            "iter:  967 train_cost:  0.4200739 train_acc:  0.8671875 test_cost:  0.40007383 test_acc:  0.84375\n",
            "iter:  968 train_cost:  0.5495568 train_acc:  0.8125 test_cost:  0.35307887 test_acc:  0.890625\n",
            "iter:  969 train_cost:  0.31187385 train_acc:  0.9296875 test_cost:  0.6216413 test_acc:  0.84375\n",
            "iter:  970 train_cost:  0.31234533 train_acc:  0.90625 test_cost:  0.363757 test_acc:  0.8984375\n",
            "iter:  971 train_cost:  0.5931737 train_acc:  0.828125 test_cost:  0.48311064 test_acc:  0.8671875\n",
            "iter:  972 train_cost:  0.32287568 train_acc:  0.90625 test_cost:  0.45342875 test_acc:  0.8671875\n",
            "iter:  973 train_cost:  0.5119539 train_acc:  0.8359375 test_cost:  0.4749217 test_acc:  0.8671875\n",
            "iter:  974 train_cost:  0.4774269 train_acc:  0.8671875 test_cost:  0.35872626 test_acc:  0.875\n",
            "iter:  975 train_cost:  0.5000119 train_acc:  0.84375 test_cost:  0.5666772 test_acc:  0.8203125\n",
            "iter:  976 train_cost:  0.51779824 train_acc:  0.8203125 test_cost:  0.49563068 test_acc:  0.8203125\n",
            "iter:  977 train_cost:  0.43032843 train_acc:  0.859375 test_cost:  0.4984255 test_acc:  0.84375\n",
            "iter:  978 train_cost:  0.5466066 train_acc:  0.8125 test_cost:  0.5137312 test_acc:  0.8359375\n",
            "iter:  979 train_cost:  0.3094432 train_acc:  0.8984375 test_cost:  0.43573123 test_acc:  0.8671875\n",
            "iter:  980 train_cost:  0.33131838 train_acc:  0.890625 test_cost:  0.54795516 test_acc:  0.859375\n",
            "iter:  981 train_cost:  0.42943916 train_acc:  0.828125 test_cost:  0.4219978 test_acc:  0.890625\n",
            "iter:  982 train_cost:  0.5084585 train_acc:  0.84375 test_cost:  0.36969712 test_acc:  0.859375\n",
            "iter:  983 train_cost:  0.7431919 train_acc:  0.7890625 test_cost:  0.568632 test_acc:  0.859375\n",
            "iter:  984 train_cost:  0.5024302 train_acc:  0.8359375 test_cost:  0.51298106 test_acc:  0.828125\n",
            "iter:  985 train_cost:  0.43897885 train_acc:  0.828125 test_cost:  0.39880985 test_acc:  0.8515625\n",
            "iter:  986 train_cost:  0.31124297 train_acc:  0.890625 test_cost:  0.38255167 test_acc:  0.8671875\n",
            "iter:  987 train_cost:  0.537134 train_acc:  0.8515625 test_cost:  0.41374496 test_acc:  0.875\n",
            "iter:  988 train_cost:  0.48010287 train_acc:  0.859375 test_cost:  0.4736725 test_acc:  0.8515625\n",
            "iter:  989 train_cost:  0.303166 train_acc:  0.921875 test_cost:  0.44193143 test_acc:  0.8359375\n",
            "iter:  990 train_cost:  0.5264531 train_acc:  0.8515625 test_cost:  0.44320524 test_acc:  0.8828125\n",
            "iter:  991 train_cost:  0.48353302 train_acc:  0.84375 test_cost:  0.40756294 test_acc:  0.8828125\n",
            "iter:  992 train_cost:  0.3280577 train_acc:  0.890625 test_cost:  0.4348686 test_acc:  0.875\n",
            "iter:  993 train_cost:  0.61975795 train_acc:  0.8203125 test_cost:  0.5978416 test_acc:  0.8203125\n",
            "iter:  994 train_cost:  0.50785595 train_acc:  0.8203125 test_cost:  0.40050304 test_acc:  0.8671875\n",
            "iter:  995 train_cost:  0.43122566 train_acc:  0.875 test_cost:  0.4783832 test_acc:  0.8828125\n",
            "iter:  996 train_cost:  0.4084322 train_acc:  0.8671875 test_cost:  0.5683757 test_acc:  0.8515625\n",
            "iter:  997 train_cost:  0.3764871 train_acc:  0.875 test_cost:  0.37794513 test_acc:  0.8984375\n",
            "iter:  998 train_cost:  0.5813334 train_acc:  0.8515625 test_cost:  0.62472653 test_acc:  0.8046875\n",
            "iter:  999 train_cost:  0.35270137 train_acc:  0.8828125 test_cost:  0.4804685 test_acc:  0.8359375\n",
            "predicted  [[-1.2477537   5.9756837   0.6207233   1.423921  ]\n",
            " [-1.4750469   5.6989446   0.5577158   1.2042632 ]\n",
            " [-1.4107713   5.7938414   0.5821939   1.2413826 ]\n",
            " [-1.4418812   5.7458324   0.56632453  1.2346345 ]\n",
            " [-1.2363346   5.9920564   0.65204597  1.4277827 ]\n",
            " [-1.227309    5.9981165   0.6149899   1.4424824 ]\n",
            " [-1.0931115   6.1276073   0.6602364   1.6102186 ]\n",
            " [-1.4714556   5.741755    0.5315476   1.1362072 ]\n",
            " [-1.2333767   6.011944    0.6158439   1.3924607 ]\n",
            " [-1.3408425   5.8636127   0.61586833  1.347872  ]]\n",
            "real  [[0.32463124 0.73313092 0.47488059 0.30511154]\n",
            " [0.7638626  0.63017407 0.09125469 0.53216256]\n",
            " [0.26815479 0.06324862 0.71566913 0.62970509]\n",
            " [0.8064397  0.15393563 0.48375146 0.74241536]\n",
            " [0.80525852 0.38296255 0.97285628 0.65525673]\n",
            " [0.17314031 0.18034827 0.33841957 0.10681907]\n",
            " [0.83897411 0.57246599 0.93045043 0.32084221]\n",
            " [0.55663316 0.55168506 0.48916313 0.84250453]\n",
            " [0.68276726 0.72478758 0.5224834  0.9933377 ]\n",
            " [0.74340984 0.06693573 0.89536741 0.26781805]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDkPoIsB90MW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf \n",
        "\n",
        "\n",
        "def variable_summaries(var):\n",
        "  \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
        "  with tf.name_scope('summaries'):\n",
        "    mean = tf.reduce_mean(var)\n",
        "    tf.summary.scalar('mean', mean)\n",
        "    with tf.name_scope('stddev'):\n",
        "      stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
        "    tf.summary.scalar('stddev', stddev)\n",
        "    tf.summary.scalar('max', tf.reduce_max(var))\n",
        "    tf.summary.scalar('min', tf.reduce_min(var))\n",
        "    tf.summary.histogram('histogram', var)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRJsyYmvBE43",
        "colab_type": "code",
        "outputId": "f2fbd707-ddaa-4c4a-928d-1ebbeabffb59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        }
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "from matplotlib.pyplot import imshow\n",
        "\n",
        "# Import MNIST data\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
        "\n",
        "mnist.train.images.shape\n",
        "\n",
        "image =mnist.train.images[0].reshape((28,28))\n",
        "#MNIST data input (img shape: 28*28)\n",
        "imshow(image)\n",
        "\n",
        "mnist.train.labels[0]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
            "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCIrwA0ERfiAowg8ERfiBoAg/ENT/AyErW1pw/s8cAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXZtWbWQ92P_",
        "colab_type": "text"
      },
      "source": [
        "# CNN using Tensorflow\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6k9YPZQ95z5",
        "colab_type": "code",
        "outputId": "8daa0dda-e9c9-4d95-b6e6-0a014600dce8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2288
        }
      },
      "source": [
        "n_classes=10\n",
        "learning_rate=0.002\n",
        "batch_size=64\n",
        "\n",
        "import tensorflow as tf\n",
        "# Create some wrappers for simplicity\n",
        "def conv2d(x, W, b, strides=1):\n",
        "    # Conv2D wrapper, with bias and relu activation\n",
        "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
        "    x = tf.nn.bias_add(x, b)\n",
        "    return tf.nn.relu(x)\n",
        "\n",
        "\n",
        "num_inputs = 784\n",
        "num_outputs= 10\n",
        "\n",
        "# tf Graph input\n",
        "x = tf.placeholder(tf.float32, [None, num_inputs],name = 'input')\n",
        "y = tf.placeholder(tf.float32, [None, num_outputs],name =  'output')\n",
        "\n",
        "\n",
        "def maxpool2d(x, k=2):\n",
        "    # MaxPool2D wrapper\n",
        "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
        "                          padding='SAME')\n",
        "\n",
        "\n",
        "# Create model\n",
        "def conv_net(x, weights, biases):\n",
        "    # Reshape input picture\n",
        "    \n",
        "    with tf.name_scope('input-reshape'):\n",
        "        x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
        "\n",
        "    with tf.name_scope('conv-1'):\n",
        "\n",
        "        # Convolution Layer\n",
        "        conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
        "        print('con1_before max',conv1.get_shape().as_list())\n",
        "\n",
        "    with tf.name_scope('maxpooling-1'):\n",
        "        \n",
        "        # Max Pooling (down-sampling)\n",
        "        conv1 = maxpool2d(conv1, k=2)\n",
        "        print('con1_after max',conv1.get_shape().as_list())\n",
        "\n",
        "\n",
        "    # Convolution Layer\n",
        "    with tf.name_scope('conv-2'):\n",
        "\n",
        "        conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
        "        print('con2_before max',conv2.get_shape().as_list())\n",
        "\n",
        "\n",
        "    with tf.name_scope('maxpooling-2'):\n",
        "        \n",
        "     \n",
        "        # Max Pooling (down-sampling)\n",
        "        conv2 = maxpool2d(conv2, k=2)\n",
        "        print('con2_after max', conv2.get_shape().as_list())\n",
        "\n",
        "    # Fully connected layer\n",
        "    # Reshape conv2 output to fit fully connected layer input\n",
        "    #wd1 numx3x3  wd1.get_shape() -> numx9 \n",
        "    with tf.name_scope('flatten'):\n",
        "    \n",
        "        fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
        "        fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
        "        fc1 = tf.nn.relu(fc1)\n",
        "    # Apply Dropout\n",
        "    #fc1 = tf.nn.dropout(fc1, dropout)\n",
        "\n",
        "    # Output, class prediction\n",
        "    with tf.name_scope('output'):\n",
        "    \n",
        "        out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
        "    return out\n",
        "\n",
        "\n",
        "# Store layers weight & bias\n",
        "weights = {\n",
        "    # 5x5 conv, 1 input, 32 outputs\n",
        "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32]), name=\"wc1\"),\n",
        "    # 5x5 conv, 32 inputs, 64 outputs\n",
        "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
        "    # fully connected, 7*7*64 inputs, 1024 outputs\n",
        "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
        "    # 1024 inputs, 10 outputs (class prediction)\n",
        "    'out': tf.Variable(tf.random_normal([1024, n_classes]))\n",
        "}\n",
        "\n",
        "biases = {\n",
        "    'bc1': tf.Variable(tf.random_normal([32])),\n",
        "    'bc2': tf.Variable(tf.random_normal([64])),\n",
        "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
        "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
        "}\n",
        "\n",
        "y_p = conv_net(x, weights, biases)\n",
        "\n",
        "#crossentropy cost\n",
        "with tf.name_scope('cross_entropy'):\n",
        "\n",
        "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_p, labels=y)) # cross entropy cost\n",
        "\n",
        "#tf.summary.scalar('cross_entropy', cost)\n",
        "\n",
        "\n",
        "# Evaluate model\n",
        "with tf.name_scope('operations'):\n",
        "\n",
        "    correct_pred = tf.equal(tf.argmax(y_p, 1), tf.argmax(y, 1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "#\n",
        "merged = tf.summary.scalar('accuracy', accuracy)\n",
        "\n",
        "\n",
        "\n",
        "# optimisation \n",
        "with tf.name_scope('optimisation'):\n",
        "\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# initalizing the graph and the weights\n",
        "#merged = tf.summary.merge_all()\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "\n",
        "\n",
        "# Launch the graph\n",
        "with tf.Session() as sess:\n",
        "    #graph_writer = tf.summary.FileWriter('./log/train', sess.graph)\n",
        "    #graph_writer.add_summary()\n",
        "    train_writer = tf.summary.FileWriter('./log/train', sess.graph)\n",
        "    test_writer = tf.summary.FileWriter('./log/test')\n",
        "    sess.run(init)\n",
        "    print('started')\n",
        "    \n",
        "    for i in range(10000):\n",
        "        \n",
        "        \n",
        "        \n",
        "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
        "        \n",
        "\n",
        "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
        "\n",
        "\n",
        "        summary,train_cost , train_acc = sess.run([merged,cost,accuracy], feed_dict={x: batch_x,y: batch_y})\n",
        "        train_writer.add_summary(summary, i)\n",
        "\n",
        "        #print('started')\n",
        "\n",
        "        test_batch_x, test_batch_y = mnist.test.next_batch(batch_size)\n",
        "\n",
        "        summary,test_cost, test_acc  = sess.run([merged,cost,accuracy], feed_dict={x: test_batch_x,y: test_batch_y})\n",
        "        train_writer.add_summary(summary, i)\n",
        "        if i % 100 ==0:\n",
        "\n",
        "            print('iter: ',i, 'train_cost: ', train_cost, 'train_acc: ', train_acc,'test_cost: ', test_cost, 'test_acc: ', test_acc )\n",
        "\n",
        "\n",
        "    #y_p_p = sess.run(y_p, feed_dict={x: x_gr, y: y_gr})\n",
        "    \n",
        "    print('predicted ', y_p_p)\n",
        "    print('real ', y_gr)\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "con1_before max [None, 28, 28, 32]\n",
            "con1_after max [None, 14, 14, 32]\n",
            "con2_before max [None, 14, 14, 64]\n",
            "con2_after max [None, 7, 7, 64]\n",
            "started\n",
            "iter:  0 train_cost:  42032.383 train_acc:  0.09375 test_cost:  49154.242 test_acc:  0.109375\n",
            "iter:  100 train_cost:  2872.5776 train_acc:  0.8125 test_cost:  1861.5217 test_acc:  0.828125\n",
            "iter:  200 train_cost:  1496.2236 train_acc:  0.859375 test_cost:  1191.8324 test_acc:  0.9375\n",
            "iter:  300 train_cost:  1286.6677 train_acc:  0.9375 test_cost:  432.9254 test_acc:  0.9375\n",
            "iter:  400 train_cost:  303.96716 train_acc:  0.9375 test_cost:  1015.906 test_acc:  0.953125\n",
            "iter:  500 train_cost:  560.5688 train_acc:  0.953125 test_cost:  482.98932 test_acc:  0.9375\n",
            "iter:  600 train_cost:  4.3995743 train_acc:  0.984375 test_cost:  207.38779 test_acc:  0.96875\n",
            "iter:  700 train_cost:  1497.8547 train_acc:  0.953125 test_cost:  879.27905 test_acc:  0.90625\n",
            "iter:  800 train_cost:  206.42395 train_acc:  0.984375 test_cost:  231.63116 test_acc:  0.9375\n",
            "iter:  900 train_cost:  976.64417 train_acc:  0.9375 test_cost:  433.9566 test_acc:  0.96875\n",
            "iter:  1000 train_cost:  304.1105 train_acc:  0.953125 test_cost:  275.0704 test_acc:  0.96875\n",
            "iter:  1100 train_cost:  4.695389 train_acc:  0.96875 test_cost:  520.7157 test_acc:  0.96875\n",
            "iter:  1200 train_cost:  203.87698 train_acc:  0.9375 test_cost:  196.57442 test_acc:  0.984375\n",
            "iter:  1300 train_cost:  0.0 train_acc:  1.0 test_cost:  65.59894 test_acc:  0.984375\n",
            "iter:  1400 train_cost:  168.84305 train_acc:  0.96875 test_cost:  509.83978 test_acc:  0.96875\n",
            "iter:  1500 train_cost:  9.565186 train_acc:  0.984375 test_cost:  966.69147 test_acc:  0.9375\n",
            "iter:  1600 train_cost:  262.2477 train_acc:  0.984375 test_cost:  498.54422 test_acc:  0.9375\n",
            "iter:  1700 train_cost:  329.8608 train_acc:  0.96875 test_cost:  400.8946 test_acc:  0.96875\n",
            "iter:  1800 train_cost:  0.0 train_acc:  1.0 test_cost:  482.99985 test_acc:  0.90625\n",
            "iter:  1900 train_cost:  162.84883 train_acc:  0.9375 test_cost:  744.6427 test_acc:  0.9375\n",
            "iter:  2000 train_cost:  37.03705 train_acc:  0.984375 test_cost:  19.814423 test_acc:  0.984375\n",
            "iter:  2100 train_cost:  108.16824 train_acc:  0.96875 test_cost:  223.59732 test_acc:  0.96875\n",
            "iter:  2200 train_cost:  34.623413 train_acc:  0.984375 test_cost:  370.50095 test_acc:  0.96875\n",
            "iter:  2300 train_cost:  59.82289 train_acc:  0.984375 test_cost:  107.74289 test_acc:  0.953125\n",
            "iter:  2400 train_cost:  0.0 train_acc:  1.0 test_cost:  228.1438 test_acc:  0.96875\n",
            "iter:  2500 train_cost:  52.831635 train_acc:  0.984375 test_cost:  194.03401 test_acc:  0.96875\n",
            "iter:  2600 train_cost:  33.78885 train_acc:  0.984375 test_cost:  124.9453 test_acc:  0.984375\n",
            "iter:  2700 train_cost:  8.663055 train_acc:  0.984375 test_cost:  217.08017 test_acc:  0.953125\n",
            "iter:  2800 train_cost:  0.0 train_acc:  1.0 test_cost:  28.72937 test_acc:  0.984375\n",
            "iter:  2900 train_cost:  0.0 train_acc:  1.0 test_cost:  0.0 test_acc:  1.0\n",
            "iter:  3000 train_cost:  0.0 train_acc:  1.0 test_cost:  343.46518 test_acc:  0.953125\n",
            "iter:  3100 train_cost:  101.34549 train_acc:  0.96875 test_cost:  0.0 test_acc:  1.0\n",
            "iter:  3200 train_cost:  0.0 train_acc:  1.0 test_cost:  0.0 test_acc:  1.0\n",
            "iter:  3300 train_cost:  304.5052 train_acc:  0.984375 test_cost:  142.48425 test_acc:  0.96875\n",
            "iter:  3400 train_cost:  0.0 train_acc:  1.0 test_cost:  0.0 test_acc:  1.0\n",
            "iter:  3500 train_cost:  0.0 train_acc:  1.0 test_cost:  64.75415 test_acc:  0.984375\n",
            "iter:  3600 train_cost:  0.0 train_acc:  1.0 test_cost:  333.73737 test_acc:  0.96875\n",
            "iter:  3700 train_cost:  0.0 train_acc:  1.0 test_cost:  0.0 test_acc:  1.0\n",
            "iter:  3800 train_cost:  98.38875 train_acc:  0.984375 test_cost:  696.0123 test_acc:  0.9375\n",
            "iter:  3900 train_cost:  0.0 train_acc:  1.0 test_cost:  32.161987 test_acc:  0.984375\n",
            "iter:  4000 train_cost:  14.586838 train_acc:  0.984375 test_cost:  15.187378 test_acc:  0.984375\n",
            "iter:  4100 train_cost:  0.0 train_acc:  1.0 test_cost:  28.955353 test_acc:  0.984375\n",
            "iter:  4200 train_cost:  0.0 train_acc:  1.0 test_cost:  302.33398 test_acc:  0.96875\n",
            "iter:  4300 train_cost:  0.0 train_acc:  1.0 test_cost:  0.0 test_acc:  1.0\n",
            "iter:  4400 train_cost:  0.0 train_acc:  1.0 test_cost:  133.38461 test_acc:  0.953125\n",
            "iter:  4500 train_cost:  0.0 train_acc:  1.0 test_cost:  56.43933 test_acc:  0.96875\n",
            "iter:  4600 train_cost:  67.2341 train_acc:  0.984375 test_cost:  0.0 test_acc:  1.0\n",
            "iter:  4700 train_cost:  0.0 train_acc:  1.0 test_cost:  147.61928 test_acc:  0.953125\n",
            "iter:  4800 train_cost:  0.0 train_acc:  1.0 test_cost:  291.2627 test_acc:  0.9375\n",
            "iter:  4900 train_cost:  0.0 train_acc:  1.0 test_cost:  0.0 test_acc:  1.0\n",
            "iter:  5000 train_cost:  0.0 train_acc:  1.0 test_cost:  0.0 test_acc:  1.0\n",
            "iter:  5100 train_cost:  0.0 train_acc:  1.0 test_cost:  0.0 test_acc:  1.0\n",
            "iter:  5200 train_cost:  0.0 train_acc:  1.0 test_cost:  0.0 test_acc:  1.0\n",
            "iter:  5300 train_cost:  0.0 train_acc:  1.0 test_cost:  0.0 test_acc:  1.0\n",
            "iter:  5400 train_cost:  0.0 train_acc:  1.0 test_cost:  404.09833 test_acc:  0.953125\n",
            "iter:  5500 train_cost:  0.0 train_acc:  1.0 test_cost:  80.743355 test_acc:  0.984375\n",
            "iter:  5600 train_cost:  5.8768005 train_acc:  0.984375 test_cost:  39.60556 test_acc:  0.984375\n",
            "iter:  5700 train_cost:  0.0 train_acc:  1.0 test_cost:  42.92679 test_acc:  0.984375\n",
            "iter:  5800 train_cost:  0.0 train_acc:  1.0 test_cost:  125.64801 test_acc:  0.96875\n",
            "iter:  5900 train_cost:  0.0 train_acc:  1.0 test_cost:  134.54773 test_acc:  0.96875\n",
            "iter:  6000 train_cost:  0.0 train_acc:  1.0 test_cost:  167.69385 test_acc:  0.984375\n",
            "iter:  6100 train_cost:  0.0 train_acc:  1.0 test_cost:  0.0 test_acc:  1.0\n",
            "iter:  6200 train_cost:  0.0 train_acc:  1.0 test_cost:  254.09 test_acc:  0.953125\n",
            "iter:  6300 train_cost:  0.0 train_acc:  1.0 test_cost:  0.6870117 test_acc:  0.984375\n",
            "iter:  6400 train_cost:  0.0 train_acc:  1.0 test_cost:  0.0 test_acc:  1.0\n",
            "iter:  6500 train_cost:  0.0 train_acc:  1.0 test_cost:  0.0 test_acc:  1.0\n",
            "iter:  6600 train_cost:  0.0 train_acc:  1.0 test_cost:  18.335205 test_acc:  0.984375\n",
            "iter:  6700 train_cost:  0.0 train_acc:  1.0 test_cost:  11.208191 test_acc:  0.96875\n",
            "iter:  6800 train_cost:  0.0 train_acc:  1.0 test_cost:  353.07843 test_acc:  0.96875\n",
            "iter:  6900 train_cost:  0.22390749 train_acc:  0.984375 test_cost:  76.82361 test_acc:  0.984375\n",
            "iter:  7000 train_cost:  0.0 train_acc:  1.0 test_cost:  66.81475 test_acc:  0.96875\n",
            "iter:  7100 train_cost:  36.1967 train_acc:  0.984375 test_cost:  37.526703 test_acc:  0.984375\n",
            "iter:  7200 train_cost:  0.0 train_acc:  1.0 test_cost:  209.92014 test_acc:  0.96875\n",
            "iter:  7300 train_cost:  0.0 train_acc:  1.0 test_cost:  140.3687 test_acc:  0.96875\n",
            "iter:  7400 train_cost:  0.0 train_acc:  1.0 test_cost:  180.49464 test_acc:  0.96875\n",
            "iter:  7500 train_cost:  0.0 train_acc:  1.0 test_cost:  86.93492 test_acc:  0.953125\n",
            "iter:  7600 train_cost:  0.0 train_acc:  1.0 test_cost:  275.87256 test_acc:  0.96875\n",
            "iter:  7700 train_cost:  0.0 train_acc:  1.0 test_cost:  29.47026 test_acc:  0.96875\n",
            "iter:  7800 train_cost:  0.0 train_acc:  1.0 test_cost:  0.0 test_acc:  1.0\n",
            "iter:  7900 train_cost:  0.0 train_acc:  1.0 test_cost:  16.639587 test_acc:  0.984375\n",
            "iter:  8000 train_cost:  35.05101 train_acc:  0.984375 test_cost:  0.0 test_acc:  1.0\n",
            "iter:  8100 train_cost:  0.0 train_acc:  1.0 test_cost:  101.9117 test_acc:  0.953125\n",
            "iter:  8200 train_cost:  0.0 train_acc:  1.0 test_cost:  196.74678 test_acc:  0.9375\n",
            "iter:  8300 train_cost:  0.0 train_acc:  1.0 test_cost:  211.65576 test_acc:  0.96875\n",
            "iter:  8400 train_cost:  0.0 train_acc:  1.0 test_cost:  33.810287 test_acc:  0.984375\n",
            "iter:  8500 train_cost:  0.0 train_acc:  1.0 test_cost:  139.20033 test_acc:  0.96875\n",
            "iter:  8600 train_cost:  0.0 train_acc:  1.0 test_cost:  0.0 test_acc:  1.0\n",
            "iter:  8700 train_cost:  0.0 train_acc:  1.0 test_cost:  0.0 test_acc:  1.0\n",
            "iter:  8800 train_cost:  0.0 train_acc:  1.0 test_cost:  129.30313 test_acc:  0.984375\n",
            "iter:  8900 train_cost:  0.0 train_acc:  1.0 test_cost:  0.0 test_acc:  1.0\n",
            "iter:  9000 train_cost:  0.0 train_acc:  1.0 test_cost:  389.74005 test_acc:  0.953125\n",
            "iter:  9100 train_cost:  0.0 train_acc:  1.0 test_cost:  138.8289 test_acc:  0.953125\n",
            "iter:  9200 train_cost:  0.0 train_acc:  1.0 test_cost:  41.88907 test_acc:  0.984375\n",
            "iter:  9300 train_cost:  0.0 train_acc:  1.0 test_cost:  125.56615 test_acc:  0.9375\n",
            "iter:  9400 train_cost:  0.0 train_acc:  1.0 test_cost:  0.0 test_acc:  1.0\n",
            "iter:  9500 train_cost:  31.503662 train_acc:  0.984375 test_cost:  0.53819275 test_acc:  0.984375\n",
            "iter:  9600 train_cost:  0.0 train_acc:  1.0 test_cost:  74.68451 test_acc:  0.984375\n",
            "iter:  9700 train_cost:  0.0 train_acc:  1.0 test_cost:  132.61597 test_acc:  0.984375\n",
            "iter:  9800 train_cost:  0.0 train_acc:  1.0 test_cost:  0.0 test_acc:  1.0\n",
            "iter:  9900 train_cost:  0.0 train_acc:  1.0 test_cost:  64.5014 test_acc:  0.984375\n",
            "predicted  [[-1.2477537   5.9756837   0.6207233   1.423921  ]\n",
            " [-1.4750469   5.6989446   0.5577158   1.2042632 ]\n",
            " [-1.4107713   5.7938414   0.5821939   1.2413826 ]\n",
            " [-1.4418812   5.7458324   0.56632453  1.2346345 ]\n",
            " [-1.2363346   5.9920564   0.65204597  1.4277827 ]\n",
            " [-1.227309    5.9981165   0.6149899   1.4424824 ]\n",
            " [-1.0931115   6.1276073   0.6602364   1.6102186 ]\n",
            " [-1.4714556   5.741755    0.5315476   1.1362072 ]\n",
            " [-1.2333767   6.011944    0.6158439   1.3924607 ]\n",
            " [-1.3408425   5.8636127   0.61586833  1.347872  ]]\n",
            "real  [[0.32463124 0.73313092 0.47488059 0.30511154]\n",
            " [0.7638626  0.63017407 0.09125469 0.53216256]\n",
            " [0.26815479 0.06324862 0.71566913 0.62970509]\n",
            " [0.8064397  0.15393563 0.48375146 0.74241536]\n",
            " [0.80525852 0.38296255 0.97285628 0.65525673]\n",
            " [0.17314031 0.18034827 0.33841957 0.10681907]\n",
            " [0.83897411 0.57246599 0.93045043 0.32084221]\n",
            " [0.55663316 0.55168506 0.48916313 0.84250453]\n",
            " [0.68276726 0.72478758 0.5224834  0.9933377 ]\n",
            " [0.74340984 0.06693573 0.89536741 0.26781805]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhbgp1CwE_4t",
        "colab_type": "code",
        "outputId": "91a9543d-470f-435c-cfe7-b96ade029c68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        }
      },
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-06-06 07:31:04--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 34.195.209.198, 52.4.95.48, 34.206.253.53, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|34.195.209.198|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16648024 (16M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  15.88M  7.15MB/s    in 2.2s    \n",
            "\n",
            "2019-06-06 07:31:07 (7.15 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [16648024/16648024]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yipQU39JOccV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf log"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96bJem1FFBXx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LOG_DIR = './log'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faCuFGDZFGCj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_ipython().system_raw('./ngrok http 6006 &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdPYibnJFI1H",
        "colab_type": "code",
        "outputId": "9ab8fc86-f749-438c-a034-a9306fa2a741",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "http://6fb4add5.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}