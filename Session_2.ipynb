{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Session 2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahmadhajmosa/Machine-learning-labs/blob/Michal/Session_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmSUExzC-ZBV",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgWZldyi-Zef",
        "colab_type": "text"
      },
      "source": [
        "# Lab on Machine Learning and Applications in Intelligent Vehicles\n",
        "## Session 1: Introduction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a75NLOuwKXSs",
        "colab_type": "text"
      },
      "source": [
        "#Session 2: 05.06 - 13:00 - 14:30 :\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciUtGM-W_ehP",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "## Intro:\n",
        "\n",
        "Tensorflow is a powerful framework for implementing and deploying large-scale deep learning models. Recently, it has been widely used in both reasearch and production. TF objective is to combine scale and flexibility.\n",
        "\n",
        "In the past session, we will learning the following:\n",
        "\n",
        "1. TF programming stack\n",
        "2. TF programming concepts including computatoin graphs, operations and sessions. \n",
        "3. Implementation of linear regression\n",
        "4. Implementation of feed-forward neural networks\n",
        "\n",
        "## TF stack:\n",
        "\n",
        "TensorFlow is a framework composed of two core building blocks — a library for defining computational graphs and a runtime for executing such graphs on a variety of different hardware\n",
        "\n",
        "\n",
        "![alt text](https://www.tensorflow.org/images/layers.png)\n",
        "\n",
        "\n",
        "Before goining into details about the stack, let us talk about computational graphs.\n",
        "\n",
        "### Computational Graphs\n",
        "\n",
        "A directed graph is a data structure consisting of nodes (vertices) and edges. It’s a set of vertices connected pairwise by directed edges.\n",
        "\n",
        "Graphs come in many shapes and sizes and are used to solve many real-life problems, such as representing networks including telephone networks, circuit networks, road networks, and even social networks. \n",
        "![alt text](https://cdn-images-1.medium.com/max/800/1*V6aYjD3AxDbEKYahkGqVQw.png)\n",
        "\n",
        "TensorFlow uses directed graphs internally to represent computations, and they call this data flow graphs (or computational graphs).\n",
        "\n",
        "The nodes in TF data flow graph mostly represents operations, variables and placeholders.\n",
        "\n",
        "Take for example the following operation:\n",
        "![alt text](https://cdn-images-1.medium.com/max/800/1*6E3sfit6DCeJ9mOz17g4bA.png)\n",
        "\n",
        "To create a computational graph out of this program, we create nodes for each of the operations in our program, along with the input variables a and b. In fact, a and b could be constants if they don’t change. If one node is used as the input to another operation we draw a directed arrow that goes from one node to another.\n",
        "\n",
        "The computational graph for this program might look like this:\n",
        "![alt text](https://cdn-images-1.medium.com/max/800/1*vPb9E0Yd1QUAD0oFmAgaOw.png)\n",
        "\n",
        "Operations create or manipulate data according to specific rules. In TensorFlow those rules are called Ops, short for operations. Variables on the other hand represent shared, persistent state that can be manipulated by running Ops on those variables.\n",
        "\n",
        "The questions now what are the advantages of representing operations as directed graphs: The main advantage of using directed graphs is the ability to do **parallelism** and what is called **dependency driving scheduling**. \n",
        "For example, consider again the follwoing code:\n",
        "![alt text](https://cdn-images-1.medium.com/max/800/1*6E3sfit6DCeJ9mOz17g4bA.png)\n",
        "At the most fundamental level, most computer programs are mainly composed of two things — primitive operations and an order in which these operations are executed, often sequentially, line by line. This means we would first multiply a and b and only when this expression was evaluated we would take their sum. Computational graphs on the otherhand, exclusively specify the dependencies across the operations.\n",
        "If we look at our computational graph we see that we could execute the multiplication and addition in parallel. That’s because these two operations do not depend on each other.\n",
        " So we can use the topology of the graph to drive the scheduling of operations and execute them in the most efficient manner, e.g. using multiple GPUs on a single machine or even distribute the execution across multiple machines.\n",
        " Another key advantage is portability. The graph is a language-independent representation of our code. So we can build the graph in Python, save the model (TensorFlow uses protocol buffers), and restore the model in a different language, say C++, if you want to go really fast.\n",
        " \n",
        " \n",
        "\n",
        "--------------------------------\n",
        "# References:\n",
        "\n",
        "https://medium.com/@d3lm/understand-tensorflow-by-mimicking-its-api-from-scratch-faa55787170d\n",
        "\n",
        "https://www.tensorflow.org/guide/extend/architecture\n",
        "\n",
        "https://www.tensorflow.org/guide/low_level_intro\n",
        "\n",
        "  \n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FGOQPMTyN1G",
        "colab_type": "text"
      },
      "source": [
        "##First version (one neuron):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdSoRpQRxPRR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.001\n",
        "training_iters = 200000\n",
        "\n",
        "\n",
        "n_inp = 3\n",
        "n_out = 4\n",
        "n_samples = 10\n",
        "\n",
        "x_gr = np.random.rand(n_samples, n_inp)\n",
        "y_gr = np.random.rand(n_samples, n_out)\n",
        "\n",
        "# tf Graph input\n",
        "x = tf.placeholder(tf.float32, [None, n_inp])\n",
        "y = tf.placeholder(tf.float32, [None, n_out])\n",
        "#keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n",
        "\n",
        "# weights\n",
        "w_1 = tf.Variable(tf.random_normal([n_inp, n_out]))\n",
        "\n",
        "# output of the model\n",
        "y_p = tf.matmul(x, w_1)   #matrix multiplication\n",
        "\n",
        "#cost\n",
        "cost = tf.reduce_mean(tf.pow(y-y_p,2))   #MSE\n",
        "\n",
        "\n",
        "# oprtimization\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
        "\n",
        "\n",
        "# Initializing the variables\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Launch the graph\n",
        "with tf.Session() as sess:\n",
        "  sess.run(init)\n",
        "                      \n",
        "  for i in range(100):\n",
        "    sess.run(optimizer, feed_dict={x: x_gr , y: y_gr })\n",
        "    pr_cost = sess.run (cost, feed_dict = {x: x_gr, y: y_gr})\n",
        "\n",
        "    print(\"cost \", pr_cost)\n",
        "  \n",
        "    \n",
        "  y_p_p = sess.run(y_p, feed_dict = {x: x_gr, y: y_gr})\n",
        "    \n",
        "  print (\"out: \", y_p_p)\n",
        "  print (\"out: \", y_gr)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJS_Za1IxR6s",
        "colab_type": "text"
      },
      "source": [
        "##Second version (multiple layers):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHH4kN7XoyEf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2101
        },
        "outputId": "06768f13-0274-43b0-f68e-cfda1dc9bec9"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.001\n",
        "training_iters = 200000\n",
        "\n",
        "\n",
        "n_inp = 3\n",
        "n_out = 4\n",
        "n_h1 = 4\n",
        "n_h2 = 10\n",
        "\n",
        "n_samples = 10\n",
        "\n",
        "x_gr = np.random.rand(n_samples, n_inp)\n",
        "y_gr = np.random.rand(n_samples, n_out)\n",
        "\n",
        "# tf Graph input\n",
        "x = tf.placeholder(tf.float32, [None, n_inp])\n",
        "y = tf.placeholder(tf.float32, [None, n_out])\n",
        "#keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n",
        "\n",
        "# weights\n",
        "w_1 = tf.Variable(tf.random_normal([n_inp, n_h1]))\n",
        "w_2 = tf.Variable(tf.random_normal([n_h1, n_h2]))\n",
        "w_3 = tf.Variable(tf.random_normal([n_h2, n_out]))\n",
        "\n",
        "#bias\n",
        "\n",
        "b_1 = tf.Variable (tf.random_normal([n_h1]))\n",
        "b_2 = tf.Variable (tf.random_normal([n_h2]))\n",
        "b_3 = tf.Variable (tf.random_normal([n_out]))\n",
        "\n",
        "\n",
        "# model\n",
        "h1 = tf.nn.sigmoid(tf.add(tf.matmul(x,w_1), b_1))   # can use relu too\n",
        "h2 = tf.nn.sigmoid(tf.add(tf.matmul(h1,w_2),b_2))\n",
        "\n",
        "\n",
        "y_p = tf.add(tf.matmul(h2, w_3), b_3)   # linear activation\n",
        "\n",
        "#cost\n",
        "cost = tf.reduce_mean(tf.pow(y-y_p,2))   # MSE\n",
        "\n",
        "\n",
        "# oprtimization\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
        "\n",
        "\n",
        "# Initializing the variables\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Launch the graph\n",
        "with tf.Session() as sess:\n",
        "  sess.run(init)\n",
        "                      \n",
        "  for i in range(100):\n",
        "    sess.run(optimizer, feed_dict={x: x_gr , y: y_gr })\n",
        "    pr_cost = sess.run (cost, feed_dict = {x: x_gr, y: y_gr})\n",
        "\n",
        "    print(\"cost \", pr_cost)\n",
        "  \n",
        "    \n",
        "  y_p_p = sess.run(y_p, feed_dict = {x: x_gr, y: y_gr})\n",
        "    \n",
        "  print (\"out: \", y_p_p)\n",
        "  print (\"out: \", y_gr)\n",
        "  "
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cost  1.9131916\n",
            "cost  1.8903126\n",
            "cost  1.8676132\n",
            "cost  1.8450956\n",
            "cost  1.8227621\n",
            "cost  1.8006153\n",
            "cost  1.7786567\n",
            "cost  1.756889\n",
            "cost  1.7353134\n",
            "cost  1.713932\n",
            "cost  1.6927468\n",
            "cost  1.6717583\n",
            "cost  1.6509686\n",
            "cost  1.6303784\n",
            "cost  1.6099888\n",
            "cost  1.5898008\n",
            "cost  1.5698152\n",
            "cost  1.5500324\n",
            "cost  1.5304531\n",
            "cost  1.5110775\n",
            "cost  1.4919058\n",
            "cost  1.4729382\n",
            "cost  1.4541743\n",
            "cost  1.4356142\n",
            "cost  1.4172575\n",
            "cost  1.3991036\n",
            "cost  1.3811524\n",
            "cost  1.363403\n",
            "cost  1.3458544\n",
            "cost  1.3285062\n",
            "cost  1.3113573\n",
            "cost  1.2944064\n",
            "cost  1.2776531\n",
            "cost  1.2610958\n",
            "cost  1.2447336\n",
            "cost  1.2285646\n",
            "cost  1.2125883\n",
            "cost  1.1968029\n",
            "cost  1.1812068\n",
            "cost  1.1657989\n",
            "cost  1.1505773\n",
            "cost  1.1355407\n",
            "cost  1.1206874\n",
            "cost  1.1060157\n",
            "cost  1.0915241\n",
            "cost  1.077211\n",
            "cost  1.0630744\n",
            "cost  1.0491128\n",
            "cost  1.0353247\n",
            "cost  1.0217078\n",
            "cost  1.0082608\n",
            "cost  0.9949819\n",
            "cost  0.9818691\n",
            "cost  0.9689209\n",
            "cost  0.9561354\n",
            "cost  0.9435109\n",
            "cost  0.93104565\n",
            "cost  0.9187377\n",
            "cost  0.9065858\n",
            "cost  0.89458764\n",
            "cost  0.88274175\n",
            "cost  0.87104654\n",
            "cost  0.8595002\n",
            "cost  0.84810096\n",
            "cost  0.836847\n",
            "cost  0.8257371\n",
            "cost  0.81476927\n",
            "cost  0.8039419\n",
            "cost  0.7932532\n",
            "cost  0.7827018\n",
            "cost  0.77228606\n",
            "cost  0.76200426\n",
            "cost  0.7518549\n",
            "cost  0.74183637\n",
            "cost  0.73194706\n",
            "cost  0.7221856\n",
            "cost  0.71255046\n",
            "cost  0.70304\n",
            "cost  0.69365287\n",
            "cost  0.6843875\n",
            "cost  0.67524254\n",
            "cost  0.66621625\n",
            "cost  0.65730774\n",
            "cost  0.6485149\n",
            "cost  0.6398371\n",
            "cost  0.63127244\n",
            "cost  0.6228196\n",
            "cost  0.6144775\n",
            "cost  0.6062445\n",
            "cost  0.59811944\n",
            "cost  0.5901011\n",
            "cost  0.5821878\n",
            "cost  0.57437885\n",
            "cost  0.5666725\n",
            "cost  0.55906785\n",
            "cost  0.5515634\n",
            "cost  0.5441581\n",
            "cost  0.53685063\n",
            "cost  0.52963984\n",
            "cost  0.5225245\n",
            "out:  [[-0.04157782  0.5897963   1.2095951  -0.5872015 ]\n",
            " [-0.08435512  0.6147671   1.2146966  -0.6192394 ]\n",
            " [-0.06539965  0.60865545  1.1996257  -0.581425  ]\n",
            " [ 0.08947015  0.72283363  1.1886425  -0.44269812]\n",
            " [-0.14296842  0.6321596   1.2235243  -0.67317605]\n",
            " [ 0.09055209  0.6771506   1.1829494  -0.44663942]\n",
            " [-0.09237468  0.6085818   1.2132462  -0.62388074]\n",
            " [-0.12567163  0.65029895  1.2320997  -0.6680953 ]\n",
            " [-0.03566551  0.60578763  1.2129159  -0.58731484]\n",
            " [ 0.09222007  0.7327405   1.1854562  -0.43041396]]\n",
            "out:  [[0.66953162 0.1625464  0.97115629 0.37670977]\n",
            " [0.58457022 0.60537945 0.39582247 0.43523289]\n",
            " [0.56093243 0.54663453 0.48177938 0.14834293]\n",
            " [0.17444581 0.45262493 0.16064107 0.82853472]\n",
            " [0.71083625 0.27743389 0.14918671 0.30809691]\n",
            " [0.12042472 0.19926897 0.62672935 0.58421244]\n",
            " [0.08198928 0.52045353 0.33664438 0.5274035 ]\n",
            " [0.54947138 0.6260512  0.28336755 0.33728225]\n",
            " [0.54535049 0.7287987  0.21673157 0.01766295]\n",
            " [0.31485906 0.59907164 0.08511251 0.42858249]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejI5RDQMyigT",
        "colab_type": "text"
      },
      "source": [
        "##Displaying results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjlhyGkQyoF_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9v3V8ztAKDCg",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "\n",
        "Before we jump into Tensorflow, we will implemented our first neural network model using Python Numpy package. NumPy is the fundamental package for scientific computing with Python, such as:\n",
        "\n",
        "1. Linear Algebra\n",
        "2. Statistics\n",
        "3. Calculus\n",
        "\n",
        "## A brief intro to Numpy operations:\n",
        "\n",
        "1. Creating a Vector:\n",
        "Here we use Numpy to create a 1-D Array which we then call a vector.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTf7M4r7Lgj9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load Library\n",
        "import numpy as np\n",
        "\n",
        "#Create a vector as a Row\n",
        "vector_row = np.array([1,2,3])\n",
        "\n",
        "#Create vector as a Column\n",
        "vector_column = np.array([[1],[2],[3]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYFjSo0OLqA3",
        "colab_type": "text"
      },
      "source": [
        "2. Creating a Matrix\n",
        "We Create a 2-D Array in Numpy and call it a Matrix. It contains 2 rows and 3 columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJlDBq5rLmA-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load Library\n",
        "import numpy as np\n",
        "\n",
        "#Create a Matrix\n",
        "matrix = np.array([[1,2,3],[4,5,6]])\n",
        "print(matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wv99hZqULygH",
        "colab_type": "text"
      },
      "source": [
        "3. Selecting Elements\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLQlxFzkPrKM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load Library\n",
        "import numpy as np\n",
        "\n",
        "#Create a vector as a Row\n",
        "vector_row = np.array([ 1,2,3,4,5,6 ])\n",
        "\n",
        "#Create a Matrix\n",
        "matrix = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
        "print(matrix)\n",
        "\n",
        "#Select 3rd element of Vector\n",
        "print(vector_row[2])\n",
        "\n",
        "#Select 2nd row 2nd column\n",
        "print(matrix[1,1])\n",
        "#Select all elements of a vector\n",
        "print(vector_row[:])\n",
        "#Select everything up to and including the 3rd element\n",
        "print(vector_row[:3])\n",
        "#Select the everything after the 3rd element\n",
        "print(vector_row[3:])\n",
        "#Select the last element\n",
        "print(vector_row[-1])\n",
        "#Select the first 2 rows and all the columns of the matrix\n",
        "print(matrix[:2,:])\n",
        "#Select all rows and the 2nd column of the matrix\n",
        "print(matrix[:,1:2])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QO3vTGEKQhm7",
        "colab_type": "text"
      },
      "source": [
        "4. Describing a Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8bDjBhhQpg5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "#Create a Matrix\n",
        "matrix = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
        "#View the Number of Rows and Columns\n",
        "print(matrix.shape)\n",
        "#View the number of elements (rows*columns)\n",
        "print(matrix.size)\n",
        "#View the number of Dimensions(2 in this case)\n",
        "print(matrix.ndim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKISvY8kQtA0",
        "colab_type": "text"
      },
      "source": [
        "5. Finding the max and min values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abPJd0JrQ4mM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load Library\n",
        "import numpy as np\n",
        "\n",
        "#Create a Matrix\n",
        "matrix = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
        "print(matrix)\n",
        "#Return the max element\n",
        "print(np.max(matrix))\n",
        "#Return the min element\n",
        "print(np.min(matrix))\n",
        "#To find the max element in each column\n",
        "print(np.max(matrix,axis=0))\n",
        "#To find the max element in each row\n",
        "print(np.max(matrix,axis=1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Qm64s_eR0zQ",
        "colab_type": "text"
      },
      "source": [
        "6. Reshaping Arrays\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pwepq7h_SBBD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load Library\n",
        "import numpy as np\n",
        "\n",
        "#Create a Matrix\n",
        "matrix = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
        "print(matrix)\n",
        "#Reshape\n",
        "print(matrix.reshape(9,1))\n",
        "#Here -1 says as many columns as needed and 1 row\n",
        "print(matrix.reshape(1,-1))\n",
        "#If we provide only 1 value Reshape would return a 1-d array of that length\n",
        "print(matrix.reshape(9))\n",
        "#We can also use the Flatten method to convert a matrix to 1-d array\n",
        "print(matrix.flatten())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJU3xABuVem_",
        "colab_type": "text"
      },
      "source": [
        "7. Calculating Dot Products"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPKg382VVivy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load Library\n",
        "import numpy as np\n",
        "\n",
        "#Create vector-1\n",
        "vector_1 = np.array([ 1,2,3 ])\n",
        "#Create vector-2\n",
        "vector_2 = np.array([ 4,5,6 ])\n",
        "#Calculate Dot Product\n",
        "print(np.dot(vector_1,vector_2))\n",
        "#Alternatively you can use @ to calculate dot products\n",
        "print(vector_1 @ vector_2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cB-jK7jEXY7F",
        "colab_type": "text"
      },
      "source": [
        "##Linear regression in Numpy:\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Write the numpy code for the following model:\n",
        "\n",
        "$Y=WX+B$\n",
        "\n",
        "where $X$ is 3x10 matrix:  10 samples and 3 features\n",
        "\n",
        "$Y$ is 4x10 matrix: 10 samples and 4 outputs\n",
        "\n",
        "$W$ is the weights matrix with the shape 4x3: connecting 3 inputs to 4 outputs\n",
        "\n",
        "$b$ is a vector with a size 4 ( one bias per output)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EtM5LVtWCpm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load Library\n",
        "import numpy as np\n",
        "\n",
        "# Generate a random X (we do not have a real data)\n",
        "X = np.random.rand(3,10)\n",
        "display(X.shape)\n",
        "\n",
        "# Generate a random weights vector\n",
        "W = np.random.rand(4,3)\n",
        "\n",
        "# Generate a random bias \n",
        "b = np.random.rand(4,1)\n",
        "\n",
        "# Calculate Y\n",
        "Y= np.dot(W,X) + b\n",
        "display(Y.shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMIoucH9hFfr",
        "colab_type": "text"
      },
      "source": [
        "## One neuron model in numpy:\n",
        "\n",
        "A single neuron has multiple inputs and one output, in addition to the linear regression model, we need to add non linearity through an activation function:\n",
        "\n",
        "$Y= f(WX+B)$\n",
        "\n",
        "where $X$ is n x m matrix:  m samples and n features/inputs\n",
        "\n",
        "$f(g)= \\frac{1}{1+\\exp(-g)}$  is a sigmoid acitavation function\n",
        "\n",
        "$Y$ is nh1 x m matrix: m samples and ny outputs\n",
        "\n",
        "$W$ is the weights matrix with the shape nh1 x n: connecting 3 inputs to 4 outputs\n",
        "\n",
        "$b$ is a vector with a size nh1 ( one bias per output)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qry1JDGEiLmx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load Library\n",
        "import numpy as np \n",
        "\n",
        "f = lambda x: 1.0/(1.0 + np.exp(-x)) # activation function (use sigmoid)\n",
        "\n",
        "# Generate a random X (we do not have a real data)\n",
        "X = np.random.rand(3,10)\n",
        "\n",
        "\n",
        "# Generate a random weights vector\n",
        "W = np.random.rand(1,3)\n",
        "\n",
        "\n",
        "# Generate a random bias \n",
        "b = np.random.rand()\n",
        "\n",
        "# Calculate Y\n",
        "Y= f(np.dot(W,X) + b)\n",
        "display(Y)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSnbti9ooIIs",
        "colab_type": "text"
      },
      "source": [
        "## One hidden layer model in numpy:\n",
        "\n",
        "The difference from the one neuron model is simple:  we need only to change the number of output \"ny\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAY3o6zBnpA0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load Library\n",
        "import numpy as np \n",
        "\n",
        "#Suppose we have the following NN architecture\n",
        "\n",
        "m = 10 # Number of samples\n",
        "ni= 3 # Number of input neurons\n",
        "h = 1 # Number of hidden layers\n",
        "nh1 = 4 # Number of neurons in the hidden layer 1\n",
        "no =1 # Number of neurons in the output layer\n",
        "\n",
        "\n",
        "\n",
        "f = lambda x: 1.0/(1.0 + np.exp(-x)) # activation function (use sigmoid)\n",
        "\n",
        "# Generate a random X (we do not have a real data)\n",
        "X = np.random.rand(ni,m)\n",
        "\n",
        "\n",
        "# Generate a random weights vector for the first hidden layer\n",
        "W1 = np.random.rand(nh1,ni)\n",
        "\n",
        "\n",
        "# Generate a random bias for the first hidden layer \n",
        "b1 = np.random.rand(nh1,1)\n",
        "\n",
        "# Generate a random weights vector for the output layer\n",
        "W2 = np.random.rand(no,nh1)\n",
        "\n",
        "# Generate a random bias for the output layer \n",
        "b2 = np.random.rand(no,1)\n",
        "\n",
        "# Calculate output of the first hidden layer\n",
        "Yh1= f(np.dot(W1,X) + b1)\n",
        "\n",
        "# Calculate output of the output layer\n",
        "\n",
        "Y= f(np.dot(W2,Yh1) + b2)\n",
        "\n",
        "display(Yh1.shape)\n",
        "display(Y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11fqb_bQvIEi",
        "colab_type": "text"
      },
      "source": [
        "## Gradient descent in Numpy:\n",
        "Let us now start training a neural network\n",
        "We start by implementing a simple gradient descent for linear regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzBJxwb7FFZ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaQyLoxk2FyG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "converged = False\n",
        "iter = 0\n",
        "m = 10 # Number of samples\n",
        "ni= 1 # Number of input neurons\n",
        "h = 1 # Number of hidden layers\n",
        "no =1 # Number of neurons in the output layer\n",
        "\n",
        "# Generate a random X (we do not have a real data)\n",
        "X = np.random.rand(m)\n",
        "display(X)\n",
        "\n",
        "# learning rate\n",
        "alpha =0.01\n",
        "\n",
        "# early stop criteria \n",
        "ep=0.001\n",
        "\n",
        "# maximum number of training iterations\n",
        "max_iter=100\n",
        "\n",
        "# Generate a random weights vector for the output layer\n",
        "W1 = np.random.rand()\n",
        "\n",
        "# Generate a random bias for the output layer \n",
        "b1 = np.random.rand()\n",
        "\n",
        "# Generate a random ground truth\n",
        "Y_gr = np.random.rand(m)\n",
        "\n",
        "\n",
        "J = sum([(b1 + W1*X[i] - Y_gr[i])**2 for i in range(m)])\n",
        "\n",
        "while not converged:\n",
        "        # for each training sample, compute the gradient (d/d_theta j(theta))\n",
        "        grad0 = 1.0/m * sum([(b1 + W1*X[i] - Y_gr[i]) for i in range(m)]) \n",
        "        grad1 = 1.0/m * sum([(b1 + W1*X[i] - Y_gr[i])*X[i] for i in range(m)])\n",
        "        \n",
        "        # update the theta_temp\n",
        "        temp0 = W1 - alpha * grad0\n",
        "        temp1 = b1 - alpha * grad1\n",
        "        # update theta\n",
        "        W1 = temp0\n",
        "        b1 = temp1\n",
        "        \n",
        "        # sum squared error\n",
        "        e = sum([(b1 + W1*X[i] - Y_gr[i])**2 for i in range(m)]) \n",
        "\n",
        "        if abs(J-e) <= ep:\n",
        "            print('Converged, iterations: ', iter, '!!!')\n",
        "            converged = True\n",
        "    \n",
        "        J = e   # update error \n",
        "        iter += 1  # update iter\n",
        "    \n",
        "        if iter == max_iter:\n",
        "            print('Max interactions exceeded!')\n",
        "            converged = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRqCCVlmFIZl",
        "colab_type": "text"
      },
      "source": [
        "##Assignment 1\n",
        "### Backpropagation in Numpy:\n"
      ]
    }
  ]
}