{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Session 2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahmadhajmosa/Machine-learning-labs/blob/Michal/Session_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmSUExzC-ZBV",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgWZldyi-Zef",
        "colab_type": "text"
      },
      "source": [
        "# Lab on Machine Learning and Applications in Intelligent Vehicles\n",
        "## Session 1: Introduction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a75NLOuwKXSs",
        "colab_type": "text"
      },
      "source": [
        "#Session 2: 05.06 - 13:00 - 14:30 :\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciUtGM-W_ehP",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "## Intro:\n",
        "\n",
        "Tensorflow is a powerful framework for implementing and deploying large-scale deep learning models. Recently, it has been widely used in both reasearch and production. TF objective is to combine scale and flexibility.\n",
        "\n",
        "In the past session, we will learning the following:\n",
        "\n",
        "1. TF programming stack\n",
        "2. TF programming concepts including computatoin graphs, operations and sessions. \n",
        "3. Implementation of linear regression\n",
        "4. Implementation of feed-forward neural networks\n",
        "\n",
        "## TF stack:\n",
        "\n",
        "TensorFlow is a framework composed of two core building blocks — a library for defining computational graphs and a runtime for executing such graphs on a variety of different hardware\n",
        "\n",
        "\n",
        "![alt text](https://www.tensorflow.org/images/layers.png)\n",
        "\n",
        "\n",
        "Before goining into details about the stack, let us talk about computational graphs.\n",
        "\n",
        "### Computational Graphs\n",
        "\n",
        "A directed graph is a data structure consisting of nodes (vertices) and edges. It’s a set of vertices connected pairwise by directed edges.\n",
        "\n",
        "Graphs come in many shapes and sizes and are used to solve many real-life problems, such as representing networks including telephone networks, circuit networks, road networks, and even social networks. \n",
        "![alt text](https://cdn-images-1.medium.com/max/800/1*V6aYjD3AxDbEKYahkGqVQw.png)\n",
        "\n",
        "TensorFlow uses directed graphs internally to represent computations, and they call this data flow graphs (or computational graphs).\n",
        "\n",
        "The nodes in TF data flow graph mostly represents operations, variables and placeholders.\n",
        "\n",
        "Take for example the following operation:\n",
        "![alt text](https://cdn-images-1.medium.com/max/800/1*6E3sfit6DCeJ9mOz17g4bA.png)\n",
        "\n",
        "To create a computational graph out of this program, we create nodes for each of the operations in our program, along with the input variables a and b. In fact, a and b could be constants if they don’t change. If one node is used as the input to another operation we draw a directed arrow that goes from one node to another.\n",
        "\n",
        "The computational graph for this program might look like this:\n",
        "![alt text](https://cdn-images-1.medium.com/max/800/1*vPb9E0Yd1QUAD0oFmAgaOw.png)\n",
        "\n",
        "Operations create or manipulate data according to specific rules. In TensorFlow those rules are called Ops, short for operations. Variables on the other hand represent shared, persistent state that can be manipulated by running Ops on those variables.\n",
        "\n",
        "The questions now what are the advantages of representing operations as directed graphs: The main advantage of using directed graphs is the ability to do **parallelism** and what is called **dependency driving scheduling**. \n",
        "For example, consider again the follwoing code:\n",
        "![alt text](https://cdn-images-1.medium.com/max/800/1*6E3sfit6DCeJ9mOz17g4bA.png)\n",
        "At the most fundamental level, most computer programs are mainly composed of two things — primitive operations and an order in which these operations are executed, often sequentially, line by line. This means we would first multiply a and b and only when this expression was evaluated we would take their sum. Computational graphs on the otherhand, exclusively specify the dependencies across the operations.\n",
        "If we look at our computational graph we see that we could execute the multiplication and addition in parallel. That’s because these two operations do not depend on each other.\n",
        " So we can use the topology of the graph to drive the scheduling of operations and execute them in the most efficient manner, e.g. using multiple GPUs on a single machine or even distribute the execution across multiple machines.\n",
        " Another key advantage is portability. The graph is a language-independent representation of our code. So we can build the graph in Python, save the model (TensorFlow uses protocol buffers), and restore the model in a different language, say C++, if you want to go really fast.\n",
        " \n",
        " \n",
        "\n",
        "--------------------------------\n",
        "# References:\n",
        "\n",
        "https://medium.com/@d3lm/understand-tensorflow-by-mimicking-its-api-from-scratch-faa55787170d\n",
        "\n",
        "https://www.tensorflow.org/guide/extend/architecture\n",
        "\n",
        "https://www.tensorflow.org/guide/low_level_intro\n",
        "\n",
        "  \n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FGOQPMTyN1G",
        "colab_type": "text"
      },
      "source": [
        "##First version (one neuron):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdSoRpQRxPRR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.001\n",
        "training_iters = 200000\n",
        "\n",
        "\n",
        "n_inp = 3\n",
        "n_out = 4\n",
        "n_samples = 10\n",
        "\n",
        "x_gr = np.random.rand(n_samples, n_inp)\n",
        "y_gr = np.random.rand(n_samples, n_out)\n",
        "\n",
        "# tf Graph input\n",
        "x = tf.placeholder(tf.float32, [None, n_inp])\n",
        "y = tf.placeholder(tf.float32, [None, n_out])\n",
        "#keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n",
        "\n",
        "# weights\n",
        "w_1 = tf.Variable(tf.random_normal([n_inp, n_out]))\n",
        "\n",
        "# output of the model\n",
        "y_p = tf.matmul(x, w_1)   #matrix multiplication\n",
        "\n",
        "#cost\n",
        "cost = tf.reduce_mean(tf.pow(y-y_p,2))   #MSE\n",
        "\n",
        "\n",
        "# oprtimization\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
        "\n",
        "\n",
        "# Initializing the variables\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Launch the graph\n",
        "with tf.Session() as sess:\n",
        "  sess.run(init)\n",
        "                      \n",
        "  for i in range(100):\n",
        "    sess.run(optimizer, feed_dict={x: x_gr , y: y_gr })\n",
        "    pr_cost = sess.run (cost, feed_dict = {x: x_gr, y: y_gr})\n",
        "\n",
        "    print(\"cost \", pr_cost)\n",
        "  \n",
        "    \n",
        "  y_p_p = sess.run(y_p, feed_dict = {x: x_gr, y: y_gr})\n",
        "    \n",
        "  print (\"out: \", y_p_p)\n",
        "  print (\"out: \", y_gr)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJS_Za1IxR6s",
        "colab_type": "text"
      },
      "source": [
        "##Second version (multiple layers):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHH4kN7XoyEf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.001\n",
        "training_iters = 200000\n",
        "\n",
        "\n",
        "n_inp = 3\n",
        "n_out = 4\n",
        "n_h1 = 4\n",
        "n_h2 = 10\n",
        "\n",
        "n_samples = 10\n",
        "\n",
        "x_gr = np.random.rand(n_samples, n_inp)\n",
        "y_gr = np.random.rand(n_samples, n_out)\n",
        "\n",
        "# tf Graph input\n",
        "x = tf.placeholder(tf.float32, [None, n_inp])\n",
        "y = tf.placeholder(tf.float32, [None, n_out])\n",
        "#keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n",
        "\n",
        "# weights\n",
        "w_1 = tf.Variable(tf.random_normal([n_inp, n_h1]))\n",
        "w_2 = tf.Variable(tf.random_normal([n_h1, n_h2]))\n",
        "w_3 = tf.Variable(tf.random_normal([n_h2, n_out]))\n",
        "\n",
        "#bias\n",
        "\n",
        "b_1 = tf.Variable (tf.random_normal([n_h1]))\n",
        "b_2 = tf.Variable (tf.random_normal([n_h2]))\n",
        "b_3 = tf.Variable (tf.random_normal([n_out]))\n",
        "\n",
        "\n",
        "# model\n",
        "h1 = tf.nn.sigmoid(tf.add(tf.matmul(x,w_1), b_1))   # can use relu too\n",
        "h2 = tf.nn.sigmoid(tf.add(tf.matmul(h1,w_2),b_2))\n",
        "\n",
        "\n",
        "y_p = tf.add(tf.matmul(h2, w_3), b_3)   # linear activation\n",
        "\n",
        "#cost\n",
        "cost = tf.reduce_mean(tf.pow(y-y_p,2))   # MSE\n",
        "\n",
        "\n",
        "# oprtimization\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
        "\n",
        "\n",
        "# Initializing the variables\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Launch the graph\n",
        "with tf.Session() as sess:\n",
        "  sess.run(init)\n",
        "                      \n",
        "  for i in range(100):\n",
        "    sess.run(optimizer, feed_dict={x: x_gr , y: y_gr })\n",
        "    pr_cost = sess.run (cost, feed_dict = {x: x_gr, y: y_gr})\n",
        "\n",
        "    print(\"cost \", pr_cost)\n",
        "  \n",
        "    \n",
        "  y_p_p = sess.run(y_p, feed_dict = {x: x_gr, y: y_gr})\n",
        "    \n",
        "  print (\"out: \", y_p_p)\n",
        "  print (\"out: \", y_gr)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejI5RDQMyigT",
        "colab_type": "text"
      },
      "source": [
        "##\n",
        "Displaying results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjlhyGkQyoF_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "351be35f-da66-4f1c-cbd1-3038508f5ce8"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Import MNIST data\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
        "\n",
        "mnist.train.images.shape\n",
        "# MNIST data input (img shape: 28*28)\n",
        "\n",
        "\n",
        "image  = mnist.train.images[1].reshape((28,28))\n",
        "\n",
        "print (mnist.train.labels[1])\n",
        "\n",
        "plt.imshow(image)\n",
        "\n",
        "print (np.max(image), np.min(image))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
            "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
            "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "1.0 0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngiL8QFCEHwiK8ANBEX4gKMIPBPX/EhqoeSQulYEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjUQopb81BK1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34750
        },
        "outputId": "000d7b93-112b-4436-fd02-3b1a479917ab"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "#training\n",
        "X_train = mnist.train.images\n",
        "Y_train = mnist.train.labels\n",
        "\n",
        "#test\n",
        "X_test = mnist.train.images\n",
        "Y_test = mnist.train.labels\n",
        "\n",
        "#validation\n",
        "X_val = mnist.train.images\n",
        "Y_val = mnist.train.labels\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.001\n",
        "training_iters = 200000\n",
        "batch_size = 128\n",
        "\n",
        "\n",
        "n_inp = 784\n",
        "n_h1 = 100\n",
        "n_h2 = 100\n",
        "n_out = 10\n",
        "\n",
        "\n",
        "# tf Graph input\n",
        "x = tf.placeholder(tf.float32, [None, n_inp])\n",
        "y = tf.placeholder(tf.float32, [None, n_out])\n",
        "#keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n",
        "\n",
        "# weights\n",
        "w_1 = tf.Variable(tf.random_normal([n_inp, n_h1]))\n",
        "w_2 = tf.Variable(tf.random_normal([n_h1, n_h2]))\n",
        "w_3 = tf.Variable(tf.random_normal([n_h2, n_out]))\n",
        "\n",
        "#bias\n",
        "\n",
        "b_1 = tf.Variable (tf.random_normal([n_h1]))\n",
        "b_2 = tf.Variable (tf.random_normal([n_h2]))\n",
        "b_3 = tf.Variable (tf.random_normal([n_out]))\n",
        "\n",
        "\n",
        "# model\n",
        "h1 = tf.nn.sigmoid(tf.add(tf.matmul(x,w_1), b_1))   # can use relu too\n",
        "h2 = tf.nn.sigmoid(tf.add(tf.matmul(h1,w_2),b_2))\n",
        "\n",
        "\n",
        "y_p = tf.add(tf.matmul(h2, w_3), b_3)   # linear activation\n",
        "\n",
        "#cost\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_p, labels = y)) \n",
        "\n",
        "\n",
        "# Evaluate model\n",
        "correct_pred = tf.equal(tf.argmax(y_p, 1), tf.argmax(y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "\n",
        "# oprtimization\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
        "\n",
        "\n",
        "# Initializing the variables\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Launch the graph\n",
        "with tf.Session() as sess:\n",
        "  sess.run(init)\n",
        "                      \n",
        "  for i in range(1000):\n",
        "    \n",
        "    batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
        "    \n",
        "    sess.run(optimizer, feed_dict={x: batch_x , y: batch_y })\n",
        "    train_cost, train_acc = sess.run ([cost,accuracy], feed_dict={x: batch_x , y: batch_y })\n",
        "\n",
        "    print(\"cost \", train_cost, \"acc: \", train_acc)\n",
        "    \n",
        "    test_batch_x, test_batch_y = mnist.test.next_batch(batch_size)\n",
        "    \n",
        "    test_cost, test_acc = sess.run ([cost,accuracy], feed_dict={x: test_batch_x , y: test_batch_y })\n",
        "    print(\"cost2 \", test_cost, \"acc2: \", test_acc)\n",
        "    \n",
        "  #y_p_p = sess.run(y_p, feed_dict = {x: x_gr, y: y_gr})\n",
        "    \n",
        "#   print (\"out: \", y_p_p)\n",
        "#   print (\"out: \", y_gr)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cost  8.418411 acc:  0.0703125\n",
            "cost2  7.9525013 acc2:  0.0546875\n",
            "cost  7.557711 acc:  0.109375\n",
            "cost2  7.309879 acc2:  0.09375\n",
            "cost  7.50981 acc:  0.0703125\n",
            "cost2  7.298354 acc2:  0.0703125\n",
            "cost  7.760049 acc:  0.046875\n",
            "cost2  6.3965206 acc2:  0.140625\n",
            "cost  6.947362 acc:  0.0859375\n",
            "cost2  7.064424 acc2:  0.09375\n",
            "cost  6.984566 acc:  0.0859375\n",
            "cost2  6.828556 acc2:  0.078125\n",
            "cost  6.417666 acc:  0.171875\n",
            "cost2  7.0806327 acc2:  0.1015625\n",
            "cost  6.7837086 acc:  0.140625\n",
            "cost2  5.465892 acc2:  0.1328125\n",
            "cost  6.4945655 acc:  0.078125\n",
            "cost2  6.9391317 acc2:  0.078125\n",
            "cost  6.7596154 acc:  0.0703125\n",
            "cost2  5.977803 acc2:  0.140625\n",
            "cost  6.9567747 acc:  0.09375\n",
            "cost2  6.509716 acc2:  0.0859375\n",
            "cost  6.4179206 acc:  0.1484375\n",
            "cost2  5.875046 acc2:  0.109375\n",
            "cost  6.0638666 acc:  0.1484375\n",
            "cost2  5.8836155 acc2:  0.140625\n",
            "cost  5.708351 acc:  0.1171875\n",
            "cost2  5.8294826 acc2:  0.1328125\n",
            "cost  5.520355 acc:  0.140625\n",
            "cost2  6.180374 acc2:  0.109375\n",
            "cost  5.891445 acc:  0.1328125\n",
            "cost2  5.4085207 acc2:  0.125\n",
            "cost  5.4462276 acc:  0.1171875\n",
            "cost2  5.69895 acc2:  0.0859375\n",
            "cost  5.240784 acc:  0.1015625\n",
            "cost2  5.568157 acc2:  0.2109375\n",
            "cost  5.399582 acc:  0.1328125\n",
            "cost2  4.8941894 acc2:  0.1875\n",
            "cost  5.195423 acc:  0.1328125\n",
            "cost2  4.965636 acc2:  0.140625\n",
            "cost  4.9496365 acc:  0.140625\n",
            "cost2  5.8009653 acc2:  0.0625\n",
            "cost  5.0790706 acc:  0.140625\n",
            "cost2  5.1398644 acc2:  0.1640625\n",
            "cost  4.4536624 acc:  0.1328125\n",
            "cost2  5.021368 acc2:  0.140625\n",
            "cost  5.5493994 acc:  0.109375\n",
            "cost2  5.1305857 acc2:  0.1640625\n",
            "cost  5.18351 acc:  0.1328125\n",
            "cost2  5.502136 acc2:  0.140625\n",
            "cost  4.635431 acc:  0.15625\n",
            "cost2  4.3924217 acc2:  0.2265625\n",
            "cost  4.8982344 acc:  0.109375\n",
            "cost2  5.179473 acc2:  0.140625\n",
            "cost  4.859763 acc:  0.1640625\n",
            "cost2  5.1610804 acc2:  0.1640625\n",
            "cost  4.7453294 acc:  0.140625\n",
            "cost2  4.022071 acc2:  0.1796875\n",
            "cost  4.825399 acc:  0.1171875\n",
            "cost2  4.792165 acc2:  0.1875\n",
            "cost  4.690005 acc:  0.1640625\n",
            "cost2  4.4000673 acc2:  0.203125\n",
            "cost  4.207568 acc:  0.1875\n",
            "cost2  3.898438 acc2:  0.1640625\n",
            "cost  4.7002945 acc:  0.109375\n",
            "cost2  4.2372913 acc2:  0.1875\n",
            "cost  3.5946069 acc:  0.2421875\n",
            "cost2  4.0990415 acc2:  0.1796875\n",
            "cost  4.1388664 acc:  0.1796875\n",
            "cost2  4.3747993 acc2:  0.1953125\n",
            "cost  4.113629 acc:  0.1953125\n",
            "cost2  4.095555 acc2:  0.1875\n",
            "cost  4.0075836 acc:  0.234375\n",
            "cost2  3.9296649 acc2:  0.171875\n",
            "cost  3.9397392 acc:  0.1953125\n",
            "cost2  3.517857 acc2:  0.265625\n",
            "cost  4.1246123 acc:  0.2109375\n",
            "cost2  3.736627 acc2:  0.234375\n",
            "cost  4.1744914 acc:  0.1796875\n",
            "cost2  3.6252089 acc2:  0.203125\n",
            "cost  3.8152637 acc:  0.1796875\n",
            "cost2  4.343514 acc2:  0.2109375\n",
            "cost  4.447231 acc:  0.1640625\n",
            "cost2  3.5284147 acc2:  0.25\n",
            "cost  3.3749876 acc:  0.203125\n",
            "cost2  4.157873 acc2:  0.2109375\n",
            "cost  3.6775935 acc:  0.1796875\n",
            "cost2  4.1213093 acc2:  0.1484375\n",
            "cost  3.4986444 acc:  0.21875\n",
            "cost2  3.9977012 acc2:  0.21875\n",
            "cost  3.7164507 acc:  0.234375\n",
            "cost2  3.7361536 acc2:  0.1953125\n",
            "cost  3.354748 acc:  0.2421875\n",
            "cost2  3.5188322 acc2:  0.21875\n",
            "cost  3.7752504 acc:  0.2109375\n",
            "cost2  3.6120837 acc2:  0.265625\n",
            "cost  3.025547 acc:  0.25\n",
            "cost2  3.302682 acc2:  0.3125\n",
            "cost  3.2462602 acc:  0.28125\n",
            "cost2  3.072654 acc2:  0.3046875\n",
            "cost  3.0618215 acc:  0.3359375\n",
            "cost2  3.4065645 acc2:  0.203125\n",
            "cost  3.3456886 acc:  0.1953125\n",
            "cost2  3.0294776 acc2:  0.2421875\n",
            "cost  2.9968724 acc:  0.3046875\n",
            "cost2  2.8173614 acc2:  0.34375\n",
            "cost  3.0888042 acc:  0.296875\n",
            "cost2  3.0079005 acc2:  0.28125\n",
            "cost  2.7875783 acc:  0.3203125\n",
            "cost2  3.3736124 acc2:  0.2421875\n",
            "cost  3.1249812 acc:  0.265625\n",
            "cost2  3.085713 acc2:  0.265625\n",
            "cost  2.8661342 acc:  0.328125\n",
            "cost2  3.0161204 acc2:  0.265625\n",
            "cost  2.7225525 acc:  0.2578125\n",
            "cost2  3.0058277 acc2:  0.2734375\n",
            "cost  2.718896 acc:  0.3359375\n",
            "cost2  2.7831447 acc2:  0.2890625\n",
            "cost  2.5345871 acc:  0.3046875\n",
            "cost2  2.6692176 acc2:  0.296875\n",
            "cost  2.775772 acc:  0.3046875\n",
            "cost2  2.8605528 acc2:  0.25\n",
            "cost  2.7528784 acc:  0.265625\n",
            "cost2  2.7547803 acc2:  0.296875\n",
            "cost  2.892023 acc:  0.2578125\n",
            "cost2  2.8334472 acc2:  0.3359375\n",
            "cost  2.9440818 acc:  0.25\n",
            "cost2  2.431858 acc2:  0.375\n",
            "cost  2.6074662 acc:  0.34375\n",
            "cost2  2.7003179 acc2:  0.3125\n",
            "cost  2.4813337 acc:  0.3125\n",
            "cost2  2.3846388 acc2:  0.34375\n",
            "cost  2.8690917 acc:  0.3046875\n",
            "cost2  3.1248355 acc2:  0.234375\n",
            "cost  2.4615488 acc:  0.34375\n",
            "cost2  2.7158005 acc2:  0.2890625\n",
            "cost  2.6740847 acc:  0.2734375\n",
            "cost2  2.7121835 acc2:  0.3671875\n",
            "cost  2.2136695 acc:  0.3984375\n",
            "cost2  2.5925422 acc2:  0.3515625\n",
            "cost  2.398977 acc:  0.2890625\n",
            "cost2  2.2196643 acc2:  0.4140625\n",
            "cost  2.764748 acc:  0.3125\n",
            "cost2  2.2731555 acc2:  0.3359375\n",
            "cost  2.1842463 acc:  0.4296875\n",
            "cost2  2.5349352 acc2:  0.3359375\n",
            "cost  2.8062913 acc:  0.34375\n",
            "cost2  2.6916478 acc2:  0.359375\n",
            "cost  1.9570446 acc:  0.4375\n",
            "cost2  2.1981335 acc2:  0.40625\n",
            "cost  2.635207 acc:  0.2890625\n",
            "cost2  2.3243334 acc2:  0.375\n",
            "cost  2.6486983 acc:  0.3359375\n",
            "cost2  2.3039603 acc2:  0.375\n",
            "cost  2.5321527 acc:  0.328125\n",
            "cost2  2.1919272 acc2:  0.375\n",
            "cost  2.3021545 acc:  0.390625\n",
            "cost2  2.329438 acc2:  0.34375\n",
            "cost  2.1972785 acc:  0.4140625\n",
            "cost2  2.162324 acc2:  0.359375\n",
            "cost  2.0321467 acc:  0.4296875\n",
            "cost2  2.1644163 acc2:  0.4140625\n",
            "cost  2.305272 acc:  0.328125\n",
            "cost2  2.2223113 acc2:  0.40625\n",
            "cost  2.4756444 acc:  0.359375\n",
            "cost2  2.1561427 acc2:  0.359375\n",
            "cost  2.0418553 acc:  0.3984375\n",
            "cost2  2.269432 acc2:  0.375\n",
            "cost  2.0860538 acc:  0.4375\n",
            "cost2  2.3353777 acc2:  0.4140625\n",
            "cost  2.0698166 acc:  0.4140625\n",
            "cost2  2.2629142 acc2:  0.390625\n",
            "cost  2.2758136 acc:  0.390625\n",
            "cost2  2.0723686 acc2:  0.3984375\n",
            "cost  2.092457 acc:  0.4375\n",
            "cost2  1.8831863 acc2:  0.40625\n",
            "cost  2.0551226 acc:  0.484375\n",
            "cost2  2.086872 acc2:  0.4609375\n",
            "cost  2.2974539 acc:  0.390625\n",
            "cost2  2.1239612 acc2:  0.453125\n",
            "cost  1.9788208 acc:  0.4140625\n",
            "cost2  1.8179064 acc2:  0.4765625\n",
            "cost  1.9364543 acc:  0.453125\n",
            "cost2  1.8827896 acc2:  0.4453125\n",
            "cost  2.059277 acc:  0.3984375\n",
            "cost2  1.8416989 acc2:  0.4609375\n",
            "cost  2.0402312 acc:  0.4453125\n",
            "cost2  1.9381672 acc2:  0.453125\n",
            "cost  1.9779763 acc:  0.4921875\n",
            "cost2  1.8396583 acc2:  0.4453125\n",
            "cost  1.8798404 acc:  0.453125\n",
            "cost2  1.7614847 acc2:  0.46875\n",
            "cost  1.7485887 acc:  0.5\n",
            "cost2  1.804011 acc2:  0.4765625\n",
            "cost  1.7798043 acc:  0.4453125\n",
            "cost2  1.9521254 acc2:  0.4375\n",
            "cost  2.0042536 acc:  0.4609375\n",
            "cost2  2.278016 acc2:  0.4296875\n",
            "cost  2.0245326 acc:  0.359375\n",
            "cost2  1.7772983 acc2:  0.5\n",
            "cost  1.564512 acc:  0.46875\n",
            "cost2  1.8647025 acc2:  0.484375\n",
            "cost  1.9535257 acc:  0.4609375\n",
            "cost2  1.9240665 acc2:  0.421875\n",
            "cost  2.0701241 acc:  0.390625\n",
            "cost2  2.0231853 acc2:  0.5\n",
            "cost  1.8330873 acc:  0.46875\n",
            "cost2  1.9151974 acc2:  0.4453125\n",
            "cost  2.0292983 acc:  0.453125\n",
            "cost2  1.9649572 acc2:  0.3984375\n",
            "cost  2.014026 acc:  0.4375\n",
            "cost2  1.8533955 acc2:  0.4453125\n",
            "cost  1.7888008 acc:  0.484375\n",
            "cost2  1.9893413 acc2:  0.4609375\n",
            "cost  1.904229 acc:  0.4765625\n",
            "cost2  1.6318903 acc2:  0.546875\n",
            "cost  1.9310017 acc:  0.4296875\n",
            "cost2  1.9353892 acc2:  0.4296875\n",
            "cost  1.9151555 acc:  0.484375\n",
            "cost2  1.8666254 acc2:  0.46875\n",
            "cost  1.9649324 acc:  0.4140625\n",
            "cost2  2.1258564 acc2:  0.375\n",
            "cost  1.8982644 acc:  0.46875\n",
            "cost2  1.7169483 acc2:  0.5234375\n",
            "cost  1.8755243 acc:  0.484375\n",
            "cost2  1.912403 acc2:  0.484375\n",
            "cost  1.7649894 acc:  0.515625\n",
            "cost2  1.6235384 acc2:  0.4921875\n",
            "cost  1.6057333 acc:  0.5625\n",
            "cost2  1.7180037 acc2:  0.4765625\n",
            "cost  1.462136 acc:  0.5390625\n",
            "cost2  1.8768356 acc2:  0.4609375\n",
            "cost  1.7459936 acc:  0.5234375\n",
            "cost2  1.7350154 acc2:  0.484375\n",
            "cost  1.5247887 acc:  0.546875\n",
            "cost2  1.7740171 acc2:  0.5078125\n",
            "cost  1.67773 acc:  0.5\n",
            "cost2  1.9101813 acc2:  0.5\n",
            "cost  1.6859891 acc:  0.453125\n",
            "cost2  1.826581 acc2:  0.4921875\n",
            "cost  1.763248 acc:  0.5390625\n",
            "cost2  1.6858186 acc2:  0.53125\n",
            "cost  1.8749061 acc:  0.4921875\n",
            "cost2  1.8099585 acc2:  0.5390625\n",
            "cost  1.8076371 acc:  0.5390625\n",
            "cost2  1.667649 acc2:  0.484375\n",
            "cost  2.012682 acc:  0.390625\n",
            "cost2  1.8725133 acc2:  0.4609375\n",
            "cost  1.7711407 acc:  0.53125\n",
            "cost2  1.7115017 acc2:  0.46875\n",
            "cost  1.5272582 acc:  0.5859375\n",
            "cost2  1.518142 acc2:  0.53125\n",
            "cost  1.7852378 acc:  0.484375\n",
            "cost2  1.4483216 acc2:  0.5625\n",
            "cost  1.7759203 acc:  0.4453125\n",
            "cost2  1.7408339 acc2:  0.546875\n",
            "cost  1.7391828 acc:  0.453125\n",
            "cost2  1.3851359 acc2:  0.5859375\n",
            "cost  1.4732962 acc:  0.5546875\n",
            "cost2  1.5895858 acc2:  0.5390625\n",
            "cost  1.376384 acc:  0.59375\n",
            "cost2  1.6432157 acc2:  0.5390625\n",
            "cost  1.8032296 acc:  0.5\n",
            "cost2  1.6453913 acc2:  0.578125\n",
            "cost  1.4313877 acc:  0.515625\n",
            "cost2  1.6972752 acc2:  0.53125\n",
            "cost  1.8066456 acc:  0.484375\n",
            "cost2  1.5156622 acc2:  0.5546875\n",
            "cost  1.2707276 acc:  0.6015625\n",
            "cost2  1.6509707 acc2:  0.5078125\n",
            "cost  1.4121506 acc:  0.5625\n",
            "cost2  1.5151267 acc2:  0.5703125\n",
            "cost  1.7655942 acc:  0.4921875\n",
            "cost2  1.7250113 acc2:  0.5390625\n",
            "cost  1.7549008 acc:  0.53125\n",
            "cost2  1.6034484 acc2:  0.4765625\n",
            "cost  1.7074802 acc:  0.5234375\n",
            "cost2  1.3227184 acc2:  0.578125\n",
            "cost  1.9666798 acc:  0.453125\n",
            "cost2  1.7015476 acc2:  0.5\n",
            "cost  1.805622 acc:  0.4921875\n",
            "cost2  1.4398744 acc2:  0.515625\n",
            "cost  1.28022 acc:  0.5703125\n",
            "cost2  1.4881766 acc2:  0.5703125\n",
            "cost  1.6846591 acc:  0.4765625\n",
            "cost2  1.1451285 acc2:  0.6484375\n",
            "cost  1.5894964 acc:  0.5625\n",
            "cost2  1.6066043 acc2:  0.5546875\n",
            "cost  1.8469654 acc:  0.4609375\n",
            "cost2  1.6020452 acc2:  0.5625\n",
            "cost  1.7367288 acc:  0.46875\n",
            "cost2  1.5314999 acc2:  0.546875\n",
            "cost  1.4956564 acc:  0.5859375\n",
            "cost2  1.4762783 acc2:  0.6015625\n",
            "cost  1.5032496 acc:  0.5390625\n",
            "cost2  1.5271158 acc2:  0.5\n",
            "cost  1.3775584 acc:  0.5859375\n",
            "cost2  1.4823657 acc2:  0.5390625\n",
            "cost  1.6043925 acc:  0.546875\n",
            "cost2  1.5061193 acc2:  0.5625\n",
            "cost  1.402631 acc:  0.578125\n",
            "cost2  1.4910002 acc2:  0.546875\n",
            "cost  1.4293199 acc:  0.546875\n",
            "cost2  1.580459 acc2:  0.578125\n",
            "cost  1.5473309 acc:  0.5234375\n",
            "cost2  1.6472683 acc2:  0.5234375\n",
            "cost  1.5594032 acc:  0.515625\n",
            "cost2  1.80912 acc2:  0.484375\n",
            "cost  1.4645091 acc:  0.4921875\n",
            "cost2  1.7589412 acc2:  0.515625\n",
            "cost  1.5952334 acc:  0.5546875\n",
            "cost2  1.3118111 acc2:  0.625\n",
            "cost  1.4149113 acc:  0.5546875\n",
            "cost2  1.398565 acc2:  0.609375\n",
            "cost  1.433975 acc:  0.6015625\n",
            "cost2  1.5242772 acc2:  0.59375\n",
            "cost  1.3781986 acc:  0.5859375\n",
            "cost2  1.5312805 acc2:  0.5703125\n",
            "cost  1.4504062 acc:  0.53125\n",
            "cost2  1.6323626 acc2:  0.5625\n",
            "cost  1.3137128 acc:  0.6171875\n",
            "cost2  1.2974203 acc2:  0.609375\n",
            "cost  1.4869881 acc:  0.5078125\n",
            "cost2  1.7764556 acc2:  0.46875\n",
            "cost  1.4336879 acc:  0.546875\n",
            "cost2  1.4779415 acc2:  0.578125\n",
            "cost  1.0063536 acc:  0.65625\n",
            "cost2  1.4283345 acc2:  0.5390625\n",
            "cost  1.6031804 acc:  0.546875\n",
            "cost2  1.1960464 acc2:  0.6484375\n",
            "cost  1.5443947 acc:  0.546875\n",
            "cost2  1.386574 acc2:  0.6640625\n",
            "cost  1.402978 acc:  0.6171875\n",
            "cost2  1.2475058 acc2:  0.6015625\n",
            "cost  1.2973924 acc:  0.578125\n",
            "cost2  1.2763529 acc2:  0.625\n",
            "cost  1.3934022 acc:  0.6015625\n",
            "cost2  1.177055 acc2:  0.6640625\n",
            "cost  1.359962 acc:  0.6171875\n",
            "cost2  1.1429096 acc2:  0.6953125\n",
            "cost  1.4482938 acc:  0.5\n",
            "cost2  1.3833026 acc2:  0.546875\n",
            "cost  1.0805813 acc:  0.625\n",
            "cost2  1.1528363 acc2:  0.6328125\n",
            "cost  1.4297457 acc:  0.578125\n",
            "cost2  1.468613 acc2:  0.5390625\n",
            "cost  1.4529369 acc:  0.5703125\n",
            "cost2  1.5613089 acc2:  0.5390625\n",
            "cost  1.6011178 acc:  0.53125\n",
            "cost2  1.3286648 acc2:  0.6328125\n",
            "cost  1.4445704 acc:  0.546875\n",
            "cost2  1.3467929 acc2:  0.625\n",
            "cost  1.48019 acc:  0.5546875\n",
            "cost2  1.1819838 acc2:  0.59375\n",
            "cost  1.4920564 acc:  0.5859375\n",
            "cost2  1.3522053 acc2:  0.59375\n",
            "cost  1.3531795 acc:  0.5859375\n",
            "cost2  1.0701008 acc2:  0.6875\n",
            "cost  1.2321429 acc:  0.6484375\n",
            "cost2  1.1345868 acc2:  0.703125\n",
            "cost  1.1874764 acc:  0.625\n",
            "cost2  1.108321 acc2:  0.6015625\n",
            "cost  1.1054568 acc:  0.6484375\n",
            "cost2  1.1782823 acc2:  0.6015625\n",
            "cost  1.0207561 acc:  0.71875\n",
            "cost2  1.4711812 acc2:  0.546875\n",
            "cost  1.031225 acc:  0.7109375\n",
            "cost2  1.1919082 acc2:  0.5703125\n",
            "cost  1.1851101 acc:  0.640625\n",
            "cost2  1.3274311 acc2:  0.59375\n",
            "cost  1.1910062 acc:  0.6171875\n",
            "cost2  1.3580906 acc2:  0.515625\n",
            "cost  1.3603437 acc:  0.6015625\n",
            "cost2  1.4104532 acc2:  0.6015625\n",
            "cost  1.3808259 acc:  0.625\n",
            "cost2  1.3099773 acc2:  0.6484375\n",
            "cost  1.3778841 acc:  0.640625\n",
            "cost2  1.1843199 acc2:  0.65625\n",
            "cost  1.32305 acc:  0.578125\n",
            "cost2  1.2850574 acc2:  0.6328125\n",
            "cost  1.1322303 acc:  0.6796875\n",
            "cost2  1.2495265 acc2:  0.640625\n",
            "cost  1.3186121 acc:  0.6328125\n",
            "cost2  1.0826153 acc2:  0.65625\n",
            "cost  1.297642 acc:  0.6171875\n",
            "cost2  1.2252569 acc2:  0.6484375\n",
            "cost  1.4222155 acc:  0.59375\n",
            "cost2  1.2122076 acc2:  0.6328125\n",
            "cost  1.306158 acc:  0.59375\n",
            "cost2  1.2767887 acc2:  0.6171875\n",
            "cost  1.2871518 acc:  0.5703125\n",
            "cost2  1.521801 acc2:  0.53125\n",
            "cost  1.0457798 acc:  0.6640625\n",
            "cost2  1.2774835 acc2:  0.6328125\n",
            "cost  1.0464046 acc:  0.6640625\n",
            "cost2  1.4891958 acc2:  0.609375\n",
            "cost  1.0177909 acc:  0.65625\n",
            "cost2  1.1456084 acc2:  0.6640625\n",
            "cost  1.0695843 acc:  0.65625\n",
            "cost2  1.3540657 acc2:  0.609375\n",
            "cost  1.223575 acc:  0.640625\n",
            "cost2  1.4072515 acc2:  0.6171875\n",
            "cost  1.2813368 acc:  0.671875\n",
            "cost2  1.295701 acc2:  0.6171875\n",
            "cost  1.2256901 acc:  0.6484375\n",
            "cost2  1.164608 acc2:  0.7109375\n",
            "cost  1.18532 acc:  0.625\n",
            "cost2  1.2956624 acc2:  0.6015625\n",
            "cost  1.2277296 acc:  0.640625\n",
            "cost2  1.2820495 acc2:  0.6015625\n",
            "cost  1.4628615 acc:  0.640625\n",
            "cost2  1.0502322 acc2:  0.6796875\n",
            "cost  1.2429491 acc:  0.609375\n",
            "cost2  1.4139742 acc2:  0.59375\n",
            "cost  0.9788197 acc:  0.671875\n",
            "cost2  1.2051188 acc2:  0.6953125\n",
            "cost  1.2808478 acc:  0.65625\n",
            "cost2  1.4817612 acc2:  0.578125\n",
            "cost  0.9102624 acc:  0.671875\n",
            "cost2  1.0743842 acc2:  0.7109375\n",
            "cost  1.5476456 acc:  0.5390625\n",
            "cost2  1.1040355 acc2:  0.640625\n",
            "cost  1.1065989 acc:  0.6640625\n",
            "cost2  1.2955633 acc2:  0.6484375\n",
            "cost  1.2681978 acc:  0.6484375\n",
            "cost2  1.0089968 acc2:  0.6484375\n",
            "cost  1.0378582 acc:  0.6484375\n",
            "cost2  1.0230948 acc2:  0.671875\n",
            "cost  1.2243042 acc:  0.609375\n",
            "cost2  1.0901644 acc2:  0.6640625\n",
            "cost  1.2740067 acc:  0.6171875\n",
            "cost2  1.1399293 acc2:  0.6953125\n",
            "cost  1.4530716 acc:  0.59375\n",
            "cost2  0.97625715 acc2:  0.6640625\n",
            "cost  0.9269208 acc:  0.734375\n",
            "cost2  1.0989257 acc2:  0.7265625\n",
            "cost  1.2244008 acc:  0.6171875\n",
            "cost2  1.0871637 acc2:  0.7109375\n",
            "cost  1.1965163 acc:  0.671875\n",
            "cost2  1.1298368 acc2:  0.671875\n",
            "cost  1.0990112 acc:  0.6640625\n",
            "cost2  1.3619514 acc2:  0.5703125\n",
            "cost  1.3822138 acc:  0.59375\n",
            "cost2  1.2536085 acc2:  0.6484375\n",
            "cost  1.1257985 acc:  0.6328125\n",
            "cost2  1.3014839 acc2:  0.609375\n",
            "cost  1.0653373 acc:  0.671875\n",
            "cost2  1.0934324 acc2:  0.640625\n",
            "cost  1.471594 acc:  0.609375\n",
            "cost2  0.92196715 acc2:  0.7265625\n",
            "cost  1.3524256 acc:  0.609375\n",
            "cost2  1.3057078 acc2:  0.6171875\n",
            "cost  1.0579436 acc:  0.6953125\n",
            "cost2  1.3083632 acc2:  0.6171875\n",
            "cost  1.173745 acc:  0.65625\n",
            "cost2  0.92334753 acc2:  0.7421875\n",
            "cost  1.1044593 acc:  0.7109375\n",
            "cost2  1.0799364 acc2:  0.6328125\n",
            "cost  1.0346775 acc:  0.6796875\n",
            "cost2  0.9672336 acc2:  0.6953125\n",
            "cost  1.2678547 acc:  0.65625\n",
            "cost2  1.0526866 acc2:  0.6875\n",
            "cost  1.2461203 acc:  0.6484375\n",
            "cost2  1.0265626 acc2:  0.65625\n",
            "cost  1.134661 acc:  0.6875\n",
            "cost2  1.1077657 acc2:  0.703125\n",
            "cost  1.243365 acc:  0.6328125\n",
            "cost2  1.0881352 acc2:  0.6796875\n",
            "cost  1.1799533 acc:  0.671875\n",
            "cost2  1.310463 acc2:  0.625\n",
            "cost  1.3464034 acc:  0.625\n",
            "cost2  0.9634845 acc2:  0.734375\n",
            "cost  1.1019063 acc:  0.671875\n",
            "cost2  1.2512002 acc2:  0.5625\n",
            "cost  1.091006 acc:  0.6953125\n",
            "cost2  1.105767 acc2:  0.6484375\n",
            "cost  1.2492796 acc:  0.625\n",
            "cost2  0.99039346 acc2:  0.734375\n",
            "cost  1.2673913 acc:  0.625\n",
            "cost2  1.1218331 acc2:  0.6796875\n",
            "cost  0.91498303 acc:  0.7265625\n",
            "cost2  0.91408217 acc2:  0.6796875\n",
            "cost  1.2491765 acc:  0.625\n",
            "cost2  0.97639203 acc2:  0.6875\n",
            "cost  1.1881754 acc:  0.6171875\n",
            "cost2  1.009551 acc2:  0.671875\n",
            "cost  1.1641755 acc:  0.671875\n",
            "cost2  1.121125 acc2:  0.6640625\n",
            "cost  1.1139354 acc:  0.6484375\n",
            "cost2  1.1906638 acc2:  0.6328125\n",
            "cost  1.0739813 acc:  0.65625\n",
            "cost2  0.92936707 acc2:  0.71875\n",
            "cost  1.1510983 acc:  0.65625\n",
            "cost2  0.98179615 acc2:  0.7578125\n",
            "cost  1.1748803 acc:  0.6015625\n",
            "cost2  0.8379498 acc2:  0.7265625\n",
            "cost  1.2804453 acc:  0.609375\n",
            "cost2  1.093325 acc2:  0.625\n",
            "cost  1.1574634 acc:  0.6796875\n",
            "cost2  0.8383564 acc2:  0.7265625\n",
            "cost  1.1299189 acc:  0.671875\n",
            "cost2  1.3215022 acc2:  0.640625\n",
            "cost  1.2624767 acc:  0.6484375\n",
            "cost2  1.0007974 acc2:  0.7578125\n",
            "cost  1.1316267 acc:  0.6640625\n",
            "cost2  1.0892012 acc2:  0.640625\n",
            "cost  1.1814163 acc:  0.625\n",
            "cost2  1.0679637 acc2:  0.671875\n",
            "cost  1.1167862 acc:  0.6875\n",
            "cost2  1.1431687 acc2:  0.6875\n",
            "cost  1.2181188 acc:  0.671875\n",
            "cost2  0.9252056 acc2:  0.7265625\n",
            "cost  1.2882365 acc:  0.6328125\n",
            "cost2  1.0220466 acc2:  0.6796875\n",
            "cost  1.0535412 acc:  0.671875\n",
            "cost2  1.2661955 acc2:  0.6328125\n",
            "cost  1.0708984 acc:  0.671875\n",
            "cost2  1.0776484 acc2:  0.6875\n",
            "cost  1.0606432 acc:  0.71875\n",
            "cost2  1.0706697 acc2:  0.65625\n",
            "cost  1.2893366 acc:  0.609375\n",
            "cost2  1.3148522 acc2:  0.59375\n",
            "cost  1.0198 acc:  0.703125\n",
            "cost2  1.0931524 acc2:  0.6015625\n",
            "cost  0.89753175 acc:  0.7265625\n",
            "cost2  0.9495573 acc2:  0.6953125\n",
            "cost  1.164473 acc:  0.65625\n",
            "cost2  0.9776553 acc2:  0.6953125\n",
            "cost  1.0261275 acc:  0.7109375\n",
            "cost2  1.0334634 acc2:  0.6875\n",
            "cost  1.3163323 acc:  0.6171875\n",
            "cost2  0.8845188 acc2:  0.671875\n",
            "cost  1.0455413 acc:  0.6796875\n",
            "cost2  1.0357925 acc2:  0.7109375\n",
            "cost  1.0410589 acc:  0.6484375\n",
            "cost2  1.1333218 acc2:  0.671875\n",
            "cost  0.83391 acc:  0.75\n",
            "cost2  0.80094385 acc2:  0.7265625\n",
            "cost  1.3249236 acc:  0.609375\n",
            "cost2  0.8410154 acc2:  0.7421875\n",
            "cost  1.0336041 acc:  0.7109375\n",
            "cost2  1.0741282 acc2:  0.6796875\n",
            "cost  1.2693026 acc:  0.609375\n",
            "cost2  1.0141143 acc2:  0.7109375\n",
            "cost  1.15123 acc:  0.640625\n",
            "cost2  1.0574597 acc2:  0.7265625\n",
            "cost  1.1315706 acc:  0.6796875\n",
            "cost2  0.76237065 acc2:  0.75\n",
            "cost  0.9496597 acc:  0.7265625\n",
            "cost2  0.788547 acc2:  0.75\n",
            "cost  1.0643735 acc:  0.671875\n",
            "cost2  0.92691016 acc2:  0.625\n",
            "cost  1.1707577 acc:  0.640625\n",
            "cost2  0.8991208 acc2:  0.7265625\n",
            "cost  0.97052854 acc:  0.734375\n",
            "cost2  1.2631966 acc2:  0.6328125\n",
            "cost  1.0879469 acc:  0.671875\n",
            "cost2  0.82635576 acc2:  0.71875\n",
            "cost  1.0733579 acc:  0.7109375\n",
            "cost2  0.8570218 acc2:  0.703125\n",
            "cost  1.0172641 acc:  0.6953125\n",
            "cost2  1.4243126 acc2:  0.5703125\n",
            "cost  0.9905261 acc:  0.6875\n",
            "cost2  0.9614005 acc2:  0.6796875\n",
            "cost  0.79155314 acc:  0.75\n",
            "cost2  1.0469118 acc2:  0.65625\n",
            "cost  0.88067836 acc:  0.703125\n",
            "cost2  0.9387043 acc2:  0.6875\n",
            "cost  1.0062243 acc:  0.65625\n",
            "cost2  1.0761267 acc2:  0.6484375\n",
            "cost  0.94692844 acc:  0.703125\n",
            "cost2  1.1033986 acc2:  0.671875\n",
            "cost  0.89514065 acc:  0.7265625\n",
            "cost2  1.0006293 acc2:  0.6796875\n",
            "cost  0.91208494 acc:  0.734375\n",
            "cost2  1.148628 acc2:  0.625\n",
            "cost  1.0723659 acc:  0.6640625\n",
            "cost2  0.81465185 acc2:  0.75\n",
            "cost  0.75582826 acc:  0.7890625\n",
            "cost2  1.0273288 acc2:  0.6640625\n",
            "cost  1.104529 acc:  0.6328125\n",
            "cost2  0.9353982 acc2:  0.6640625\n",
            "cost  1.047488 acc:  0.6953125\n",
            "cost2  1.0147408 acc2:  0.6953125\n",
            "cost  1.1119337 acc:  0.6640625\n",
            "cost2  1.0587184 acc2:  0.703125\n",
            "cost  0.8052831 acc:  0.7109375\n",
            "cost2  1.0306371 acc2:  0.6796875\n",
            "cost  1.0583059 acc:  0.640625\n",
            "cost2  0.76813793 acc2:  0.75\n",
            "cost  1.0070095 acc:  0.7109375\n",
            "cost2  1.0216006 acc2:  0.640625\n",
            "cost  0.8161744 acc:  0.7421875\n",
            "cost2  1.1347027 acc2:  0.703125\n",
            "cost  0.9852437 acc:  0.6875\n",
            "cost2  1.165587 acc2:  0.671875\n",
            "cost  1.0433164 acc:  0.640625\n",
            "cost2  0.9109411 acc2:  0.703125\n",
            "cost  0.90344864 acc:  0.6875\n",
            "cost2  1.0244977 acc2:  0.6953125\n",
            "cost  0.6943344 acc:  0.7421875\n",
            "cost2  0.720554 acc2:  0.78125\n",
            "cost  1.3169373 acc:  0.5859375\n",
            "cost2  0.75390434 acc2:  0.7421875\n",
            "cost  0.9196028 acc:  0.703125\n",
            "cost2  0.8850483 acc2:  0.7109375\n",
            "cost  0.9740138 acc:  0.7421875\n",
            "cost2  0.95826286 acc2:  0.7265625\n",
            "cost  0.8288154 acc:  0.7421875\n",
            "cost2  0.80278933 acc2:  0.7890625\n",
            "cost  1.0553367 acc:  0.6640625\n",
            "cost2  0.89666975 acc2:  0.703125\n",
            "cost  1.1318096 acc:  0.703125\n",
            "cost2  0.96983457 acc2:  0.6875\n",
            "cost  1.1096134 acc:  0.6484375\n",
            "cost2  0.95792216 acc2:  0.7109375\n",
            "cost  0.8548248 acc:  0.7109375\n",
            "cost2  0.807984 acc2:  0.7421875\n",
            "cost  0.92985624 acc:  0.75\n",
            "cost2  0.78265595 acc2:  0.75\n",
            "cost  1.0780511 acc:  0.640625\n",
            "cost2  0.90586853 acc2:  0.7265625\n",
            "cost  1.0449115 acc:  0.6796875\n",
            "cost2  0.8584415 acc2:  0.75\n",
            "cost  1.0880991 acc:  0.703125\n",
            "cost2  0.87681043 acc2:  0.7421875\n",
            "cost  1.0696844 acc:  0.6796875\n",
            "cost2  1.1154922 acc2:  0.6875\n",
            "cost  1.0144031 acc:  0.703125\n",
            "cost2  0.77603287 acc2:  0.7421875\n",
            "cost  0.9310127 acc:  0.703125\n",
            "cost2  0.6825828 acc2:  0.7734375\n",
            "cost  0.9120077 acc:  0.6953125\n",
            "cost2  1.1738535 acc2:  0.671875\n",
            "cost  1.0525689 acc:  0.6328125\n",
            "cost2  0.86603856 acc2:  0.734375\n",
            "cost  0.8305609 acc:  0.7109375\n",
            "cost2  0.8402773 acc2:  0.71875\n",
            "cost  0.8462526 acc:  0.703125\n",
            "cost2  0.7481415 acc2:  0.7265625\n",
            "cost  0.76987654 acc:  0.7421875\n",
            "cost2  0.96805847 acc2:  0.7109375\n",
            "cost  0.92258114 acc:  0.703125\n",
            "cost2  0.9387945 acc2:  0.6875\n",
            "cost  0.8130727 acc:  0.7578125\n",
            "cost2  0.9999854 acc2:  0.7265625\n",
            "cost  1.0067626 acc:  0.6953125\n",
            "cost2  0.9947705 acc2:  0.7421875\n",
            "cost  0.8251922 acc:  0.7421875\n",
            "cost2  0.7963526 acc2:  0.75\n",
            "cost  0.98225546 acc:  0.6953125\n",
            "cost2  0.85279155 acc2:  0.7578125\n",
            "cost  1.0437431 acc:  0.703125\n",
            "cost2  1.0877949 acc2:  0.71875\n",
            "cost  0.91104007 acc:  0.6796875\n",
            "cost2  0.9071909 acc2:  0.7109375\n",
            "cost  0.91358954 acc:  0.7109375\n",
            "cost2  0.9525162 acc2:  0.6875\n",
            "cost  1.2205398 acc:  0.625\n",
            "cost2  0.7753836 acc2:  0.7109375\n",
            "cost  0.9767639 acc:  0.703125\n",
            "cost2  0.9419265 acc2:  0.6796875\n",
            "cost  0.8456225 acc:  0.71875\n",
            "cost2  1.0996716 acc2:  0.671875\n",
            "cost  0.89357543 acc:  0.734375\n",
            "cost2  0.9956312 acc2:  0.6875\n",
            "cost  0.721465 acc:  0.7734375\n",
            "cost2  0.84036845 acc2:  0.75\n",
            "cost  1.002182 acc:  0.7109375\n",
            "cost2  0.85925066 acc2:  0.7265625\n",
            "cost  0.8694479 acc:  0.7421875\n",
            "cost2  0.881872 acc2:  0.765625\n",
            "cost  1.016242 acc:  0.734375\n",
            "cost2  0.96361274 acc2:  0.734375\n",
            "cost  0.7877463 acc:  0.7578125\n",
            "cost2  0.75102144 acc2:  0.75\n",
            "cost  0.7372185 acc:  0.7890625\n",
            "cost2  0.9551588 acc2:  0.734375\n",
            "cost  0.90949786 acc:  0.7265625\n",
            "cost2  0.92606646 acc2:  0.75\n",
            "cost  1.0356495 acc:  0.65625\n",
            "cost2  0.9624021 acc2:  0.671875\n",
            "cost  0.84286624 acc:  0.734375\n",
            "cost2  0.95740557 acc2:  0.734375\n",
            "cost  0.93101597 acc:  0.7421875\n",
            "cost2  0.8721418 acc2:  0.7265625\n",
            "cost  1.0482876 acc:  0.71875\n",
            "cost2  0.7981215 acc2:  0.734375\n",
            "cost  0.7978382 acc:  0.7578125\n",
            "cost2  0.93341815 acc2:  0.75\n",
            "cost  0.8806652 acc:  0.7265625\n",
            "cost2  0.8248712 acc2:  0.71875\n",
            "cost  0.84226394 acc:  0.71875\n",
            "cost2  0.8259061 acc2:  0.7421875\n",
            "cost  1.3285847 acc:  0.640625\n",
            "cost2  0.7722966 acc2:  0.765625\n",
            "cost  1.0689151 acc:  0.7109375\n",
            "cost2  0.9887661 acc2:  0.71875\n",
            "cost  0.82303226 acc:  0.71875\n",
            "cost2  0.8672233 acc2:  0.7265625\n",
            "cost  0.80779564 acc:  0.6875\n",
            "cost2  0.85125446 acc2:  0.734375\n",
            "cost  0.6417798 acc:  0.734375\n",
            "cost2  0.7370686 acc2:  0.7734375\n",
            "cost  0.9287218 acc:  0.7265625\n",
            "cost2  0.7972852 acc2:  0.7890625\n",
            "cost  0.69138306 acc:  0.828125\n",
            "cost2  1.0542185 acc2:  0.6484375\n",
            "cost  1.0152862 acc:  0.6875\n",
            "cost2  0.94142205 acc2:  0.7265625\n",
            "cost  0.7875589 acc:  0.75\n",
            "cost2  1.0685793 acc2:  0.703125\n",
            "cost  0.8646443 acc:  0.7578125\n",
            "cost2  0.8760623 acc2:  0.7734375\n",
            "cost  1.1682036 acc:  0.703125\n",
            "cost2  0.88361955 acc2:  0.7734375\n",
            "cost  0.80367905 acc:  0.765625\n",
            "cost2  0.9223888 acc2:  0.7109375\n",
            "cost  0.70925784 acc:  0.7890625\n",
            "cost2  0.61713123 acc2:  0.828125\n",
            "cost  0.7057065 acc:  0.8046875\n",
            "cost2  0.91651785 acc2:  0.6953125\n",
            "cost  0.676769 acc:  0.7890625\n",
            "cost2  0.82186204 acc2:  0.734375\n",
            "cost  0.9333881 acc:  0.734375\n",
            "cost2  0.7931154 acc2:  0.7421875\n",
            "cost  0.8544751 acc:  0.765625\n",
            "cost2  1.1310637 acc2:  0.640625\n",
            "cost  0.7368996 acc:  0.796875\n",
            "cost2  0.7499967 acc2:  0.8046875\n",
            "cost  1.0528212 acc:  0.6484375\n",
            "cost2  0.8112569 acc2:  0.7109375\n",
            "cost  0.89129966 acc:  0.703125\n",
            "cost2  0.8999234 acc2:  0.703125\n",
            "cost  0.76125336 acc:  0.8046875\n",
            "cost2  0.58313626 acc2:  0.8203125\n",
            "cost  0.9028295 acc:  0.71875\n",
            "cost2  0.8373827 acc2:  0.7421875\n",
            "cost  0.9354166 acc:  0.71875\n",
            "cost2  0.83299375 acc2:  0.7734375\n",
            "cost  1.1710265 acc:  0.640625\n",
            "cost2  0.74491876 acc2:  0.7734375\n",
            "cost  0.8511679 acc:  0.7265625\n",
            "cost2  0.8526675 acc2:  0.765625\n",
            "cost  0.9309728 acc:  0.7265625\n",
            "cost2  0.815719 acc2:  0.7734375\n",
            "cost  0.6946153 acc:  0.78125\n",
            "cost2  0.63336873 acc2:  0.796875\n",
            "cost  0.8552324 acc:  0.703125\n",
            "cost2  0.661258 acc2:  0.78125\n",
            "cost  0.80349326 acc:  0.75\n",
            "cost2  0.76896536 acc2:  0.765625\n",
            "cost  0.87121445 acc:  0.75\n",
            "cost2  0.7319733 acc2:  0.78125\n",
            "cost  0.6767453 acc:  0.7421875\n",
            "cost2  0.93085027 acc2:  0.7109375\n",
            "cost  0.97372204 acc:  0.71875\n",
            "cost2  0.9553863 acc2:  0.71875\n",
            "cost  0.7710038 acc:  0.7734375\n",
            "cost2  0.51153076 acc2:  0.828125\n",
            "cost  0.9084999 acc:  0.75\n",
            "cost2  0.8177136 acc2:  0.7578125\n",
            "cost  0.75835216 acc:  0.7890625\n",
            "cost2  0.95819044 acc2:  0.7109375\n",
            "cost  0.7888969 acc:  0.7734375\n",
            "cost2  1.0837754 acc2:  0.6796875\n",
            "cost  0.6507633 acc:  0.7578125\n",
            "cost2  0.84018576 acc2:  0.734375\n",
            "cost  0.86288047 acc:  0.703125\n",
            "cost2  0.8968171 acc2:  0.75\n",
            "cost  0.60739845 acc:  0.8125\n",
            "cost2  0.82951385 acc2:  0.734375\n",
            "cost  0.66936225 acc:  0.7578125\n",
            "cost2  0.8987745 acc2:  0.6875\n",
            "cost  0.9424217 acc:  0.734375\n",
            "cost2  0.7958185 acc2:  0.7578125\n",
            "cost  0.9060822 acc:  0.734375\n",
            "cost2  0.7018136 acc2:  0.7890625\n",
            "cost  0.8239176 acc:  0.7578125\n",
            "cost2  0.8425308 acc2:  0.703125\n",
            "cost  0.9149251 acc:  0.7265625\n",
            "cost2  0.8304715 acc2:  0.6953125\n",
            "cost  0.6602238 acc:  0.8046875\n",
            "cost2  0.7432835 acc2:  0.8203125\n",
            "cost  0.91548127 acc:  0.7265625\n",
            "cost2  0.92634934 acc2:  0.75\n",
            "cost  0.8176192 acc:  0.734375\n",
            "cost2  0.9172438 acc2:  0.78125\n",
            "cost  0.9368267 acc:  0.7109375\n",
            "cost2  0.5846435 acc2:  0.8203125\n",
            "cost  0.7160773 acc:  0.7734375\n",
            "cost2  0.7361852 acc2:  0.796875\n",
            "cost  0.7052613 acc:  0.7421875\n",
            "cost2  0.8011572 acc2:  0.765625\n",
            "cost  0.88903105 acc:  0.7421875\n",
            "cost2  0.94330466 acc2:  0.7265625\n",
            "cost  0.6956274 acc:  0.765625\n",
            "cost2  0.6637356 acc2:  0.796875\n",
            "cost  0.73140836 acc:  0.796875\n",
            "cost2  0.7338667 acc2:  0.7578125\n",
            "cost  0.8602466 acc:  0.734375\n",
            "cost2  0.8806597 acc2:  0.71875\n",
            "cost  0.6945889 acc:  0.7578125\n",
            "cost2  0.54251957 acc2:  0.8203125\n",
            "cost  0.66280574 acc:  0.796875\n",
            "cost2  1.0053902 acc2:  0.734375\n",
            "cost  0.917053 acc:  0.6875\n",
            "cost2  0.6748336 acc2:  0.7890625\n",
            "cost  0.8008014 acc:  0.78125\n",
            "cost2  0.9465832 acc2:  0.6953125\n",
            "cost  0.8219237 acc:  0.7890625\n",
            "cost2  0.6861553 acc2:  0.796875\n",
            "cost  0.7945906 acc:  0.7734375\n",
            "cost2  0.66093314 acc2:  0.7578125\n",
            "cost  0.73679304 acc:  0.78125\n",
            "cost2  0.82325816 acc2:  0.75\n",
            "cost  0.7649397 acc:  0.7578125\n",
            "cost2  0.85094094 acc2:  0.734375\n",
            "cost  0.6824418 acc:  0.7890625\n",
            "cost2  0.69265497 acc2:  0.796875\n",
            "cost  0.7584325 acc:  0.75\n",
            "cost2  1.1360604 acc2:  0.671875\n",
            "cost  0.8214012 acc:  0.7421875\n",
            "cost2  0.84276026 acc2:  0.7421875\n",
            "cost  0.6635475 acc:  0.765625\n",
            "cost2  0.7197375 acc2:  0.78125\n",
            "cost  0.9468484 acc:  0.703125\n",
            "cost2  0.75011426 acc2:  0.78125\n",
            "cost  0.74944 acc:  0.765625\n",
            "cost2  0.58636296 acc2:  0.8359375\n",
            "cost  0.88096845 acc:  0.71875\n",
            "cost2  0.94973695 acc2:  0.703125\n",
            "cost  0.82045037 acc:  0.7421875\n",
            "cost2  0.7379526 acc2:  0.7734375\n",
            "cost  0.8450636 acc:  0.7734375\n",
            "cost2  0.87752354 acc2:  0.7421875\n",
            "cost  0.88704073 acc:  0.765625\n",
            "cost2  0.66172594 acc2:  0.8203125\n",
            "cost  0.83723223 acc:  0.703125\n",
            "cost2  0.9917631 acc2:  0.703125\n",
            "cost  0.66368437 acc:  0.8046875\n",
            "cost2  0.6146435 acc2:  0.8046875\n",
            "cost  0.74364644 acc:  0.7421875\n",
            "cost2  0.8469932 acc2:  0.765625\n",
            "cost  0.7489034 acc:  0.78125\n",
            "cost2  0.6860384 acc2:  0.7265625\n",
            "cost  0.8129947 acc:  0.71875\n",
            "cost2  0.9709104 acc2:  0.75\n",
            "cost  0.8498274 acc:  0.765625\n",
            "cost2  0.68321943 acc2:  0.796875\n",
            "cost  0.53377706 acc:  0.84375\n",
            "cost2  0.9201522 acc2:  0.765625\n",
            "cost  0.844285 acc:  0.7734375\n",
            "cost2  0.5749339 acc2:  0.8125\n",
            "cost  0.7528956 acc:  0.78125\n",
            "cost2  0.6024874 acc2:  0.796875\n",
            "cost  0.7248035 acc:  0.796875\n",
            "cost2  0.6858762 acc2:  0.796875\n",
            "cost  0.8917824 acc:  0.734375\n",
            "cost2  0.88852876 acc2:  0.7734375\n",
            "cost  0.65575624 acc:  0.78125\n",
            "cost2  0.91325784 acc2:  0.71875\n",
            "cost  1.0869049 acc:  0.6640625\n",
            "cost2  0.7936332 acc2:  0.7578125\n",
            "cost  0.82025975 acc:  0.7578125\n",
            "cost2  0.61228347 acc2:  0.84375\n",
            "cost  0.9014201 acc:  0.7265625\n",
            "cost2  0.87986165 acc2:  0.765625\n",
            "cost  0.9538529 acc:  0.71875\n",
            "cost2  0.85711217 acc2:  0.75\n",
            "cost  0.6826434 acc:  0.7734375\n",
            "cost2  0.577601 acc2:  0.8046875\n",
            "cost  0.64771044 acc:  0.8125\n",
            "cost2  0.8875029 acc2:  0.7734375\n",
            "cost  0.7357134 acc:  0.796875\n",
            "cost2  0.62352747 acc2:  0.7890625\n",
            "cost  0.77824533 acc:  0.75\n",
            "cost2  0.66217834 acc2:  0.8046875\n",
            "cost  0.5568881 acc:  0.84375\n",
            "cost2  0.8297311 acc2:  0.75\n",
            "cost  0.7929932 acc:  0.7578125\n",
            "cost2  0.80820054 acc2:  0.765625\n",
            "cost  0.7316287 acc:  0.78125\n",
            "cost2  0.82969296 acc2:  0.765625\n",
            "cost  1.0061135 acc:  0.734375\n",
            "cost2  0.7386866 acc2:  0.765625\n",
            "cost  1.1381463 acc:  0.6640625\n",
            "cost2  0.80817163 acc2:  0.78125\n",
            "cost  0.72811645 acc:  0.7890625\n",
            "cost2  0.6945374 acc2:  0.7578125\n",
            "cost  0.7045337 acc:  0.75\n",
            "cost2  0.5959975 acc2:  0.8359375\n",
            "cost  0.9569719 acc:  0.734375\n",
            "cost2  0.89487296 acc2:  0.7265625\n",
            "cost  0.80030096 acc:  0.765625\n",
            "cost2  0.6256337 acc2:  0.8125\n",
            "cost  0.7100296 acc:  0.765625\n",
            "cost2  0.7320516 acc2:  0.75\n",
            "cost  0.8706207 acc:  0.7578125\n",
            "cost2  0.8378663 acc2:  0.71875\n",
            "cost  0.64562905 acc:  0.78125\n",
            "cost2  0.79150724 acc2:  0.7890625\n",
            "cost  0.6915146 acc:  0.796875\n",
            "cost2  0.7751003 acc2:  0.7890625\n",
            "cost  0.7261306 acc:  0.78125\n",
            "cost2  0.69646853 acc2:  0.8203125\n",
            "cost  0.6655441 acc:  0.7890625\n",
            "cost2  0.5865666 acc2:  0.796875\n",
            "cost  0.7478245 acc:  0.7421875\n",
            "cost2  0.8657418 acc2:  0.7421875\n",
            "cost  0.8148041 acc:  0.7890625\n",
            "cost2  0.6384667 acc2:  0.796875\n",
            "cost  0.65490615 acc:  0.7890625\n",
            "cost2  0.54769933 acc2:  0.84375\n",
            "cost  0.5262145 acc:  0.8359375\n",
            "cost2  0.64280045 acc2:  0.78125\n",
            "cost  0.800048 acc:  0.71875\n",
            "cost2  0.8161546 acc2:  0.703125\n",
            "cost  0.92950296 acc:  0.6640625\n",
            "cost2  0.7452344 acc2:  0.7578125\n",
            "cost  0.8011358 acc:  0.8046875\n",
            "cost2  0.88157433 acc2:  0.703125\n",
            "cost  1.0770724 acc:  0.71875\n",
            "cost2  0.65996635 acc2:  0.8125\n",
            "cost  0.7037134 acc:  0.78125\n",
            "cost2  0.5990182 acc2:  0.828125\n",
            "cost  0.6214147 acc:  0.7890625\n",
            "cost2  0.84102404 acc2:  0.7421875\n",
            "cost  0.81951034 acc:  0.7890625\n",
            "cost2  0.64660174 acc2:  0.7890625\n",
            "cost  0.7019125 acc:  0.7734375\n",
            "cost2  0.69228864 acc2:  0.7890625\n",
            "cost  0.6937493 acc:  0.7578125\n",
            "cost2  0.8194252 acc2:  0.7890625\n",
            "cost  0.67747724 acc:  0.7890625\n",
            "cost2  0.662902 acc2:  0.8046875\n",
            "cost  1.075381 acc:  0.6796875\n",
            "cost2  0.8075624 acc2:  0.78125\n",
            "cost  0.9777374 acc:  0.71875\n",
            "cost2  0.67658997 acc2:  0.78125\n",
            "cost  0.83718634 acc:  0.7578125\n",
            "cost2  0.59215194 acc2:  0.8125\n",
            "cost  1.0195632 acc:  0.734375\n",
            "cost2  0.7518177 acc2:  0.796875\n",
            "cost  1.0386086 acc:  0.703125\n",
            "cost2  0.7264214 acc2:  0.765625\n",
            "cost  0.7833133 acc:  0.7578125\n",
            "cost2  0.70844346 acc2:  0.828125\n",
            "cost  0.9594203 acc:  0.7734375\n",
            "cost2  0.64391696 acc2:  0.7734375\n",
            "cost  0.8074597 acc:  0.7578125\n",
            "cost2  0.8516555 acc2:  0.7578125\n",
            "cost  0.6548951 acc:  0.7890625\n",
            "cost2  0.70817995 acc2:  0.796875\n",
            "cost  0.77656496 acc:  0.703125\n",
            "cost2  0.69053966 acc2:  0.8046875\n",
            "cost  0.6093397 acc:  0.796875\n",
            "cost2  0.57423073 acc2:  0.8203125\n",
            "cost  0.669091 acc:  0.765625\n",
            "cost2  0.79984975 acc2:  0.7578125\n",
            "cost  0.88932204 acc:  0.7578125\n",
            "cost2  0.6183636 acc2:  0.7890625\n",
            "cost  0.72539634 acc:  0.7890625\n",
            "cost2  0.7847948 acc2:  0.7421875\n",
            "cost  0.85194445 acc:  0.734375\n",
            "cost2  0.8567661 acc2:  0.734375\n",
            "cost  1.0626066 acc:  0.6875\n",
            "cost2  0.74450195 acc2:  0.8046875\n",
            "cost  0.65900326 acc:  0.765625\n",
            "cost2  0.6013999 acc2:  0.7734375\n",
            "cost  0.6258422 acc:  0.828125\n",
            "cost2  0.6408718 acc2:  0.7890625\n",
            "cost  0.7892649 acc:  0.75\n",
            "cost2  0.6291506 acc2:  0.7890625\n",
            "cost  0.65853596 acc:  0.8046875\n",
            "cost2  0.84364325 acc2:  0.7109375\n",
            "cost  0.775156 acc:  0.7734375\n",
            "cost2  0.8098279 acc2:  0.7734375\n",
            "cost  0.69761217 acc:  0.8125\n",
            "cost2  0.705575 acc2:  0.78125\n",
            "cost  0.6997305 acc:  0.78125\n",
            "cost2  0.60136265 acc2:  0.828125\n",
            "cost  0.7872975 acc:  0.8046875\n",
            "cost2  0.7622839 acc2:  0.734375\n",
            "cost  0.6630661 acc:  0.7734375\n",
            "cost2  0.6462421 acc2:  0.7890625\n",
            "cost  0.55692124 acc:  0.8125\n",
            "cost2  0.7213825 acc2:  0.796875\n",
            "cost  0.6013722 acc:  0.7734375\n",
            "cost2  0.7764413 acc2:  0.7734375\n",
            "cost  0.6406583 acc:  0.8046875\n",
            "cost2  0.90502506 acc2:  0.734375\n",
            "cost  0.78105867 acc:  0.7578125\n",
            "cost2  0.505545 acc2:  0.8515625\n",
            "cost  0.754263 acc:  0.765625\n",
            "cost2  0.8699604 acc2:  0.734375\n",
            "cost  0.5351176 acc:  0.84375\n",
            "cost2  0.72655845 acc2:  0.8046875\n",
            "cost  0.8779158 acc:  0.7578125\n",
            "cost2  0.62314546 acc2:  0.796875\n",
            "cost  0.86873573 acc:  0.71875\n",
            "cost2  0.62389046 acc2:  0.8046875\n",
            "cost  0.8280473 acc:  0.7265625\n",
            "cost2  0.8586752 acc2:  0.7734375\n",
            "cost  0.7644268 acc:  0.78125\n",
            "cost2  0.88370883 acc2:  0.75\n",
            "cost  0.8337267 acc:  0.75\n",
            "cost2  0.7958445 acc2:  0.78125\n",
            "cost  0.6337303 acc:  0.8359375\n",
            "cost2  0.56973815 acc2:  0.828125\n",
            "cost  0.76824534 acc:  0.734375\n",
            "cost2  0.5534889 acc2:  0.828125\n",
            "cost  0.9626771 acc:  0.6953125\n",
            "cost2  0.6862593 acc2:  0.7890625\n",
            "cost  0.8771354 acc:  0.7421875\n",
            "cost2  0.5020837 acc2:  0.8515625\n",
            "cost  0.53410673 acc:  0.828125\n",
            "cost2  0.76107424 acc2:  0.765625\n",
            "cost  0.77633315 acc:  0.71875\n",
            "cost2  0.62906957 acc2:  0.8125\n",
            "cost  0.7773614 acc:  0.734375\n",
            "cost2  0.7128248 acc2:  0.78125\n",
            "cost  0.63672143 acc:  0.7734375\n",
            "cost2  0.5809557 acc2:  0.84375\n",
            "cost  0.6468834 acc:  0.8046875\n",
            "cost2  0.697011 acc2:  0.828125\n",
            "cost  0.6928222 acc:  0.8046875\n",
            "cost2  0.597499 acc2:  0.8125\n",
            "cost  0.67335296 acc:  0.7734375\n",
            "cost2  0.5483294 acc2:  0.8203125\n",
            "cost  0.5749285 acc:  0.8046875\n",
            "cost2  0.6667424 acc2:  0.7734375\n",
            "cost  0.49859208 acc:  0.8359375\n",
            "cost2  0.7372489 acc2:  0.734375\n",
            "cost  0.5608032 acc:  0.828125\n",
            "cost2  0.63915414 acc2:  0.7734375\n",
            "cost  0.7895049 acc:  0.75\n",
            "cost2  0.4739235 acc2:  0.8671875\n",
            "cost  0.7689105 acc:  0.7734375\n",
            "cost2  0.80296254 acc2:  0.75\n",
            "cost  0.72333956 acc:  0.7734375\n",
            "cost2  0.47669566 acc2:  0.8515625\n",
            "cost  0.6157862 acc:  0.796875\n",
            "cost2  0.8532791 acc2:  0.7890625\n",
            "cost  0.7963217 acc:  0.734375\n",
            "cost2  0.6299476 acc2:  0.8359375\n",
            "cost  0.7020354 acc:  0.7734375\n",
            "cost2  0.58908993 acc2:  0.8125\n",
            "cost  0.58314145 acc:  0.8359375\n",
            "cost2  0.62933946 acc2:  0.828125\n",
            "cost  0.7090243 acc:  0.8125\n",
            "cost2  0.6605357 acc2:  0.796875\n",
            "cost  0.6560594 acc:  0.7890625\n",
            "cost2  0.62009716 acc2:  0.7890625\n",
            "cost  0.6105497 acc:  0.78125\n",
            "cost2  0.6141313 acc2:  0.8203125\n",
            "cost  0.7411296 acc:  0.765625\n",
            "cost2  0.63171375 acc2:  0.8203125\n",
            "cost  0.7477809 acc:  0.7890625\n",
            "cost2  0.5287974 acc2:  0.796875\n",
            "cost  0.82481784 acc:  0.7265625\n",
            "cost2  0.5930181 acc2:  0.8046875\n",
            "cost  0.70319515 acc:  0.7578125\n",
            "cost2  0.7168709 acc2:  0.828125\n",
            "cost  0.6571779 acc:  0.8359375\n",
            "cost2  0.52505696 acc2:  0.8203125\n",
            "cost  0.72172153 acc:  0.765625\n",
            "cost2  0.6969222 acc2:  0.734375\n",
            "cost  0.60944545 acc:  0.8203125\n",
            "cost2  0.7074135 acc2:  0.765625\n",
            "cost  0.6602829 acc:  0.7890625\n",
            "cost2  0.65489185 acc2:  0.8203125\n",
            "cost  0.72821105 acc:  0.7890625\n",
            "cost2  0.7870218 acc2:  0.75\n",
            "cost  0.74267656 acc:  0.7734375\n",
            "cost2  0.81817836 acc2:  0.796875\n",
            "cost  0.47884488 acc:  0.8359375\n",
            "cost2  0.6144918 acc2:  0.78125\n",
            "cost  0.7836716 acc:  0.7578125\n",
            "cost2  0.6583946 acc2:  0.8125\n",
            "cost  0.78117764 acc:  0.734375\n",
            "cost2  0.6791519 acc2:  0.78125\n",
            "cost  0.6704949 acc:  0.78125\n",
            "cost2  0.75003153 acc2:  0.7734375\n",
            "cost  0.70709085 acc:  0.765625\n",
            "cost2  0.55245215 acc2:  0.796875\n",
            "cost  0.718109 acc:  0.7890625\n",
            "cost2  0.5130668 acc2:  0.828125\n",
            "cost  0.47933662 acc:  0.8359375\n",
            "cost2  0.74848133 acc2:  0.75\n",
            "cost  0.62669504 acc:  0.71875\n",
            "cost2  0.9981629 acc2:  0.75\n",
            "cost  0.8501147 acc:  0.71875\n",
            "cost2  0.70324355 acc2:  0.8125\n",
            "cost  0.61407304 acc:  0.8046875\n",
            "cost2  0.6200487 acc2:  0.8203125\n",
            "cost  0.7857044 acc:  0.7265625\n",
            "cost2  0.91510195 acc2:  0.734375\n",
            "cost  0.78053236 acc:  0.796875\n",
            "cost2  0.5560023 acc2:  0.828125\n",
            "cost  0.8581387 acc:  0.78125\n",
            "cost2  0.72643983 acc2:  0.765625\n",
            "cost  0.5728158 acc:  0.796875\n",
            "cost2  0.57884526 acc2:  0.8515625\n",
            "cost  0.6954534 acc:  0.8359375\n",
            "cost2  0.56223273 acc2:  0.8203125\n",
            "cost  0.5834341 acc:  0.8359375\n",
            "cost2  0.70247674 acc2:  0.78125\n",
            "cost  0.6891625 acc:  0.8046875\n",
            "cost2  0.6024835 acc2:  0.7890625\n",
            "cost  0.90598404 acc:  0.765625\n",
            "cost2  0.59835017 acc2:  0.796875\n",
            "cost  0.55708516 acc:  0.8203125\n",
            "cost2  0.7832899 acc2:  0.796875\n",
            "cost  0.53303045 acc:  0.8359375\n",
            "cost2  0.518559 acc2:  0.8359375\n",
            "cost  0.6775181 acc:  0.7890625\n",
            "cost2  0.6027801 acc2:  0.8359375\n",
            "cost  0.5689761 acc:  0.84375\n",
            "cost2  0.54860795 acc2:  0.8046875\n",
            "cost  0.6253791 acc:  0.8125\n",
            "cost2  0.5570017 acc2:  0.828125\n",
            "cost  0.7005834 acc:  0.8203125\n",
            "cost2  0.49649832 acc2:  0.8671875\n",
            "cost  0.7179577 acc:  0.734375\n",
            "cost2  0.7964667 acc2:  0.765625\n",
            "cost  0.83461535 acc:  0.734375\n",
            "cost2  0.9244565 acc2:  0.7734375\n",
            "cost  0.63771784 acc:  0.796875\n",
            "cost2  0.6714393 acc2:  0.78125\n",
            "cost  0.61578727 acc:  0.8203125\n",
            "cost2  0.7298657 acc2:  0.8125\n",
            "cost  0.48845214 acc:  0.828125\n",
            "cost2  0.6741514 acc2:  0.8046875\n",
            "cost  0.56675446 acc:  0.828125\n",
            "cost2  0.67812043 acc2:  0.796875\n",
            "cost  0.6038736 acc:  0.796875\n",
            "cost2  0.7969005 acc2:  0.7578125\n",
            "cost  0.6071647 acc:  0.8359375\n",
            "cost2  0.658803 acc2:  0.84375\n",
            "cost  0.44130126 acc:  0.859375\n",
            "cost2  0.8176472 acc2:  0.7890625\n",
            "cost  0.6076404 acc:  0.84375\n",
            "cost2  0.69238025 acc2:  0.796875\n",
            "cost  0.7112584 acc:  0.78125\n",
            "cost2  0.6074735 acc2:  0.8125\n",
            "cost  0.6676214 acc:  0.796875\n",
            "cost2  0.64500344 acc2:  0.8125\n",
            "cost  0.76899 acc:  0.7734375\n",
            "cost2  0.76931995 acc2:  0.7890625\n",
            "cost  0.6643716 acc:  0.7734375\n",
            "cost2  0.62804115 acc2:  0.78125\n",
            "cost  0.5839007 acc:  0.8125\n",
            "cost2  0.41071066 acc2:  0.8671875\n",
            "cost  0.6449233 acc:  0.8359375\n",
            "cost2  0.6074195 acc2:  0.78125\n",
            "cost  0.637537 acc:  0.7890625\n",
            "cost2  0.80114436 acc2:  0.796875\n",
            "cost  0.54678243 acc:  0.8359375\n",
            "cost2  0.9322692 acc2:  0.75\n",
            "cost  0.6612922 acc:  0.796875\n",
            "cost2  0.571934 acc2:  0.84375\n",
            "cost  0.74237204 acc:  0.7890625\n",
            "cost2  0.5244649 acc2:  0.8359375\n",
            "cost  0.59023464 acc:  0.828125\n",
            "cost2  0.78247297 acc2:  0.796875\n",
            "cost  0.660589 acc:  0.7734375\n",
            "cost2  0.64424646 acc2:  0.8359375\n",
            "cost  0.64969206 acc:  0.8359375\n",
            "cost2  0.5804299 acc2:  0.8125\n",
            "cost  0.5261979 acc:  0.8359375\n",
            "cost2  0.7182695 acc2:  0.7890625\n",
            "cost  0.53821164 acc:  0.828125\n",
            "cost2  0.49797833 acc2:  0.8203125\n",
            "cost  0.75885093 acc:  0.75\n",
            "cost2  0.5969378 acc2:  0.7890625\n",
            "cost  0.5144329 acc:  0.828125\n",
            "cost2  0.55214995 acc2:  0.859375\n",
            "cost  0.689868 acc:  0.8359375\n",
            "cost2  0.5747757 acc2:  0.8046875\n",
            "cost  0.6037411 acc:  0.796875\n",
            "cost2  0.6392444 acc2:  0.84375\n",
            "cost  0.5833198 acc:  0.8125\n",
            "cost2  0.6439418 acc2:  0.78125\n",
            "cost  0.7905657 acc:  0.7890625\n",
            "cost2  0.6418884 acc2:  0.796875\n",
            "cost  0.65423876 acc:  0.78125\n",
            "cost2  0.6538822 acc2:  0.7734375\n",
            "cost  0.72893375 acc:  0.8203125\n",
            "cost2  0.67152536 acc2:  0.8046875\n",
            "cost  0.5912591 acc:  0.828125\n",
            "cost2  0.69101644 acc2:  0.734375\n",
            "cost  0.7189227 acc:  0.765625\n",
            "cost2  0.719057 acc2:  0.7890625\n",
            "cost  0.5243402 acc:  0.8203125\n",
            "cost2  0.64048254 acc2:  0.84375\n",
            "cost  0.45019907 acc:  0.8359375\n",
            "cost2  0.7475752 acc2:  0.8203125\n",
            "cost  0.8143915 acc:  0.734375\n",
            "cost2  0.60375416 acc2:  0.7890625\n",
            "cost  0.63722277 acc:  0.828125\n",
            "cost2  0.80489516 acc2:  0.7890625\n",
            "cost  0.5553283 acc:  0.828125\n",
            "cost2  0.6345102 acc2:  0.7890625\n",
            "cost  0.53850156 acc:  0.828125\n",
            "cost2  0.44556367 acc2:  0.8671875\n",
            "cost  0.7211567 acc:  0.78125\n",
            "cost2  0.70211965 acc2:  0.7734375\n",
            "cost  0.55807644 acc:  0.8125\n",
            "cost2  0.6082038 acc2:  0.7890625\n",
            "cost  0.7439193 acc:  0.7265625\n",
            "cost2  0.596318 acc2:  0.84375\n",
            "cost  0.5550469 acc:  0.7890625\n",
            "cost2  0.70496315 acc2:  0.78125\n",
            "cost  0.70299816 acc:  0.7734375\n",
            "cost2  0.57337415 acc2:  0.8203125\n",
            "cost  0.6639011 acc:  0.796875\n",
            "cost2  0.7743037 acc2:  0.796875\n",
            "cost  0.58786404 acc:  0.8125\n",
            "cost2  0.7846764 acc2:  0.75\n",
            "cost  0.7604372 acc:  0.78125\n",
            "cost2  0.67338014 acc2:  0.8046875\n",
            "cost  0.6211101 acc:  0.796875\n",
            "cost2  0.7651223 acc2:  0.7421875\n",
            "cost  0.64921594 acc:  0.8046875\n",
            "cost2  0.49232942 acc2:  0.859375\n",
            "cost  0.6286786 acc:  0.78125\n",
            "cost2  0.6107764 acc2:  0.8125\n",
            "cost  0.53570193 acc:  0.828125\n",
            "cost2  0.43092728 acc2:  0.890625\n",
            "cost  0.83779955 acc:  0.71875\n",
            "cost2  0.60537744 acc2:  0.8203125\n",
            "cost  0.61737615 acc:  0.8203125\n",
            "cost2  0.7928655 acc2:  0.7734375\n",
            "cost  0.6440798 acc:  0.8203125\n",
            "cost2  0.5082631 acc2:  0.828125\n",
            "cost  0.5248787 acc:  0.796875\n",
            "cost2  0.6047404 acc2:  0.7890625\n",
            "cost  0.6483903 acc:  0.8125\n",
            "cost2  0.7746612 acc2:  0.8046875\n",
            "cost  0.4977573 acc:  0.84375\n",
            "cost2  0.5228454 acc2:  0.828125\n",
            "cost  0.6291764 acc:  0.8125\n",
            "cost2  0.70970297 acc2:  0.8125\n",
            "cost  0.73144114 acc:  0.8046875\n",
            "cost2  0.94379336 acc2:  0.71875\n",
            "cost  0.54791665 acc:  0.8515625\n",
            "cost2  0.73567504 acc2:  0.7890625\n",
            "cost  0.71733415 acc:  0.7890625\n",
            "cost2  0.539428 acc2:  0.8515625\n",
            "cost  0.7501488 acc:  0.765625\n",
            "cost2  0.6696012 acc2:  0.796875\n",
            "cost  0.5746255 acc:  0.859375\n",
            "cost2  0.3897314 acc2:  0.890625\n",
            "cost  0.67241865 acc:  0.8125\n",
            "cost2  0.6361782 acc2:  0.7734375\n",
            "cost  0.72242284 acc:  0.765625\n",
            "cost2  0.6366688 acc2:  0.8203125\n",
            "cost  0.68100196 acc:  0.8046875\n",
            "cost2  0.6079614 acc2:  0.7890625\n",
            "cost  0.89853823 acc:  0.734375\n",
            "cost2  0.5750312 acc2:  0.8203125\n",
            "cost  0.60645497 acc:  0.7890625\n",
            "cost2  0.5726613 acc2:  0.8828125\n",
            "cost  0.6832326 acc:  0.796875\n",
            "cost2  0.5469309 acc2:  0.8203125\n",
            "cost  0.7301723 acc:  0.7578125\n",
            "cost2  0.41075912 acc2:  0.8828125\n",
            "cost  0.6348538 acc:  0.8515625\n",
            "cost2  0.68119174 acc2:  0.8125\n",
            "cost  0.63670164 acc:  0.8203125\n",
            "cost2  0.70559204 acc2:  0.8125\n",
            "cost  0.6231944 acc:  0.8359375\n",
            "cost2  0.6958046 acc2:  0.796875\n",
            "cost  0.69344103 acc:  0.796875\n",
            "cost2  0.58799917 acc2:  0.828125\n",
            "cost  0.48069748 acc:  0.8359375\n",
            "cost2  0.6692954 acc2:  0.7890625\n",
            "cost  0.5899514 acc:  0.8046875\n",
            "cost2  0.52795696 acc2:  0.8046875\n",
            "cost  0.55920005 acc:  0.78125\n",
            "cost2  0.50486475 acc2:  0.8671875\n",
            "cost  0.8161162 acc:  0.7734375\n",
            "cost2  0.57755077 acc2:  0.828125\n",
            "cost  0.56000316 acc:  0.8046875\n",
            "cost2  0.47155434 acc2:  0.8515625\n",
            "cost  0.73107153 acc:  0.765625\n",
            "cost2  0.5162375 acc2:  0.84375\n",
            "cost  0.6824518 acc:  0.78125\n",
            "cost2  0.7149507 acc2:  0.8046875\n",
            "cost  0.72001076 acc:  0.8046875\n",
            "cost2  0.52671516 acc2:  0.84375\n",
            "cost  0.5355714 acc:  0.8125\n",
            "cost2  0.5176616 acc2:  0.859375\n",
            "cost  0.7907804 acc:  0.78125\n",
            "cost2  0.37053704 acc2:  0.84375\n",
            "cost  0.45160964 acc:  0.84375\n",
            "cost2  0.5851078 acc2:  0.84375\n",
            "cost  0.7012693 acc:  0.796875\n",
            "cost2  0.5695547 acc2:  0.8515625\n",
            "cost  0.62398434 acc:  0.8046875\n",
            "cost2  0.53210956 acc2:  0.875\n",
            "cost  0.7257451 acc:  0.796875\n",
            "cost2  0.9156705 acc2:  0.7734375\n",
            "cost  0.533111 acc:  0.8203125\n",
            "cost2  0.61738783 acc2:  0.8125\n",
            "cost  0.52851474 acc:  0.8828125\n",
            "cost2  0.5281249 acc2:  0.8359375\n",
            "cost  0.6391728 acc:  0.84375\n",
            "cost2  0.5256089 acc2:  0.828125\n",
            "cost  0.63388616 acc:  0.8046875\n",
            "cost2  0.55417305 acc2:  0.84375\n",
            "cost  0.5112406 acc:  0.84375\n",
            "cost2  0.5693617 acc2:  0.8203125\n",
            "cost  0.4654962 acc:  0.8359375\n",
            "cost2  0.60487986 acc2:  0.8203125\n",
            "cost  0.55785114 acc:  0.84375\n",
            "cost2  0.6287606 acc2:  0.8125\n",
            "cost  0.7203253 acc:  0.7734375\n",
            "cost2  0.39517874 acc2:  0.875\n",
            "cost  0.67264223 acc:  0.828125\n",
            "cost2  0.585346 acc2:  0.796875\n",
            "cost  0.5976178 acc:  0.8359375\n",
            "cost2  0.53994554 acc2:  0.8359375\n",
            "cost  0.5682163 acc:  0.8046875\n",
            "cost2  0.6390059 acc2:  0.828125\n",
            "cost  0.42360145 acc:  0.8671875\n",
            "cost2  0.7425429 acc2:  0.75\n",
            "cost  0.7159031 acc:  0.765625\n",
            "cost2  0.72511077 acc2:  0.78125\n",
            "cost  0.5160472 acc:  0.828125\n",
            "cost2  0.58996487 acc2:  0.8203125\n",
            "cost  0.49778026 acc:  0.84375\n",
            "cost2  0.5987628 acc2:  0.78125\n",
            "cost  0.78322744 acc:  0.8203125\n",
            "cost2  0.7178552 acc2:  0.78125\n",
            "cost  0.6444068 acc:  0.7734375\n",
            "cost2  0.6299334 acc2:  0.828125\n",
            "cost  0.56690204 acc:  0.828125\n",
            "cost2  0.6148032 acc2:  0.796875\n",
            "cost  0.6106755 acc:  0.8046875\n",
            "cost2  0.5646243 acc2:  0.8359375\n",
            "cost  0.6708368 acc:  0.8046875\n",
            "cost2  0.575449 acc2:  0.8359375\n",
            "cost  0.4462733 acc:  0.8671875\n",
            "cost2  0.5883385 acc2:  0.828125\n",
            "cost  0.5394436 acc:  0.828125\n",
            "cost2  0.6769464 acc2:  0.8125\n",
            "cost  0.57056284 acc:  0.8203125\n",
            "cost2  0.443393 acc2:  0.8359375\n",
            "cost  0.72415423 acc:  0.78125\n",
            "cost2  0.45806065 acc2:  0.875\n",
            "cost  0.71496946 acc:  0.8046875\n",
            "cost2  0.70276386 acc2:  0.8046875\n",
            "cost  0.5054613 acc:  0.8671875\n",
            "cost2  0.6051253 acc2:  0.828125\n",
            "cost  0.4383866 acc:  0.859375\n",
            "cost2  0.5691268 acc2:  0.8046875\n",
            "cost  0.46560863 acc:  0.8984375\n",
            "cost2  0.6199466 acc2:  0.8515625\n",
            "cost  0.6364959 acc:  0.796875\n",
            "cost2  0.5030121 acc2:  0.8671875\n",
            "cost  0.6882805 acc:  0.84375\n",
            "cost2  0.73441225 acc2:  0.78125\n",
            "cost  0.6524936 acc:  0.8203125\n",
            "cost2  0.7044216 acc2:  0.765625\n",
            "cost  0.4546551 acc:  0.84375\n",
            "cost2  0.7542692 acc2:  0.78125\n",
            "cost  0.6082655 acc:  0.8203125\n",
            "cost2  0.46001852 acc2:  0.84375\n",
            "cost  0.6535128 acc:  0.7890625\n",
            "cost2  0.59249884 acc2:  0.828125\n",
            "cost  0.4594039 acc:  0.8828125\n",
            "cost2  0.3811763 acc2:  0.8984375\n",
            "cost  0.7193855 acc:  0.796875\n",
            "cost2  0.6745914 acc2:  0.8203125\n",
            "cost  0.56948125 acc:  0.8125\n",
            "cost2  0.5738124 acc2:  0.8125\n",
            "cost  0.6791782 acc:  0.7734375\n",
            "cost2  0.42404503 acc2:  0.8671875\n",
            "cost  0.3976491 acc:  0.8515625\n",
            "cost2  0.43525177 acc2:  0.8828125\n",
            "cost  0.4412018 acc:  0.84375\n",
            "cost2  0.61186165 acc2:  0.8203125\n",
            "cost  0.76499236 acc:  0.75\n",
            "cost2  0.53059685 acc2:  0.8515625\n",
            "cost  0.6474056 acc:  0.78125\n",
            "cost2  0.7752714 acc2:  0.8203125\n",
            "cost  0.6164613 acc:  0.8046875\n",
            "cost2  0.7113878 acc2:  0.7734375\n",
            "cost  0.55863416 acc:  0.8671875\n",
            "cost2  0.36018208 acc2:  0.8984375\n",
            "cost  0.68672144 acc:  0.828125\n",
            "cost2  0.7203838 acc2:  0.78125\n",
            "cost  0.5037172 acc:  0.84375\n",
            "cost2  0.7046781 acc2:  0.8125\n",
            "cost  0.60744596 acc:  0.84375\n",
            "cost2  0.43237576 acc2:  0.8515625\n",
            "cost  0.47917578 acc:  0.828125\n",
            "cost2  0.37938163 acc2:  0.890625\n",
            "cost  0.5621607 acc:  0.8125\n",
            "cost2  0.55625963 acc2:  0.84375\n",
            "cost  0.6363803 acc:  0.8203125\n",
            "cost2  0.6372962 acc2:  0.7734375\n",
            "cost  0.6007968 acc:  0.8125\n",
            "cost2  0.4703387 acc2:  0.8359375\n",
            "cost  0.6499866 acc:  0.8203125\n",
            "cost2  0.656373 acc2:  0.8046875\n",
            "cost  0.4383576 acc:  0.8671875\n",
            "cost2  0.5442337 acc2:  0.796875\n",
            "cost  0.6862018 acc:  0.8046875\n",
            "cost2  0.57517684 acc2:  0.84375\n",
            "cost  0.636641 acc:  0.765625\n",
            "cost2  0.77903163 acc2:  0.8046875\n",
            "cost  0.573606 acc:  0.8046875\n",
            "cost2  0.5456139 acc2:  0.84375\n",
            "cost  0.7109487 acc:  0.7890625\n",
            "cost2  0.43302062 acc2:  0.859375\n",
            "cost  0.52388465 acc:  0.8515625\n",
            "cost2  0.6699978 acc2:  0.7890625\n",
            "cost  0.4416046 acc:  0.8671875\n",
            "cost2  0.60627323 acc2:  0.84375\n",
            "cost  0.6684543 acc:  0.8046875\n",
            "cost2  0.45912927 acc2:  0.859375\n",
            "cost  0.46550834 acc:  0.8671875\n",
            "cost2  0.5291242 acc2:  0.796875\n",
            "cost  0.28167105 acc:  0.9140625\n",
            "cost2  0.4161846 acc2:  0.8828125\n",
            "cost  0.5913961 acc:  0.8125\n",
            "cost2  0.65711874 acc2:  0.8125\n",
            "cost  0.4934589 acc:  0.859375\n",
            "cost2  0.54549396 acc2:  0.84375\n",
            "cost  0.5890235 acc:  0.8671875\n",
            "cost2  0.6863019 acc2:  0.78125\n",
            "cost  0.47209018 acc:  0.8359375\n",
            "cost2  0.62869847 acc2:  0.8203125\n",
            "cost  0.65483075 acc:  0.828125\n",
            "cost2  0.45059216 acc2:  0.859375\n",
            "cost  0.5925214 acc:  0.8203125\n",
            "cost2  0.6155757 acc2:  0.7734375\n",
            "cost  0.5333029 acc:  0.8203125\n",
            "cost2  0.48240492 acc2:  0.828125\n",
            "cost  0.42632407 acc:  0.84375\n",
            "cost2  0.43536386 acc2:  0.890625\n",
            "cost  0.4767718 acc:  0.8359375\n",
            "cost2  0.61480975 acc2:  0.84375\n",
            "cost  0.5768641 acc:  0.859375\n",
            "cost2  0.60258144 acc2:  0.7890625\n",
            "cost  0.64396894 acc:  0.796875\n",
            "cost2  0.75999916 acc2:  0.796875\n",
            "cost  0.59770674 acc:  0.84375\n",
            "cost2  0.48258767 acc2:  0.84375\n",
            "cost  0.5789553 acc:  0.8359375\n",
            "cost2  0.5713871 acc2:  0.8125\n",
            "cost  0.55160624 acc:  0.84375\n",
            "cost2  0.67028874 acc2:  0.8203125\n",
            "cost  0.7446804 acc:  0.8125\n",
            "cost2  0.51334 acc2:  0.8359375\n",
            "cost  0.51034766 acc:  0.828125\n",
            "cost2  0.55087507 acc2:  0.8515625\n",
            "cost  0.5872191 acc:  0.7890625\n",
            "cost2  0.5751174 acc2:  0.84375\n",
            "cost  0.6221969 acc:  0.8515625\n",
            "cost2  0.86135375 acc2:  0.78125\n",
            "cost  0.5252516 acc:  0.8359375\n",
            "cost2  0.48711762 acc2:  0.84375\n",
            "cost  0.57574564 acc:  0.8046875\n",
            "cost2  0.317566 acc2:  0.9140625\n",
            "cost  0.55464673 acc:  0.8515625\n",
            "cost2  0.69817084 acc2:  0.8046875\n",
            "cost  0.44017047 acc:  0.8515625\n",
            "cost2  0.5580894 acc2:  0.8515625\n",
            "cost  0.5540197 acc:  0.8515625\n",
            "cost2  0.40349302 acc2:  0.859375\n",
            "cost  0.54720515 acc:  0.859375\n",
            "cost2  0.6447509 acc2:  0.8125\n",
            "cost  0.69417983 acc:  0.7890625\n",
            "cost2  0.5581616 acc2:  0.796875\n",
            "cost  0.627869 acc:  0.796875\n",
            "cost2  0.5587817 acc2:  0.828125\n",
            "cost  0.52131736 acc:  0.84375\n",
            "cost2  0.42108503 acc2:  0.859375\n",
            "cost  0.4568934 acc:  0.84375\n",
            "cost2  0.57869494 acc2:  0.828125\n",
            "cost  0.5308933 acc:  0.8359375\n",
            "cost2  0.4726345 acc2:  0.8515625\n",
            "cost  0.5582919 acc:  0.8046875\n",
            "cost2  0.68233263 acc2:  0.78125\n",
            "cost  0.4063806 acc:  0.8671875\n",
            "cost2  0.65087605 acc2:  0.796875\n",
            "cost  0.434111 acc:  0.875\n",
            "cost2  0.51315117 acc2:  0.84375\n",
            "cost  0.5846913 acc:  0.8203125\n",
            "cost2  0.6369023 acc2:  0.78125\n",
            "cost  0.60283464 acc:  0.8203125\n",
            "cost2  0.5358268 acc2:  0.8203125\n",
            "cost  0.69747615 acc:  0.7890625\n",
            "cost2  0.49120516 acc2:  0.8828125\n",
            "cost  0.55093384 acc:  0.8359375\n",
            "cost2  0.5859233 acc2:  0.84375\n",
            "cost  0.66373056 acc:  0.78125\n",
            "cost2  0.44370845 acc2:  0.875\n",
            "cost  0.50961655 acc:  0.8515625\n",
            "cost2  0.43043432 acc2:  0.8828125\n",
            "cost  0.46533746 acc:  0.828125\n",
            "cost2  0.52524513 acc2:  0.8671875\n",
            "cost  0.7100841 acc:  0.8046875\n",
            "cost2  0.5646676 acc2:  0.8125\n",
            "cost  0.5349994 acc:  0.8125\n",
            "cost2  0.49244276 acc2:  0.859375\n",
            "cost  0.6253763 acc:  0.796875\n",
            "cost2  0.51938385 acc2:  0.8671875\n",
            "cost  0.51337546 acc:  0.828125\n",
            "cost2  0.67125404 acc2:  0.796875\n",
            "cost  0.80420554 acc:  0.7890625\n",
            "cost2  0.55740386 acc2:  0.8203125\n",
            "cost  0.39002064 acc:  0.859375\n",
            "cost2  0.48506916 acc2:  0.859375\n",
            "cost  0.5388515 acc:  0.8203125\n",
            "cost2  0.40071416 acc2:  0.8828125\n",
            "cost  0.4174775 acc:  0.8359375\n",
            "cost2  0.4480526 acc2:  0.8359375\n",
            "cost  0.35689795 acc:  0.890625\n",
            "cost2  0.62763244 acc2:  0.828125\n",
            "cost  0.6924147 acc:  0.84375\n",
            "cost2  0.54832935 acc2:  0.8046875\n",
            "cost  0.64617306 acc:  0.8125\n",
            "cost2  0.47857952 acc2:  0.8515625\n",
            "cost  0.49485213 acc:  0.8359375\n",
            "cost2  0.74357265 acc2:  0.8125\n",
            "cost  0.66698354 acc:  0.7734375\n",
            "cost2  0.7529568 acc2:  0.7734375\n",
            "cost  0.5650434 acc:  0.8046875\n",
            "cost2  0.6189842 acc2:  0.8046875\n",
            "cost  0.6547204 acc:  0.8046875\n",
            "cost2  0.51833785 acc2:  0.8515625\n",
            "cost  0.5304258 acc:  0.8515625\n",
            "cost2  0.5378403 acc2:  0.875\n",
            "cost  0.44870442 acc:  0.859375\n",
            "cost2  0.599402 acc2:  0.828125\n",
            "cost  0.54993826 acc:  0.8515625\n",
            "cost2  0.5656113 acc2:  0.8515625\n",
            "cost  0.49102038 acc:  0.84375\n",
            "cost2  0.69472224 acc2:  0.7890625\n",
            "cost  0.44606692 acc:  0.8671875\n",
            "cost2  0.74622744 acc2:  0.84375\n",
            "cost  0.4020862 acc:  0.921875\n",
            "cost2  0.4621598 acc2:  0.84375\n",
            "cost  0.7425017 acc:  0.7890625\n",
            "cost2  0.38895327 acc2:  0.8828125\n",
            "cost  0.61766136 acc:  0.796875\n",
            "cost2  0.56405586 acc2:  0.828125\n",
            "cost  0.6422399 acc:  0.8125\n",
            "cost2  0.46829045 acc2:  0.890625\n",
            "cost  0.522185 acc:  0.8515625\n",
            "cost2  0.5612006 acc2:  0.8359375\n",
            "cost  0.50104356 acc:  0.859375\n",
            "cost2  0.53286433 acc2:  0.875\n",
            "cost  0.79330176 acc:  0.7734375\n",
            "cost2  0.48559064 acc2:  0.8671875\n",
            "cost  0.4404232 acc:  0.8671875\n",
            "cost2  0.3881887 acc2:  0.8828125\n",
            "cost  0.5447068 acc:  0.828125\n",
            "cost2  0.67291224 acc2:  0.8125\n",
            "cost  0.52420205 acc:  0.8359375\n",
            "cost2  0.6894196 acc2:  0.8359375\n",
            "cost  0.59732795 acc:  0.78125\n",
            "cost2  0.5865774 acc2:  0.765625\n",
            "cost  0.5941286 acc:  0.8203125\n",
            "cost2  0.42186573 acc2:  0.828125\n",
            "cost  0.5915381 acc:  0.8125\n",
            "cost2  0.53241587 acc2:  0.828125\n",
            "cost  0.5794884 acc:  0.828125\n",
            "cost2  0.63418615 acc2:  0.828125\n",
            "cost  0.5985491 acc:  0.796875\n",
            "cost2  0.5298902 acc2:  0.8359375\n",
            "cost  0.6048048 acc:  0.828125\n",
            "cost2  0.46738058 acc2:  0.828125\n",
            "cost  0.5007386 acc:  0.859375\n",
            "cost2  0.6456527 acc2:  0.796875\n",
            "cost  0.53197414 acc:  0.7890625\n",
            "cost2  0.5555605 acc2:  0.84375\n",
            "cost  0.7384758 acc:  0.8203125\n",
            "cost2  0.51695377 acc2:  0.859375\n",
            "cost  0.58125305 acc:  0.859375\n",
            "cost2  0.49942517 acc2:  0.828125\n",
            "cost  0.54633725 acc:  0.8203125\n",
            "cost2  0.5022698 acc2:  0.8203125\n",
            "cost  0.5510036 acc:  0.828125\n",
            "cost2  0.61690867 acc2:  0.859375\n",
            "cost  0.5496447 acc:  0.828125\n",
            "cost2  0.3978182 acc2:  0.8984375\n",
            "cost  0.5823207 acc:  0.828125\n",
            "cost2  0.488122 acc2:  0.8515625\n",
            "cost  0.786857 acc:  0.7578125\n",
            "cost2  0.72159564 acc2:  0.8046875\n",
            "cost  0.6533419 acc:  0.796875\n",
            "cost2  0.57849574 acc2:  0.8515625\n",
            "cost  0.49221304 acc:  0.8671875\n",
            "cost2  0.521601 acc2:  0.8359375\n",
            "cost  0.42741472 acc:  0.859375\n",
            "cost2  0.9006891 acc2:  0.78125\n",
            "cost  0.39993042 acc:  0.8671875\n",
            "cost2  0.28879845 acc2:  0.90625\n",
            "cost  0.4924939 acc:  0.8515625\n",
            "cost2  0.6310736 acc2:  0.8125\n",
            "cost  0.7151088 acc:  0.8046875\n",
            "cost2  0.71283996 acc2:  0.796875\n",
            "cost  0.5057679 acc:  0.890625\n",
            "cost2  0.5875963 acc2:  0.859375\n",
            "cost  0.5029242 acc:  0.859375\n",
            "cost2  0.40823665 acc2:  0.859375\n",
            "cost  0.41064247 acc:  0.8828125\n",
            "cost2  0.38991836 acc2:  0.859375\n",
            "cost  0.5013035 acc:  0.828125\n",
            "cost2  0.49872944 acc2:  0.84375\n",
            "cost  0.78267956 acc:  0.7890625\n",
            "cost2  0.6331392 acc2:  0.8046875\n",
            "cost  0.4009813 acc:  0.8671875\n",
            "cost2  0.58987296 acc2:  0.8125\n",
            "cost  0.82727605 acc:  0.734375\n",
            "cost2  0.6987818 acc2:  0.84375\n",
            "cost  0.44546318 acc:  0.8671875\n",
            "cost2  0.53896356 acc2:  0.828125\n",
            "cost  0.52170086 acc:  0.8359375\n",
            "cost2  0.43407366 acc2:  0.90625\n",
            "cost  0.46072745 acc:  0.8515625\n",
            "cost2  0.56084967 acc2:  0.828125\n",
            "cost  0.58263934 acc:  0.8125\n",
            "cost2  0.48761466 acc2:  0.84375\n",
            "cost  0.41066816 acc:  0.90625\n",
            "cost2  0.536119 acc2:  0.828125\n",
            "cost  0.57395303 acc:  0.8671875\n",
            "cost2  0.43775725 acc2:  0.859375\n",
            "cost  0.34791464 acc:  0.890625\n",
            "cost2  0.37076223 acc2:  0.890625\n",
            "cost  0.53397423 acc:  0.8046875\n",
            "cost2  0.47574362 acc2:  0.8359375\n",
            "cost  0.46294042 acc:  0.8671875\n",
            "cost2  0.3459592 acc2:  0.8671875\n",
            "cost  0.6702162 acc:  0.828125\n",
            "cost2  0.63139826 acc2:  0.8125\n",
            "cost  0.4471684 acc:  0.890625\n",
            "cost2  0.54814786 acc2:  0.890625\n",
            "cost  0.5695687 acc:  0.8359375\n",
            "cost2  0.5355356 acc2:  0.859375\n",
            "cost  0.5396725 acc:  0.8359375\n",
            "cost2  0.6955823 acc2:  0.765625\n",
            "cost  0.571988 acc:  0.828125\n",
            "cost2  0.4561756 acc2:  0.828125\n",
            "cost  0.5623064 acc:  0.8125\n",
            "cost2  0.5992773 acc2:  0.7890625\n",
            "cost  0.46520442 acc:  0.8515625\n",
            "cost2  0.57371724 acc2:  0.828125\n",
            "cost  0.5245986 acc:  0.828125\n",
            "cost2  0.44086057 acc2:  0.8671875\n",
            "cost  0.44944263 acc:  0.8359375\n",
            "cost2  0.69708526 acc2:  0.765625\n",
            "cost  0.45251572 acc:  0.8671875\n",
            "cost2  0.48886156 acc2:  0.8359375\n",
            "cost  0.4615533 acc:  0.8671875\n",
            "cost2  0.50997865 acc2:  0.8359375\n",
            "cost  0.5098658 acc:  0.8359375\n",
            "cost2  0.6072813 acc2:  0.828125\n",
            "cost  0.49522883 acc:  0.8515625\n",
            "cost2  0.61999416 acc2:  0.8515625\n",
            "cost  0.5054584 acc:  0.8515625\n",
            "cost2  0.57678664 acc2:  0.8515625\n",
            "cost  0.5206953 acc:  0.8125\n",
            "cost2  0.48516807 acc2:  0.8671875\n",
            "cost  0.4769385 acc:  0.8359375\n",
            "cost2  0.5263082 acc2:  0.859375\n",
            "cost  0.50785315 acc:  0.828125\n",
            "cost2  0.39101863 acc2:  0.8828125\n",
            "cost  0.58634746 acc:  0.875\n",
            "cost2  0.46498233 acc2:  0.8359375\n",
            "cost  0.44580936 acc:  0.8828125\n",
            "cost2  0.5948129 acc2:  0.828125\n",
            "cost  0.47705442 acc:  0.8359375\n",
            "cost2  0.62022233 acc2:  0.8515625\n",
            "cost  0.5619911 acc:  0.875\n",
            "cost2  0.41501698 acc2:  0.8515625\n",
            "cost  0.38002375 acc:  0.8671875\n",
            "cost2  0.44935256 acc2:  0.8359375\n",
            "cost  0.5612103 acc:  0.8515625\n",
            "cost2  0.44907457 acc2:  0.8671875\n",
            "cost  0.42198056 acc:  0.8828125\n",
            "cost2  0.411855 acc2:  0.8984375\n",
            "cost  0.58690494 acc:  0.84375\n",
            "cost2  0.730007 acc2:  0.8046875\n",
            "cost  0.58268017 acc:  0.84375\n",
            "cost2  0.49833986 acc2:  0.8515625\n",
            "cost  0.69468236 acc:  0.7734375\n",
            "cost2  0.7592289 acc2:  0.78125\n",
            "cost  0.65495926 acc:  0.796875\n",
            "cost2  0.53940845 acc2:  0.8203125\n",
            "cost  0.66906816 acc:  0.78125\n",
            "cost2  0.34129426 acc2:  0.890625\n",
            "cost  0.58763295 acc:  0.78125\n",
            "cost2  0.7288146 acc2:  0.7734375\n",
            "cost  0.4292341 acc:  0.8671875\n",
            "cost2  0.5992687 acc2:  0.828125\n",
            "cost  0.55105585 acc:  0.8125\n",
            "cost2  0.6116978 acc2:  0.84375\n",
            "cost  0.42727366 acc:  0.875\n",
            "cost2  0.7221787 acc2:  0.8359375\n",
            "cost  0.5131377 acc:  0.8671875\n",
            "cost2  0.51113 acc2:  0.859375\n",
            "cost  0.5219165 acc:  0.8046875\n",
            "cost2  0.61804366 acc2:  0.8515625\n",
            "cost  0.36107117 acc:  0.859375\n",
            "cost2  0.5320565 acc2:  0.859375\n",
            "cost  0.4971494 acc:  0.859375\n",
            "cost2  0.6387705 acc2:  0.84375\n",
            "cost  0.654964 acc:  0.8046875\n",
            "cost2  0.5363665 acc2:  0.8203125\n",
            "cost  0.44971097 acc:  0.859375\n",
            "cost2  0.58313036 acc2:  0.828125\n",
            "cost  0.46763176 acc:  0.859375\n",
            "cost2  0.34237665 acc2:  0.8828125\n",
            "cost  0.48950064 acc:  0.859375\n",
            "cost2  0.68619925 acc2:  0.796875\n",
            "cost  0.4601367 acc:  0.875\n",
            "cost2  0.4198535 acc2:  0.8671875\n",
            "cost  0.36492854 acc:  0.8984375\n",
            "cost2  0.61350405 acc2:  0.828125\n",
            "cost  0.53614914 acc:  0.8203125\n",
            "cost2  0.49427298 acc2:  0.8203125\n",
            "cost  0.55070597 acc:  0.84375\n",
            "cost2  0.58690345 acc2:  0.828125\n",
            "cost  0.5033217 acc:  0.859375\n",
            "cost2  0.62680423 acc2:  0.828125\n",
            "cost  0.35504222 acc:  0.875\n",
            "cost2  0.44933504 acc2:  0.8515625\n",
            "cost  0.44755858 acc:  0.8515625\n",
            "cost2  0.55851626 acc2:  0.8828125\n",
            "cost  0.5219338 acc:  0.8359375\n",
            "cost2  0.68180126 acc2:  0.8046875\n",
            "cost  0.3965624 acc:  0.890625\n",
            "cost2  0.44750497 acc2:  0.8671875\n",
            "cost  0.6341859 acc:  0.8125\n",
            "cost2  0.38857415 acc2:  0.8828125\n",
            "cost  0.7306092 acc:  0.796875\n",
            "cost2  0.3555634 acc2:  0.890625\n",
            "cost  0.50982684 acc:  0.8359375\n",
            "cost2  0.48776343 acc2:  0.859375\n",
            "cost  0.54006875 acc:  0.8515625\n",
            "cost2  0.63832444 acc2:  0.796875\n",
            "cost  0.5448729 acc:  0.859375\n",
            "cost2  0.5919561 acc2:  0.796875\n",
            "cost  0.5459715 acc:  0.8125\n",
            "cost2  0.32796353 acc2:  0.875\n",
            "cost  0.62635994 acc:  0.8359375\n",
            "cost2  0.5238526 acc2:  0.8359375\n",
            "cost  0.32287368 acc:  0.9140625\n",
            "cost2  0.5518978 acc2:  0.8359375\n",
            "cost  0.44453248 acc:  0.8515625\n",
            "cost2  0.60884035 acc2:  0.8515625\n",
            "cost  0.34897077 acc:  0.875\n",
            "cost2  0.43743724 acc2:  0.890625\n",
            "cost  0.5494307 acc:  0.8515625\n",
            "cost2  0.3683031 acc2:  0.890625\n",
            "cost  0.4321681 acc:  0.8671875\n",
            "cost2  0.42109996 acc2:  0.875\n",
            "cost  0.5553288 acc:  0.8046875\n",
            "cost2  0.43752104 acc2:  0.890625\n",
            "cost  0.63782465 acc:  0.828125\n",
            "cost2  0.40850282 acc2:  0.8671875\n",
            "cost  0.5472542 acc:  0.8359375\n",
            "cost2  0.4331578 acc2:  0.8828125\n",
            "cost  0.7118062 acc:  0.765625\n",
            "cost2  0.45526823 acc2:  0.875\n",
            "cost  0.41914827 acc:  0.8984375\n",
            "cost2  0.53711474 acc2:  0.8515625\n",
            "cost  0.4442005 acc:  0.859375\n",
            "cost2  0.4316556 acc2:  0.8984375\n",
            "cost  0.62619203 acc:  0.8203125\n",
            "cost2  0.5937213 acc2:  0.828125\n",
            "cost  0.55396426 acc:  0.859375\n",
            "cost2  0.46204144 acc2:  0.859375\n",
            "cost  0.5796003 acc:  0.7734375\n",
            "cost2  0.5107007 acc2:  0.84375\n",
            "cost  0.3377339 acc:  0.875\n",
            "cost2  0.49495506 acc2:  0.8671875\n",
            "cost  0.49313363 acc:  0.859375\n",
            "cost2  0.3696567 acc2:  0.859375\n",
            "cost  0.50330764 acc:  0.828125\n",
            "cost2  0.46121022 acc2:  0.875\n",
            "cost  0.5272105 acc:  0.8203125\n",
            "cost2  0.38515627 acc2:  0.890625\n",
            "cost  0.31510177 acc:  0.8828125\n",
            "cost2  0.7794574 acc2:  0.796875\n",
            "cost  0.5907194 acc:  0.828125\n",
            "cost2  0.45771217 acc2:  0.890625\n",
            "cost  0.59229267 acc:  0.8125\n",
            "cost2  0.80478805 acc2:  0.8203125\n",
            "cost  0.63239485 acc:  0.828125\n",
            "cost2  0.6624212 acc2:  0.8359375\n",
            "cost  0.4989193 acc:  0.84375\n",
            "cost2  0.697841 acc2:  0.8203125\n",
            "cost  0.5469068 acc:  0.8671875\n",
            "cost2  0.46063614 acc2:  0.859375\n",
            "cost  0.5800824 acc:  0.796875\n",
            "cost2  0.5721104 acc2:  0.828125\n",
            "cost  0.3971987 acc:  0.859375\n",
            "cost2  0.44164357 acc2:  0.859375\n",
            "cost  0.5470499 acc:  0.8359375\n",
            "cost2  0.41666037 acc2:  0.8671875\n",
            "cost  0.79815966 acc:  0.7734375\n",
            "cost2  0.24056202 acc2:  0.9375\n",
            "cost  0.72825044 acc:  0.8046875\n",
            "cost2  0.4774407 acc2:  0.8828125\n",
            "cost  0.51285195 acc:  0.8203125\n",
            "cost2  0.4323134 acc2:  0.8515625\n",
            "cost  0.5318686 acc:  0.8671875\n",
            "cost2  0.7213719 acc2:  0.8046875\n",
            "cost  0.5545504 acc:  0.828125\n",
            "cost2  0.35504463 acc2:  0.890625\n",
            "cost  0.41869104 acc:  0.8671875\n",
            "cost2  0.42607322 acc2:  0.921875\n",
            "cost  0.33099562 acc:  0.8671875\n",
            "cost2  0.5688753 acc2:  0.8125\n",
            "cost  0.4222427 acc:  0.8515625\n",
            "cost2  0.42142147 acc2:  0.8671875\n",
            "cost  0.5052687 acc:  0.859375\n",
            "cost2  0.63071835 acc2:  0.8125\n",
            "cost  0.3368481 acc:  0.890625\n",
            "cost2  0.29423085 acc2:  0.8984375\n",
            "cost  0.55922765 acc:  0.8046875\n",
            "cost2  0.5136183 acc2:  0.8515625\n",
            "cost  0.55434513 acc:  0.8359375\n",
            "cost2  0.3559678 acc2:  0.8671875\n",
            "cost  0.5691594 acc:  0.8203125\n",
            "cost2  0.29711977 acc2:  0.9296875\n",
            "cost  0.57207066 acc:  0.8359375\n",
            "cost2  0.5054414 acc2:  0.8203125\n",
            "cost  0.59676063 acc:  0.8359375\n",
            "cost2  0.46407446 acc2:  0.8671875\n",
            "cost  0.49082327 acc:  0.84375\n",
            "cost2  0.71170306 acc2:  0.8046875\n",
            "cost  0.49872237 acc:  0.8046875\n",
            "cost2  0.6318107 acc2:  0.8046875\n",
            "cost  0.3755105 acc:  0.875\n",
            "cost2  0.55561584 acc2:  0.84375\n",
            "cost  0.41215056 acc:  0.890625\n",
            "cost2  0.40100113 acc2:  0.859375\n",
            "cost  0.47207892 acc:  0.8671875\n",
            "cost2  0.4015506 acc2:  0.859375\n",
            "cost  0.5468623 acc:  0.8359375\n",
            "cost2  0.5378168 acc2:  0.8515625\n",
            "cost  0.48203692 acc:  0.8359375\n",
            "cost2  0.428863 acc2:  0.8671875\n",
            "cost  0.3515097 acc:  0.90625\n",
            "cost2  0.36050832 acc2:  0.90625\n",
            "cost  0.42943144 acc:  0.8828125\n",
            "cost2  0.40616792 acc2:  0.8828125\n",
            "cost  0.6627667 acc:  0.8046875\n",
            "cost2  0.35333782 acc2:  0.90625\n",
            "cost  0.40133917 acc:  0.859375\n",
            "cost2  0.59579337 acc2:  0.828125\n",
            "cost  0.44415903 acc:  0.84375\n",
            "cost2  0.6248822 acc2:  0.828125\n",
            "cost  0.44326776 acc:  0.859375\n",
            "cost2  0.48509875 acc2:  0.859375\n",
            "cost  0.38081503 acc:  0.8828125\n",
            "cost2  0.62489575 acc2:  0.828125\n",
            "cost  0.57085925 acc:  0.828125\n",
            "cost2  0.52304715 acc2:  0.84375\n",
            "cost  0.5923724 acc:  0.8203125\n",
            "cost2  0.53844 acc2:  0.84375\n",
            "cost  0.48546916 acc:  0.8515625\n",
            "cost2  0.61350644 acc2:  0.8046875\n",
            "cost  0.45090356 acc:  0.859375\n",
            "cost2  0.4839183 acc2:  0.84375\n",
            "cost  0.612307 acc:  0.8359375\n",
            "cost2  0.645677 acc2:  0.859375\n",
            "cost  0.5265026 acc:  0.828125\n",
            "cost2  0.42461944 acc2:  0.8671875\n",
            "cost  0.47038567 acc:  0.875\n",
            "cost2  0.43726158 acc2:  0.828125\n",
            "cost  0.6141625 acc:  0.8125\n",
            "cost2  0.47598213 acc2:  0.8515625\n",
            "cost  0.4313413 acc:  0.8671875\n",
            "cost2  0.334752 acc2:  0.8828125\n",
            "cost  0.25489813 acc:  0.9453125\n",
            "cost2  0.46243903 acc2:  0.875\n",
            "cost  0.45358628 acc:  0.8359375\n",
            "cost2  0.4467348 acc2:  0.890625\n",
            "cost  0.37291306 acc:  0.890625\n",
            "cost2  0.66640496 acc2:  0.8359375\n",
            "cost  0.5608883 acc:  0.8125\n",
            "cost2  0.49303952 acc2:  0.8671875\n",
            "cost  0.33861804 acc:  0.90625\n",
            "cost2  0.39830747 acc2:  0.8984375\n",
            "cost  0.38932478 acc:  0.890625\n",
            "cost2  0.5669731 acc2:  0.8359375\n",
            "cost  0.60581374 acc:  0.8125\n",
            "cost2  0.38937902 acc2:  0.9140625\n",
            "cost  0.5185356 acc:  0.84375\n",
            "cost2  0.48771712 acc2:  0.859375\n",
            "cost  0.4007009 acc:  0.890625\n",
            "cost2  0.5848553 acc2:  0.8203125\n",
            "cost  0.4577653 acc:  0.859375\n",
            "cost2  0.41966528 acc2:  0.8828125\n",
            "cost  0.614432 acc:  0.8125\n",
            "cost2  0.51805574 acc2:  0.859375\n",
            "cost  0.47716048 acc:  0.859375\n",
            "cost2  0.41991577 acc2:  0.875\n",
            "cost  0.5571599 acc:  0.8359375\n",
            "cost2  0.76734585 acc2:  0.7890625\n",
            "cost  0.38982353 acc:  0.8828125\n",
            "cost2  0.5075126 acc2:  0.84375\n",
            "cost  0.44536984 acc:  0.859375\n",
            "cost2  0.41343713 acc2:  0.890625\n",
            "cost  0.51642716 acc:  0.84375\n",
            "cost2  0.3770513 acc2:  0.890625\n",
            "cost  0.35891014 acc:  0.8984375\n",
            "cost2  0.32099265 acc2:  0.890625\n",
            "cost  0.6115211 acc:  0.828125\n",
            "cost2  0.41433054 acc2:  0.875\n",
            "cost  0.46280316 acc:  0.8671875\n",
            "cost2  0.44747075 acc2:  0.8515625\n",
            "cost  0.5265369 acc:  0.8359375\n",
            "cost2  0.59089947 acc2:  0.8046875\n",
            "cost  0.53996706 acc:  0.8359375\n",
            "cost2  0.5033244 acc2:  0.8671875\n",
            "cost  0.29762834 acc:  0.90625\n",
            "cost2  0.31759858 acc2:  0.8984375\n",
            "cost  0.63320374 acc:  0.8203125\n",
            "cost2  0.39956284 acc2:  0.8671875\n",
            "cost  0.39064187 acc:  0.8671875\n",
            "cost2  0.5860338 acc2:  0.796875\n",
            "cost  0.49388233 acc:  0.84375\n",
            "cost2  0.5269102 acc2:  0.8359375\n",
            "cost  0.43256745 acc:  0.890625\n",
            "cost2  0.5720179 acc2:  0.84375\n",
            "cost  0.54493 acc:  0.859375\n",
            "cost2  0.4090842 acc2:  0.859375\n",
            "cost  0.65481347 acc:  0.8203125\n",
            "cost2  0.51892257 acc2:  0.8203125\n",
            "cost  0.5923033 acc:  0.8046875\n",
            "cost2  0.65500027 acc2:  0.796875\n",
            "cost  0.36195457 acc:  0.890625\n",
            "cost2  0.6198218 acc2:  0.78125\n",
            "cost  0.45076504 acc:  0.8359375\n",
            "cost2  0.58739185 acc2:  0.8203125\n",
            "cost  0.343448 acc:  0.9140625\n",
            "cost2  0.50297314 acc2:  0.8828125\n",
            "cost  0.54135376 acc:  0.8359375\n",
            "cost2  0.47385344 acc2:  0.8359375\n",
            "cost  0.6166181 acc:  0.8046875\n",
            "cost2  0.56099105 acc2:  0.8515625\n",
            "cost  0.5243945 acc:  0.828125\n",
            "cost2  0.5389742 acc2:  0.7890625\n",
            "cost  0.4847787 acc:  0.859375\n",
            "cost2  0.42731515 acc2:  0.8828125\n",
            "cost  0.53826714 acc:  0.828125\n",
            "cost2  0.5365012 acc2:  0.8671875\n",
            "cost  0.30396014 acc:  0.9140625\n",
            "cost2  0.4997106 acc2:  0.8671875\n",
            "cost  0.6115452 acc:  0.828125\n",
            "cost2  0.37271643 acc2:  0.890625\n",
            "cost  0.2851194 acc:  0.8984375\n",
            "cost2  0.40141866 acc2:  0.875\n",
            "cost  0.4307577 acc:  0.8828125\n",
            "cost2  0.51703167 acc2:  0.8671875\n",
            "cost  0.47730732 acc:  0.8671875\n",
            "cost2  0.5459938 acc2:  0.8359375\n",
            "cost  0.41415834 acc:  0.875\n",
            "cost2  0.51615965 acc2:  0.8515625\n",
            "cost  0.46851718 acc:  0.8359375\n",
            "cost2  0.47679543 acc2:  0.859375\n",
            "cost  0.458228 acc:  0.90625\n",
            "cost2  0.5149106 acc2:  0.84375\n",
            "cost  0.47116727 acc:  0.84375\n",
            "cost2  0.48143187 acc2:  0.859375\n",
            "cost  0.45386386 acc:  0.8828125\n",
            "cost2  0.50894487 acc2:  0.8125\n",
            "cost  0.41994458 acc:  0.875\n",
            "cost2  0.4608073 acc2:  0.84375\n",
            "cost  0.4539361 acc:  0.859375\n",
            "cost2  0.32391927 acc2:  0.8828125\n",
            "cost  0.50305474 acc:  0.859375\n",
            "cost2  0.29898548 acc2:  0.90625\n",
            "cost  0.3243246 acc:  0.875\n",
            "cost2  0.48716795 acc2:  0.859375\n",
            "cost  0.3728504 acc:  0.8984375\n",
            "cost2  0.43117246 acc2:  0.8984375\n",
            "cost  0.28600773 acc:  0.9296875\n",
            "cost2  0.4802631 acc2:  0.8515625\n",
            "cost  0.45954376 acc:  0.8671875\n",
            "cost2  0.39359853 acc2:  0.8828125\n",
            "cost  0.425533 acc:  0.8984375\n",
            "cost2  0.41385046 acc2:  0.8671875\n",
            "cost  0.461032 acc:  0.859375\n",
            "cost2  0.49537742 acc2:  0.84375\n",
            "cost  0.3307782 acc:  0.921875\n",
            "cost2  0.4931432 acc2:  0.859375\n",
            "cost  0.3521117 acc:  0.8515625\n",
            "cost2  0.5099016 acc2:  0.8828125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arFOXOt7_vxP",
        "colab_type": "text"
      },
      "source": [
        "# CNN with Tenseflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PMYaRps_zVY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create some wrappers for simplicity\n",
        "def conv2d(x, W, b, strides=1):\n",
        "    # Conv2D wrapper, with bias and relu activation\n",
        "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
        "    x = tf.nn.bias_add(x, b)\n",
        "    return tf.nn.relu(x)\n",
        "\n",
        "\n",
        "def maxpool2d(x, k=2):\n",
        "    # MaxPool2D wrapper\n",
        "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
        "                          padding='SAME')\n",
        "\n",
        "\n",
        "# Create model\n",
        "def conv_net(x, weights, biases):\n",
        "    # Reshape input picture\n",
        "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
        "\n",
        "    # Convolution Layer\n",
        "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
        "    # Max Pooling (down-sampling)\n",
        "    conv1 = maxpool2d(conv1, k=2)\n",
        "\n",
        "    # Convolution Layer\n",
        "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
        "    # Max Pooling (down-sampling)\n",
        "    conv2 = maxpool2d(conv2, k=2)\n",
        "\n",
        "    # Fully connected layer\n",
        "    # Reshape conv2 output to fit fully connected layer input\n",
        "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
        "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
        "    fc1 = tf.nn.relu(fc1)\n",
        "    # Apply Dropout\n",
        "    #fc1 = tf.nn.dropout(fc1, dropout)\n",
        "\n",
        "    # Output, class prediction\n",
        "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
        "    return out\n",
        "  \n",
        "  \n",
        "# Store layers weight & bias\n",
        "weights = {\n",
        "    # 5x5 conv, 1 input, 32 outputs\n",
        "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
        "    # 5x5 conv, 32 inputs, 64 outputs\n",
        "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
        "    # fully connected, 7*7*64 inputs, 1024 outputs\n",
        "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
        "    # 1024 inputs, 10 outputs (class prediction)\n",
        "    'out': tf.Variable(tf.random_normal([1024, n_classes]))\n",
        "}\n",
        "\n",
        "biases = {\n",
        "    'bc1': tf.Variable(tf.random_normal([32])),\n",
        "    'bc2': tf.Variable(tf.random_normal([64])),\n",
        "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
        "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9v3V8ztAKDCg",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "\n",
        "Before we jump into Tensorflow, we will implemented our first neural network model using Python Numpy package. NumPy is the fundamental package for scientific computing with Python, such as:\n",
        "\n",
        "1. Linear Algebra\n",
        "2. Statistics\n",
        "3. Calculus\n",
        "\n",
        "## A brief intro to Numpy operations:\n",
        "\n",
        "1. Creating a Vector:\n",
        "Here we use Numpy to create a 1-D Array which we then call a vector.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTf7M4r7Lgj9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load Library\n",
        "import numpy as np\n",
        "\n",
        "#Create a vector as a Row\n",
        "vector_row = np.array([1,2,3])\n",
        "\n",
        "#Create vector as a Column\n",
        "vector_column = np.array([[1],[2],[3]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYFjSo0OLqA3",
        "colab_type": "text"
      },
      "source": [
        "2. Creating a Matrix\n",
        "We Create a 2-D Array in Numpy and call it a Matrix. It contains 2 rows and 3 columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJlDBq5rLmA-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load Library\n",
        "import numpy as np\n",
        "\n",
        "#Create a Matrix\n",
        "matrix = np.array([[1,2,3],[4,5,6]])\n",
        "print(matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wv99hZqULygH",
        "colab_type": "text"
      },
      "source": [
        "3. Selecting Elements\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLQlxFzkPrKM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load Library\n",
        "import numpy as np\n",
        "\n",
        "#Create a vector as a Row\n",
        "vector_row = np.array([ 1,2,3,4,5,6 ])\n",
        "\n",
        "#Create a Matrix\n",
        "matrix = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
        "print(matrix)\n",
        "\n",
        "#Select 3rd element of Vector\n",
        "print(vector_row[2])\n",
        "\n",
        "#Select 2nd row 2nd column\n",
        "print(matrix[1,1])\n",
        "#Select all elements of a vector\n",
        "print(vector_row[:])\n",
        "#Select everything up to and including the 3rd element\n",
        "print(vector_row[:3])\n",
        "#Select the everything after the 3rd element\n",
        "print(vector_row[3:])\n",
        "#Select the last element\n",
        "print(vector_row[-1])\n",
        "#Select the first 2 rows and all the columns of the matrix\n",
        "print(matrix[:2,:])\n",
        "#Select all rows and the 2nd column of the matrix\n",
        "print(matrix[:,1:2])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QO3vTGEKQhm7",
        "colab_type": "text"
      },
      "source": [
        "4. Describing a Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8bDjBhhQpg5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "#Create a Matrix\n",
        "matrix = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
        "#View the Number of Rows and Columns\n",
        "print(matrix.shape)\n",
        "#View the number of elements (rows*columns)\n",
        "print(matrix.size)\n",
        "#View the number of Dimensions(2 in this case)\n",
        "print(matrix.ndim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKISvY8kQtA0",
        "colab_type": "text"
      },
      "source": [
        "5. Finding the max and min values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abPJd0JrQ4mM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load Library\n",
        "import numpy as np\n",
        "\n",
        "#Create a Matrix\n",
        "matrix = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
        "print(matrix)\n",
        "#Return the max element\n",
        "print(np.max(matrix))\n",
        "#Return the min element\n",
        "print(np.min(matrix))\n",
        "#To find the max element in each column\n",
        "print(np.max(matrix,axis=0))\n",
        "#To find the max element in each row\n",
        "print(np.max(matrix,axis=1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Qm64s_eR0zQ",
        "colab_type": "text"
      },
      "source": [
        "6. Reshaping Arrays\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pwepq7h_SBBD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load Library\n",
        "import numpy as np\n",
        "\n",
        "#Create a Matrix\n",
        "matrix = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
        "print(matrix)\n",
        "#Reshape\n",
        "print(matrix.reshape(9,1))\n",
        "#Here -1 says as many columns as needed and 1 row\n",
        "print(matrix.reshape(1,-1))\n",
        "#If we provide only 1 value Reshape would return a 1-d array of that length\n",
        "print(matrix.reshape(9))\n",
        "#We can also use the Flatten method to convert a matrix to 1-d array\n",
        "print(matrix.flatten())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJU3xABuVem_",
        "colab_type": "text"
      },
      "source": [
        "7. Calculating Dot Products"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPKg382VVivy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load Library\n",
        "import numpy as np\n",
        "\n",
        "#Create vector-1\n",
        "vector_1 = np.array([ 1,2,3 ])\n",
        "#Create vector-2\n",
        "vector_2 = np.array([ 4,5,6 ])\n",
        "#Calculate Dot Product\n",
        "print(np.dot(vector_1,vector_2))\n",
        "#Alternatively you can use @ to calculate dot products\n",
        "print(vector_1 @ vector_2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cB-jK7jEXY7F",
        "colab_type": "text"
      },
      "source": [
        "##Linear regression in Numpy:\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Write the numpy code for the following model:\n",
        "\n",
        "$Y=WX+B$\n",
        "\n",
        "where $X$ is 3x10 matrix:  10 samples and 3 features\n",
        "\n",
        "$Y$ is 4x10 matrix: 10 samples and 4 outputs\n",
        "\n",
        "$W$ is the weights matrix with the shape 4x3: connecting 3 inputs to 4 outputs\n",
        "\n",
        "$b$ is a vector with a size 4 ( one bias per output)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EtM5LVtWCpm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load Library\n",
        "import numpy as np\n",
        "\n",
        "# Generate a random X (we do not have a real data)\n",
        "X = np.random.rand(3,10)\n",
        "display(X.shape)\n",
        "\n",
        "# Generate a random weights vector\n",
        "W = np.random.rand(4,3)\n",
        "\n",
        "# Generate a random bias \n",
        "b = np.random.rand(4,1)\n",
        "\n",
        "# Calculate Y\n",
        "Y= np.dot(W,X) + b\n",
        "display(Y.shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMIoucH9hFfr",
        "colab_type": "text"
      },
      "source": [
        "## One neuron model in numpy:\n",
        "\n",
        "A single neuron has multiple inputs and one output, in addition to the linear regression model, we need to add non linearity through an activation function:\n",
        "\n",
        "$Y= f(WX+B)$\n",
        "\n",
        "where $X$ is n x m matrix:  m samples and n features/inputs\n",
        "\n",
        "$f(g)= \\frac{1}{1+\\exp(-g)}$  is a sigmoid acitavation function\n",
        "\n",
        "$Y$ is nh1 x m matrix: m samples and ny outputs\n",
        "\n",
        "$W$ is the weights matrix with the shape nh1 x n: connecting 3 inputs to 4 outputs\n",
        "\n",
        "$b$ is a vector with a size nh1 ( one bias per output)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qry1JDGEiLmx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load Library\n",
        "import numpy as np \n",
        "\n",
        "f = lambda x: 1.0/(1.0 + np.exp(-x)) # activation function (use sigmoid)\n",
        "\n",
        "# Generate a random X (we do not have a real data)\n",
        "X = np.random.rand(3,10)\n",
        "\n",
        "\n",
        "# Generate a random weights vector\n",
        "W = np.random.rand(1,3)\n",
        "\n",
        "\n",
        "# Generate a random bias \n",
        "b = np.random.rand()\n",
        "\n",
        "# Calculate Y\n",
        "Y= f(np.dot(W,X) + b)\n",
        "display(Y)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSnbti9ooIIs",
        "colab_type": "text"
      },
      "source": [
        "## One hidden layer model in numpy:\n",
        "\n",
        "The difference from the one neuron model is simple:  we need only to change the number of output \"ny\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAY3o6zBnpA0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load Library\n",
        "import numpy as np \n",
        "\n",
        "#Suppose we have the following NN architecture\n",
        "\n",
        "m = 10 # Number of samples\n",
        "ni= 3 # Number of input neurons\n",
        "h = 1 # Number of hidden layers\n",
        "nh1 = 4 # Number of neurons in the hidden layer 1\n",
        "no =1 # Number of neurons in the output layer\n",
        "\n",
        "\n",
        "\n",
        "f = lambda x: 1.0/(1.0 + np.exp(-x)) # activation function (use sigmoid)\n",
        "\n",
        "# Generate a random X (we do not have a real data)\n",
        "X = np.random.rand(ni,m)\n",
        "\n",
        "\n",
        "# Generate a random weights vector for the first hidden layer\n",
        "W1 = np.random.rand(nh1,ni)\n",
        "\n",
        "\n",
        "# Generate a random bias for the first hidden layer \n",
        "b1 = np.random.rand(nh1,1)\n",
        "\n",
        "# Generate a random weights vector for the output layer\n",
        "W2 = np.random.rand(no,nh1)\n",
        "\n",
        "# Generate a random bias for the output layer \n",
        "b2 = np.random.rand(no,1)\n",
        "\n",
        "# Calculate output of the first hidden layer\n",
        "Yh1= f(np.dot(W1,X) + b1)\n",
        "\n",
        "# Calculate output of the output layer\n",
        "\n",
        "Y= f(np.dot(W2,Yh1) + b2)\n",
        "\n",
        "display(Yh1.shape)\n",
        "display(Y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11fqb_bQvIEi",
        "colab_type": "text"
      },
      "source": [
        "## Gradient descent in Numpy:\n",
        "Let us now start training a neural network\n",
        "We start by implementing a simple gradient descent for linear regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzBJxwb7FFZ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaQyLoxk2FyG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "converged = False\n",
        "iter = 0\n",
        "m = 10 # Number of samples\n",
        "ni= 1 # Number of input neurons\n",
        "h = 1 # Number of hidden layers\n",
        "no =1 # Number of neurons in the output layer\n",
        "\n",
        "# Generate a random X (we do not have a real data)\n",
        "X = np.random.rand(m)\n",
        "display(X)\n",
        "\n",
        "# learning rate\n",
        "alpha =0.01\n",
        "\n",
        "# early stop criteria \n",
        "ep=0.001\n",
        "\n",
        "# maximum number of training iterations\n",
        "max_iter=100\n",
        "\n",
        "# Generate a random weights vector for the output layer\n",
        "W1 = np.random.rand()\n",
        "\n",
        "# Generate a random bias for the output layer \n",
        "b1 = np.random.rand()\n",
        "\n",
        "# Generate a random ground truth\n",
        "Y_gr = np.random.rand(m)\n",
        "\n",
        "\n",
        "J = sum([(b1 + W1*X[i] - Y_gr[i])**2 for i in range(m)])\n",
        "\n",
        "while not converged:\n",
        "        # for each training sample, compute the gradient (d/d_theta j(theta))\n",
        "        grad0 = 1.0/m * sum([(b1 + W1*X[i] - Y_gr[i]) for i in range(m)]) \n",
        "        grad1 = 1.0/m * sum([(b1 + W1*X[i] - Y_gr[i])*X[i] for i in range(m)])\n",
        "        \n",
        "        # update the theta_temp\n",
        "        temp0 = W1 - alpha * grad0\n",
        "        temp1 = b1 - alpha * grad1\n",
        "        # update theta\n",
        "        W1 = temp0\n",
        "        b1 = temp1\n",
        "        \n",
        "        # sum squared error\n",
        "        e = sum([(b1 + W1*X[i] - Y_gr[i])**2 for i in range(m)]) \n",
        "\n",
        "        if abs(J-e) <= ep:\n",
        "            print('Converged, iterations: ', iter, '!!!')\n",
        "            converged = True\n",
        "    \n",
        "        J = e   # update error \n",
        "        iter += 1  # update iter\n",
        "    \n",
        "        if iter == max_iter:\n",
        "            print('Max interactions exceeded!')\n",
        "            converged = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRqCCVlmFIZl",
        "colab_type": "text"
      },
      "source": [
        "##Assignment 1\n",
        "### Backpropagation in Numpy:\n"
      ]
    }
  ]
}