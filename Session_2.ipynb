{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Session 2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahmadhajmosa/Machine-learning-labs/blob/AlessandroFornasier/Session_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmSUExzC-ZBV",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgWZldyi-Zef",
        "colab_type": "text"
      },
      "source": [
        "# Lab on Machine Learning and Applications in Intelligent Vehicles\n",
        "## Session 1: Introduction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a75NLOuwKXSs",
        "colab_type": "text"
      },
      "source": [
        "#Session 2: 05.06 - 13:00 - 14:30 :\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciUtGM-W_ehP",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "## Intro:\n",
        "\n",
        "Tensorflow is a powerful framework for implementing and deploying large-scale deep learning models. Recently, it has been widely used in both reasearch and production. TF objective is to combine scale and flexibility.\n",
        "\n",
        "In the past session, we will learning the following:\n",
        "\n",
        "1. TF programming stack\n",
        "2. TF programming concepts including computatoin graphs, operations and sessions. \n",
        "3. Implementation of linear regression\n",
        "4. Implementation of feed-forward neural networks\n",
        "\n",
        "## TF stack:\n",
        "\n",
        "TensorFlow is a framework composed of two core building blocks — a library for defining computational graphs and a runtime for executing such graphs on a variety of different hardware\n",
        "\n",
        "\n",
        "![alt text](https://www.tensorflow.org/images/layers.png)\n",
        "\n",
        "\n",
        "Before goining into details about the stack, let us talk about computational graphs.\n",
        "\n",
        "### Computational Graphs\n",
        "\n",
        "A directed graph is a data structure consisting of nodes (vertices) and edges. It’s a set of vertices connected pairwise by directed edges.\n",
        "\n",
        "Graphs come in many shapes and sizes and are used to solve many real-life problems, such as representing networks including telephone networks, circuit networks, road networks, and even social networks. \n",
        "![alt text](https://cdn-images-1.medium.com/max/800/1*V6aYjD3AxDbEKYahkGqVQw.png)\n",
        "\n",
        "TensorFlow uses directed graphs internally to represent computations, and they call this data flow graphs (or computational graphs).\n",
        "\n",
        "The nodes in TF data flow graph mostly represents operations, variables and placeholders.\n",
        "\n",
        "Take for example the following operation:\n",
        "![alt text](https://cdn-images-1.medium.com/max/800/1*6E3sfit6DCeJ9mOz17g4bA.png)\n",
        "\n",
        "To create a computational graph out of this program, we create nodes for each of the operations in our program, along with the input variables a and b. In fact, a and b could be constants if they don’t change. If one node is used as the input to another operation we draw a directed arrow that goes from one node to another.\n",
        "\n",
        "The computational graph for this program might look like this:\n",
        "![alt text](https://cdn-images-1.medium.com/max/800/1*vPb9E0Yd1QUAD0oFmAgaOw.png)\n",
        "\n",
        "Operations create or manipulate data according to specific rules. In TensorFlow those rules are called Ops, short for operations. Variables on the other hand represent shared, persistent state that can be manipulated by running Ops on those variables.\n",
        "\n",
        "The questions now what are the advantages of representing operations as directed graphs: The main advantage of using directed graphs is the ability to do **parallelism** and what is called **dependency driving scheduling**. \n",
        "For example, consider again the follwoing code:\n",
        "![alt text](https://cdn-images-1.medium.com/max/800/1*6E3sfit6DCeJ9mOz17g4bA.png)\n",
        "At the most fundamental level, most computer programs are mainly composed of two things — primitive operations and an order in which these operations are executed, often sequentially, line by line. This means we would first multiply a and b and only when this expression was evaluated we would take their sum. Computational graphs on the otherhand, exclusively specify the dependencies across the operations.\n",
        "If we look at our computational graph we see that we could execute the multiplication and addition in parallel. That’s because these two operations do not depend on each other.\n",
        " So we can use the topology of the graph to drive the scheduling of operations and execute them in the most efficient manner, e.g. using multiple GPUs on a single machine or even distribute the execution across multiple machines.\n",
        " Another key advantage is portability. The graph is a language-independent representation of our code. So we can build the graph in Python, save the model (TensorFlow uses protocol buffers), and restore the model in a different language, say C++, if you want to go really fast.\n",
        " \n",
        " \n",
        "\n",
        "--------------------------------\n",
        "# References:\n",
        "\n",
        "https://medium.com/@d3lm/understand-tensorflow-by-mimicking-its-api-from-scratch-faa55787170d\n",
        "\n",
        "https://www.tensorflow.org/guide/extend/architecture\n",
        "\n",
        "https://www.tensorflow.org/guide/low_level_intro\n",
        "\n",
        "  \n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-GFJPVDnEwx",
        "colab_type": "text"
      },
      "source": [
        "# placeholder: tensors are feeded externaly for example inputs tensors + output tensors\n",
        "\n",
        "# variables : tensors represent the parameters of the network/graph ie. nn weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlmSCbhtoJBs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.001\n",
        "training_iters = 2000\n",
        "batch_size = 128\n",
        "\n",
        "# Network Parameters\n",
        "num_inputs = 3\n",
        "num_outputs = 4\n",
        "num_samples= 10\n",
        "\n",
        "# Training data\n",
        "x_gr = np.random.rand(num_samples,num_inputs)\n",
        "y_gr = np.random.rand(num_samples,num_outputs)\n",
        "\n",
        "# tf Graph input\n",
        "x = tf.placeholder(tf.float32, [None, num_inputs])\n",
        "y = tf.placeholder(tf.float32, [None, num_outputs])\n",
        "\n",
        "# weights \n",
        "w_1 = tf.Variable(tf.random_normal([num_inputs,num_outputs ]))\n",
        "\n",
        "# model (matmul is the dotproduct)\n",
        "y_p = tf.matmul(x, w_1)\n",
        "\n",
        "# cost\n",
        "cost = tf.reduce_mean(tf.pow(y-y_p,2))\n",
        "\n",
        "# optimisation \n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# initalizing the graph and the weights\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Launch the graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    \n",
        "    for i in range(10):\n",
        "        #first run the optimizer (one iteration of the gradient descent)\n",
        "        #the feed_dict is a dictionary of the data that has to be unsed instead\n",
        "        #of the placeholders\n",
        "        sess.run(optimizer, feed_dict={x: x_gr, y: y_gr}) \n",
        "\n",
        "        pr_cost = sess.run(cost, feed_dict={x: x_gr,y: y_gr})\n",
        "    \n",
        "        print('iter: ',i, 'cost: ', pr_cost)\n",
        "    \n",
        "    y_p_p = sess.run(y_p, feed_dict={x: x_gr, y: y_gr})\n",
        "    \n",
        "    print('predicted ', y_p_p)\n",
        "    print('real ', y_gr)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzDUVNSSwBH4",
        "colab_type": "code",
        "outputId": "219b34e5-9b4b-4764-dbca-33c17dd65028",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Parameters\n",
        "learningRate = 0.001\n",
        "trainingIterations = 2000\n",
        "batchSize = 128\n",
        "\n",
        "# Network Parameters\n",
        "nInputs = 3\n",
        "nOutputs = 4\n",
        "nHiddenLayerNeurons = np.array([4,10])\n",
        "nSamples= 10\n",
        "\n",
        "# Training data (GroundTruth)\n",
        "xGroundTruth = np.random.rand(nSamples,nInputs)\n",
        "yGroundTruth = np.random.rand(nSamples,nOutputs)\n",
        "\n",
        "# TensorFlow Graph input/output placeholders\n",
        "x = tf.placeholder(tf.float32, [None, nInputs])\n",
        "y = tf.placeholder(tf.float32, [None, nOutputs])\n",
        "\n",
        "# Weights \n",
        "W1 = tf.Variable(tf.random_normal([nInputs,nHiddenLayerNeurons[0]]))\n",
        "W2 = tf.Variable(tf.random_normal([nHiddenLayerNeurons[0],nHiddenLayerNeurons[1]]))\n",
        "W3 = tf.Variable(tf.random_normal([nHiddenLayerNeurons[1],nOutputs]))\n",
        "\n",
        "# Biases\n",
        "B1 = tf.Variable(tf.random_normal([nHiddenLayerNeurons[0]]))\n",
        "B2 = tf.Variable(tf.random_normal([nHiddenLayerNeurons[1]]))\n",
        "B3 = tf.Variable(tf.random_normal([nOutputs]))\n",
        "\n",
        "# Model\n",
        "H1 = tf.nn.sigmoid(tf.add(tf.matmul(x,W1),B1))\n",
        "H2 = tf.nn.sigmoid(tf.add(tf.matmul(H2,W2),B2))\n",
        "Y = tf.add(tf.matmul(H2,W3),B3)\n",
        "\n",
        "# Cost ... complete from here ...\n",
        "cost = tf.reduce_mean(tf.pow(y-y_p,2))\n",
        "\n",
        "# optimisation \n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# initalizing the graph and the weights\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Launch the graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    \n",
        "    for i in range(10):\n",
        "        #first run the optimizer (one iteration of the gradient descent)\n",
        "        #the feed_dict is a dictionary of the data that has to be unsed instead\n",
        "        #of the placeholders\n",
        "        sess.run(optimizer, feed_dict={x: x_gr, y: y_gr}) \n",
        "\n",
        "        pr_cost = sess.run(cost, feed_dict={x: x_gr,y: y_gr})\n",
        "    \n",
        "        print('iter: ',i, 'cost: ', pr_cost)\n",
        "    \n",
        "    y_p_p = sess.run(y_p, feed_dict={x: x_gr, y: y_gr})\n",
        "    \n",
        "    print('predicted ', y_p_p)\n",
        "    print('real ', y_gr)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAqwBs3WzHRr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Libraries import\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Import MNIST data input (28x28 image --> mnist.train.images.shape)\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
        "\n",
        "#import imshow\n",
        "from matplotlib.pyplot import imshow\n",
        "%matplotlib inline\n",
        "\n",
        "# Extract the traning, test and validation data and labels\n",
        "xTrain = mnist.train.images\n",
        "yTrain = mnist.train.labels\n",
        "xTest = mnist.test.images\n",
        "yTest = mnist.test.labels\n",
        "xValidation = mnist.validation.images\n",
        "yValidation = mnist.validation.labels\n",
        "\n",
        "# Parameters\n",
        "learningRate = 0.001\n",
        "trainingIterations = 2000\n",
        "batchSize = 128\n",
        "\n",
        "# Network Parameters\n",
        "nInputs =  mnist.train.images[0].size\n",
        "nOutputs = mnist.train.labels[0].size\n",
        "nHiddenLayerNeurons = np.array([100,100])\n",
        "nSamples= 10\n",
        "\n",
        "# Training data (GroundTruth)\n",
        "xGroundTruth = np.random.rand(nSamples,nInputs)\n",
        "yGroundTruth = np.random.rand(nSamples,nOutputs)\n",
        "\n",
        "# TensorFlow Graph input/output placeholders\n",
        "x = tf.placeholder(tf.float32, [None, nInputs])\n",
        "y = tf.placeholder(tf.float32, [None, nOutputs])\n",
        "\n",
        "# Weights \n",
        "W1 = tf.Variable(tf.random_normal([nInputs,nHiddenLayerNeurons[0]]))\n",
        "W2 = tf.Variable(tf.random_normal([nHiddenLayerNeurons[0],nHiddenLayerNeurons[1]]))\n",
        "W3 = tf.Variable(tf.random_normal([nHiddenLayerNeurons[1],nOutputs]))\n",
        "\n",
        "# Biases\n",
        "B1 = tf.Variable(tf.random_normal([nHiddenLayerNeurons[0]]))\n",
        "B2 = tf.Variable(tf.random_normal([nHiddenLayerNeurons[1]]))\n",
        "B3 = tf.Variable(tf.random_normal([nOutputs]))\n",
        "\n",
        "# Model\n",
        "H1 = tf.nn.sigmoid(tf.add(tf.matmul(x,W1),B1))\n",
        "H2 = tf.nn.sigmoid(tf.add(tf.matmul(H1,W2),B2))\n",
        "Y = tf.add(tf.matmul(H2,W3),B3)\n",
        "\n",
        "# Model Evaluation (tf.equal compare the 2 matricies and returns another matrix\n",
        "# with every values equal to true if the 2 value in the matricies are the same\n",
        "# false otherwise)\n",
        "nCorrectPrediction = tf.equal(tf.argmax(Y,1), tf.argmax(y,1))\n",
        "\n",
        "# Cost\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Y, labels=y))\n",
        "\n",
        "# Accurancy\n",
        "accurancy = tf.reduce_mean(tf.cast(nCorrectPrediction, tf.float32))\n",
        "\n",
        "# Optimizer\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learningRate).minimize(cost)\n",
        "\n",
        "# Initalizing the graph and the weights\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Launch the graph\n",
        "with tf.Session() as sess:\n",
        "  \n",
        "    # Session initialization\n",
        "    sess.run(init)\n",
        "    \n",
        "    for iteration in range(trainingIterations):\n",
        "      \n",
        "      # Take the samples from the dataset of the specified batchSize\n",
        "      xTrainBatch, yTrainBatch = mnist.train.next_batch(batchSize)\n",
        "      \n",
        "      # Take the samples from the dataset of the specified batchSize\n",
        "      xTestBatch, yTestBatch = mnist.test.next_batch(batchSize)\n",
        "      \n",
        "      # First run the optimizer (one iteration of the gradient descent)\n",
        "      # the feed_dict is a dictionary of the data that has to be unsed\n",
        "      # insteadof the placeholders\n",
        "      sess.run(optimizer, feed_dict={x: xTrainBatch, y: yTrainBatch}) \n",
        "\n",
        "      # Evaluate the cost and accurancy during the traning process\n",
        "      trainCost, trainAccurancy = sess.run([cost, accurancy], feed_dict={x: xTrainBatch, y: yTrainBatch})\n",
        "      \n",
        "      # Evaluate the cost and accurancy during the traning process\n",
        "      testCost, testAccurancy = sess.run([cost, accurancy], feed_dict={x: xTestBatch, y: yTestBatch})\n",
        "      \n",
        "      print('\\nIteration: ', iteration, 'trainCost: ', trainCost, 'trainAccurancy: ', trainAccurancy, 'testCost: ', testCost, 'testAccurancy: ', testAccurancy )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JcEe5EP-L9L",
        "colab_type": "text"
      },
      "source": [
        "## CNN in TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6baFZQFv-QZ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Libraries import\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# ------------------------- Function definition -------------------------\n",
        "\n",
        "# Conv2D with bias and relu activation\n",
        "def conv2d(x, W, b, strides=1):\n",
        "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
        "    x = tf.nn.bias_add(x, b)\n",
        "    return tf.nn.relu(x)\n",
        "\n",
        "# MaxPool2D\n",
        "def maxpool2d(x, k=2):\n",
        "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n",
        "  \n",
        "# Model\n",
        "def conv_net(x, weights, biases):\n",
        "  \n",
        "    # Reshape input picture\n",
        "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
        "\n",
        "    # Convolution Layer\n",
        "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
        "    \n",
        "    # Max Pooling (down-sampling)\n",
        "    conv1 = maxpool2d(conv1, k=2)\n",
        "\n",
        "    # Convolution Layer\n",
        "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
        "    \n",
        "    # Max Pooling (down-sampling)\n",
        "    conv2 = maxpool2d(conv2, k=2)\n",
        "\n",
        "    # Flatten reshaping\n",
        "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
        "    \n",
        "    # Fully connected input layer\n",
        "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
        "    fc1 = tf.nn.relu(fc1)\n",
        "    \n",
        "    # Apply Dropout\n",
        "    #fc1 = tf.nn.dropout(fc1, dropout)\n",
        "\n",
        "    # Fully connected output layer --> class prediction\n",
        "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
        "    \n",
        "    return out\n",
        "  \n",
        "# -----------------------------------------------------------------------\n",
        "\n",
        "# Import MNIST data input (28x28 image --> mnist.train.images.shape)\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
        "\n",
        "#import imshow\n",
        "from matplotlib.pyplot import imshow\n",
        "%matplotlib inline\n",
        "\n",
        "# Extract the traning, test and validation data and labels\n",
        "xTrain = mnist.train.images\n",
        "yTrain = mnist.train.labels\n",
        "xTest = mnist.test.images\n",
        "yTest = mnist.test.labels\n",
        "xValidation = mnist.validation.images\n",
        "yValidation = mnist.validation.labels\n",
        "\n",
        "# Parameters\n",
        "learningRate = 0.001\n",
        "trainingIterations = 2000\n",
        "batchSize = 128\n",
        "#dropout = 0.75\n",
        "\n",
        "# Network Parameters\n",
        "nInputs =  mnist.train.images[0].size\n",
        "nOutputs = mnist.train.labels[0].size\n",
        "\n",
        "# TensorFlow Graph input/output placeholders\n",
        "x = tf.placeholder(tf.float32, [None, nInputs])\n",
        "y = tf.placeholder(tf.float32, [None, nOutputs])\n",
        "                   \n",
        "# Weights\n",
        "weights = {\n",
        "    \n",
        "    # 5x5 conv kernel for conv layer 1, 1 input, 32 output images\n",
        "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
        "    \n",
        "    # 5x5 conv kernel for conv layer 2, 32 inputs, 64 output images\n",
        "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
        "    \n",
        "    # Input fully connected layer, 7*7*64 inputs, 1024 output images\n",
        "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
        "    \n",
        "    # Output fully connected layer 1024 inputs, 10 output images\n",
        "    'out': tf.Variable(tf.random_normal([1024, nOutputs]))\n",
        "}\n",
        "\n",
        "# Biases\n",
        "biases = {\n",
        "    'bc1': tf.Variable(tf.random_normal([32])),\n",
        "    'bc2': tf.Variable(tf.random_normal([64])),\n",
        "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
        "    'out': tf.Variable(tf.random_normal([nOutputs]))\n",
        "}\n",
        "\n",
        "# Model\n",
        "Y = conv_net(x, weights, biases)\n",
        "\n",
        "# Model Evaluation (tf.equal compare the 2 matricies and returns another matrix\n",
        "# with every values equal to true if the 2 value in the matricies are the same\n",
        "# false otherwise)\n",
        "nCorrectPrediction = tf.equal(tf.argmax(Y,1), tf.argmax(y,1))\n",
        "\n",
        "# Cost\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Y, labels=y))\n",
        "\n",
        "# Accurancy\n",
        "accurancy = tf.reduce_mean(tf.cast(nCorrectPrediction, tf.float32))\n",
        "\n",
        "# Optimizer\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learningRate).minimize(cost)\n",
        "\n",
        "# Initalizing the graph and the weights\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Launch the graph\n",
        "with tf.Session() as sess:\n",
        "  \n",
        "    # Session initialization\n",
        "    sess.run(init)\n",
        "    \n",
        "    for iteration in range(trainingIterations):\n",
        "      \n",
        "      # Take the samples from the dataset of the specified batchSize\n",
        "      xTrainBatch, yTrainBatch = mnist.train.next_batch(batchSize)\n",
        "      \n",
        "      # Take the samples from the dataset of the specified batchSize\n",
        "      xTestBatch, yTestBatch = mnist.test.next_batch(batchSize)\n",
        "      \n",
        "      # First run the optimizer (one iteration of the gradient descent)\n",
        "      # the feed_dict is a dictionary of the data that has to be unsed\n",
        "      # insteadof the placeholders\n",
        "      sess.run(optimizer, feed_dict={x: xTrainBatch, y: yTrainBatch}) \n",
        "\n",
        "      # Evaluate the cost and accurancy during the traning process\n",
        "      trainCost, trainAccurancy = sess.run([cost, accurancy], feed_dict={x: xTrainBatch, y: yTrainBatch})\n",
        "      \n",
        "      # Evaluate the cost and accurancy during the traning process\n",
        "      testCost, testAccurancy = sess.run([cost, accurancy], feed_dict={x: xTestBatch, y: yTestBatch})\n",
        "      \n",
        "      print('\\nIteration: ', iteration, 'trainCost: ', trainCost, 'trainAccurancy: ', trainAccurancy, 'testCost: ', testCost, 'testAccurancy: ', testAccurancy )\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhE63r2cpt_X",
        "colab_type": "text"
      },
      "source": [
        "## Activation visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-W61HznIpxtk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Libraries import\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# ------------------------- Function definition -------------------------\n",
        "\n",
        "# Conv2D with bias and relu activation\n",
        "def conv2d(x, W, b, strides=1):\n",
        "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
        "    x = tf.nn.bias_add(x, b)\n",
        "    # Modification: return the vector before the activation, just after conv\n",
        "    return x\n",
        "\n",
        "# MaxPool2D\n",
        "def maxpool2d(x, k=2):\n",
        "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n",
        "  \n",
        "# Model\n",
        "def conv_net(x, weights, biases):\n",
        "  \n",
        "    # Reshape input picture\n",
        "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
        "\n",
        "    # Convolution Layer\n",
        "    conv1PreAct = conv2d(x, weights['wc1'], biases['bc1'])\n",
        "    conv1PostAct =  tf.nn.relu(conv1PreAct)\n",
        "    \n",
        "    # Max Pooling (down-sampling)\n",
        "    conv1Max = maxpool2d(conv1PostAct, k=2)\n",
        "\n",
        "    # Convolution Layer\n",
        "    conv2PreAct = conv2d(conv1Max, weights['wc2'], biases['bc2'])\n",
        "    conv2PostAct =  tf.nn.relu(conv2PreAct)\n",
        "    \n",
        "    # Max Pooling (down-sampling)\n",
        "    conv2Max = maxpool2d(conv2PostAct, k=2)\n",
        "\n",
        "    # Flatten reshaping\n",
        "    fc1 = tf.reshape(conv2Max, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
        "    \n",
        "    # Fully connected input layer\n",
        "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
        "    fc1 = tf.nn.relu(fc1)\n",
        "    \n",
        "    # Apply Dropout\n",
        "    #fc1 = tf.nn.dropout(fc1, dropout)\n",
        "\n",
        "    # Fully connected output layer --> class prediction\n",
        "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
        "    \n",
        "    # Modification: return all the tensor for visualization\n",
        "    listOfTensor = [conv1PreAct, conv1PostAct, conv1Max, conv2PreAct, conv2PostAct, conv2Max, fc1]\n",
        "    \n",
        "    return out, listOfTensor\n",
        "  \n",
        "# -----------------------------------------------------------------------\n",
        "\n",
        "# Import MNIST data input (28x28 image --> mnist.train.images.shape)\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
        "\n",
        "#import imshow\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Extract the traning, test and validation data and labels\n",
        "xTrain = mnist.train.images\n",
        "yTrain = mnist.train.labels\n",
        "xTest = mnist.test.images\n",
        "yTest = mnist.test.labels\n",
        "xValidation = mnist.validation.images\n",
        "yValidation = mnist.validation.labels\n",
        "\n",
        "# Parameters\n",
        "learningRate = 0.001\n",
        "trainingIterations = 2000\n",
        "batchSize = 128\n",
        "#dropout = 0.75\n",
        "\n",
        "# Network Parameters\n",
        "nInputs =  mnist.train.images[0].size\n",
        "nOutputs = mnist.train.labels[0].size\n",
        "\n",
        "# TensorFlow Graph input/output placeholders\n",
        "x = tf.placeholder(tf.float32, [None, nInputs])\n",
        "y = tf.placeholder(tf.float32, [None, nOutputs])\n",
        "\n",
        "# Weights\n",
        "weights = {\n",
        "    \n",
        "    # 5x5 conv kernel for conv layer 1, 1 input, 32 output images\n",
        "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
        "    \n",
        "    # 5x5 conv kernel for conv layer 2, 32 inputs, 64 output images\n",
        "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
        "    \n",
        "    # Input fully connected layer, 7*7*64 inputs, 1024 output images\n",
        "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
        "    \n",
        "    # Output fully connected layer 1024 inputs, 10 output images\n",
        "    'out': tf.Variable(tf.random_normal([1024, nOutputs]))\n",
        "}\n",
        "\n",
        "# Biases\n",
        "biases = {\n",
        "    'bc1': tf.Variable(tf.random_normal([32])),\n",
        "    'bc2': tf.Variable(tf.random_normal([64])),\n",
        "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
        "    'out': tf.Variable(tf.random_normal([nOutputs]))\n",
        "}\n",
        "\n",
        "# Model\n",
        "Y, tensorList = conv_net(x, weights, biases)\n",
        "\n",
        "# Model Evaluation (tf.equal compare the 2 matricies and returns another matrix\n",
        "# with every values equal to true if the 2 value in the matricies are the same\n",
        "# false otherwise)\n",
        "nCorrectPrediction = tf.equal(tf.argmax(Y,1), tf.argmax(y,1))\n",
        "\n",
        "# Cost\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Y, labels=y))\n",
        "\n",
        "# Accurancy\n",
        "accurancy = tf.reduce_mean(tf.cast(nCorrectPrediction, tf.float32))\n",
        "\n",
        "# Optimizer\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learningRate).minimize(cost)\n",
        "\n",
        "# Initalizing the graph and the weights\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Launch the graph\n",
        "with tf.Session() as sess:\n",
        "  \n",
        "    # Session initialization\n",
        "    sess.run(init)\n",
        "    \n",
        "    for iteration in range(trainingIterations):\n",
        "      \n",
        "      # Take the samples from the dataset of the specified batchSize\n",
        "      xTrainBatch, yTrainBatch = mnist.train.next_batch(batchSize)\n",
        "      \n",
        "      # Take the samples from the dataset of the specified batchSize\n",
        "      xTestBatch, yTestBatch = mnist.test.next_batch(batchSize)\n",
        "      \n",
        "      # First run the optimizer (one iteration of the gradient descent)\n",
        "      # the feed_dict is a dictionary of the data that has to be unsed\n",
        "      # insteadof the placeholders\n",
        "      sess.run(optimizer, feed_dict={x: xTrainBatch, y: yTrainBatch}) \n",
        "\n",
        "      # Evaluate the cost and accurancy during the traning process\n",
        "      trainCost, trainAccurancy = sess.run([cost, accurancy], feed_dict={x: xTrainBatch, y: yTrainBatch})\n",
        "      \n",
        "      # Evaluate the cost and accurancy during the traning process\n",
        "      testCost, testAccurancy = sess.run([cost, accurancy], feed_dict={x: xTestBatch, y: yTestBatch})\n",
        "      \n",
        "      # Show the result every 100 iterations\n",
        "      if iteration % 100 == 0:\n",
        "        \n",
        "        # Evaluate the list of tensor\n",
        "        valTensorList = sess.run(tensorList, feed_dict={x: xTestBatch})\n",
        "        \n",
        "        # Number of tensors\n",
        "        nTensors = len(valTensorList)\n",
        "        \n",
        "        # Show tensors (take the first filter and all the image --> [0,:,:,0])      \n",
        "        plt.imshow(valTensorList[0][0,:,:,0], cmap='gray') #conv1 pre act\n",
        "        plt.show()\n",
        "        plt.imshow(valTensorList[1][0,:,:,0], cmap='gray') #conv1 post act\n",
        "        plt.show()\n",
        "        plt.imshow(valTensorList[2][0,:,:,0], cmap='gray') #conv1 post maxpolling\n",
        "        plt.show()\n",
        "        plt.imshow(valTensorList[3][0,:,:,0], cmap='gray') #conv2 pre act\n",
        "        plt.show()\n",
        "        plt.imshow(valTensorList[4][0,:,:,0], cmap='gray') #conv2 post act\n",
        "        plt.show()\n",
        "        plt.imshow(valTensorList[5][0,:,:,0], cmap='gray') #conv2 post maxpolling\n",
        "        plt.show()\n",
        "        plt.imshow(valTensorList[6], cmap='gray') #flatten after activation\n",
        "        plt.show()\n",
        "        \n",
        "        # Print results\n",
        "        print('\\nIteration: ', iteration, 'trainCost: ', trainCost, 'trainAccurancy: ', trainAccurancy, 'testCost: ', testCost, 'testAccurancy: ', testAccurancy )\n",
        "        \n",
        "\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iL1lFc7mz8ZP",
        "colab_type": "text"
      },
      "source": [
        "## TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pD888rDp3NeQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__Kj7muC3vpt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf log"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqSYDFt33vsQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LOG_DIR = './log'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9H1o1UB3Nq6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_ipython().system_raw('./ngrok http 6006 &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXW0Pg-X31_p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "da1dffdd-bd6f-41c9-a1c7-20bb5e6cb85d"
      },
      "source": [
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://f2bc619f.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeMLBCNyz-bJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Libraries import\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# ------------------------- Function definition -------------------------\n",
        "\n",
        "# Conv2D with bias and relu activation\n",
        "def conv2d(x, W, b, strides=1):\n",
        "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
        "    x = tf.nn.bias_add(x, b)\n",
        "    # Modification: return the vector before the activation, just after conv\n",
        "    return x\n",
        "\n",
        "# MaxPool2D\n",
        "def maxpool2d(x, k=2):\n",
        "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n",
        "  \n",
        "# Model\n",
        "def conv_net(x, weights, biases):\n",
        "  \n",
        "    # Reshape input picture\n",
        "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
        "    \n",
        "    with tf.name_scope('conv1'):\n",
        "      \n",
        "      # Convolution Layer\n",
        "      conv1PreAct = conv2d(x, weights['wc1'], biases['bc1'])\n",
        "      conv1PostAct =  tf.nn.relu(conv1PreAct)\n",
        "    \n",
        "    with tf.name_scope('maxPolling1'):\n",
        "      \n",
        "      # Max Pooling (down-sampling)\n",
        "      conv1Max = maxpool2d(conv1PostAct, k=2)\n",
        "\n",
        "    with tf.name_scope('conv2'):\n",
        "      \n",
        "      # Convolution Layer\n",
        "      conv2PreAct = conv2d(conv1Max, weights['wc2'], biases['bc2'])\n",
        "      conv2PostAct =  tf.nn.relu(conv2PreAct)\n",
        "    \n",
        "    with tf.name_scope('maxPolling2'):\n",
        "      \n",
        "      # Max Pooling (down-sampling)\n",
        "      conv2Max = maxpool2d(conv2PostAct, k=2)\n",
        "\n",
        "    with tf.name_scope('flatten'):\n",
        "      \n",
        "      # Flatten reshaping\n",
        "      fc1 = tf.reshape(conv2Max, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
        "    \n",
        "    with tf.name_scope('output'):\n",
        "      \n",
        "      # Fully connected input layer\n",
        "      fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
        "      fc1 = tf.nn.relu(fc1)\n",
        "    \n",
        "    # Apply Dropout\n",
        "    #fc1 = tf.nn.dropout(fc1, dropout)\n",
        "\n",
        "    # Fully connected output layer --> class prediction\n",
        "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
        "    \n",
        "    # Modification: return all the tensor for visualization\n",
        "    listOfTensor = [conv1PreAct, conv1PostAct, conv1Max, conv2PreAct, conv2PostAct, conv2Max, fc1]\n",
        "    \n",
        "    return out, listOfTensor\n",
        "  \n",
        "# -----------------------------------------------------------------------\n",
        "\n",
        "# Import MNIST data input (28x28 image --> mnist.train.images.shape)\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
        "\n",
        "#import imshow\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Extract the traning, test and validation data and labels\n",
        "xTrain = mnist.train.images\n",
        "yTrain = mnist.train.labels\n",
        "xTest = mnist.test.images\n",
        "yTest = mnist.test.labels\n",
        "xValidation = mnist.validation.images\n",
        "yValidation = mnist.validation.labels\n",
        "\n",
        "# Parameters\n",
        "learningRate = 0.001\n",
        "trainingIterations = 2000\n",
        "batchSize = 128\n",
        "#dropout = 0.75\n",
        "\n",
        "# Network Parameters\n",
        "nInputs =  mnist.train.images[0].size\n",
        "nOutputs = mnist.train.labels[0].size\n",
        "\n",
        "# TensorFlow Graph input/output placeholders\n",
        "x = tf.placeholder(tf.float32, [None, nInputs])\n",
        "y = tf.placeholder(tf.float32, [None, nOutputs])\n",
        "\n",
        "# Weights\n",
        "weights = {\n",
        "    \n",
        "    # 5x5 conv kernel for conv layer 1, 1 input, 32 output images\n",
        "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
        "    \n",
        "    # 5x5 conv kernel for conv layer 2, 32 inputs, 64 output images\n",
        "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
        "    \n",
        "    # Input fully connected layer, 7*7*64 inputs, 1024 output images\n",
        "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
        "    \n",
        "    # Output fully connected layer 1024 inputs, 10 output images\n",
        "    'out': tf.Variable(tf.random_normal([1024, nOutputs]))\n",
        "}\n",
        "\n",
        "# Biases\n",
        "biases = {\n",
        "    'bc1': tf.Variable(tf.random_normal([32])),\n",
        "    'bc2': tf.Variable(tf.random_normal([64])),\n",
        "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
        "    'out': tf.Variable(tf.random_normal([nOutputs]))\n",
        "}\n",
        "\n",
        "# Model\n",
        "Y, tensorList = conv_net(x, weights, biases)\n",
        "\n",
        "with tf.name_scope('cost'):\n",
        "  \n",
        "  # Cost\n",
        "  cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Y, labels=y))\n",
        "\n",
        "with tf.name_scope('operations'):\n",
        "  \n",
        "  # Model Evaluation (tf.equal compare the 2 matricies and returns another matrix\n",
        "  # with every values equal to true if the 2 value in the matricies are the same\n",
        "  # false otherwise)\n",
        "  nCorrectPrediction = tf.equal(tf.argmax(Y,1), tf.argmax(y,1))\n",
        "  \n",
        "  # Accurancy\n",
        "  accurancy = tf.reduce_mean(tf.cast(nCorrectPrediction, tf.float32))\n",
        "  \n",
        "# Accurancy summary\n",
        "accurancySummary = tf.summary.scalar('accurancy', accurancy)\n",
        "\n",
        "\n",
        "with tf.name_scope('optimization'):\n",
        "  \n",
        "  # Optimizer\n",
        "  optimizer = tf.train.AdamOptimizer(learning_rate=learningRate).minimize(cost)\n",
        "\n",
        "# Initalizing the graph and the weights\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Launch the graph\n",
        "with tf.Session() as sess:\n",
        "  \n",
        "  # Define files where the summary will be written\n",
        "  trainWriter = tf.summary.FileWriter('./log/train', sess.graph)\n",
        "  testWriter = tf.summary.FileWriter('./log/test')\n",
        "  \n",
        "  # Session initialization\n",
        "  sess.run(init)\n",
        "    \n",
        "  for iteration in range(trainingIterations):\n",
        "      \n",
        "    # Take the samples from the dataset of the specified batchSize\n",
        "    xTrainBatch, yTrainBatch = mnist.train.next_batch(batchSize)\n",
        "      \n",
        "    # Take the samples from the dataset of the specified batchSize\n",
        "    xTestBatch, yTestBatch = mnist.test.next_batch(batchSize)\n",
        "      \n",
        "    # First run the optimizer (one iteration of the gradient descent)\n",
        "    # the feed_dict is a dictionary of the data that has to be unsed\n",
        "    # insteadof the placeholders\n",
        "    sess.run(optimizer, feed_dict={x: xTrainBatch, y: yTrainBatch}) \n",
        "\n",
        "    # Evaluate the cost and accurancy during the traning process\n",
        "    summary, trainCost, trainAccurancy = sess.run([accurancySummary, cost, accurancy], feed_dict={x: xTrainBatch, y: yTrainBatch})\n",
        "    trainWriter.add_summary(summary, iteration)\n",
        "      \n",
        "    # Evaluate the cost and accurancy during the traning process\n",
        "    summary, testCost, testAccurancy = sess.run([accurancySummary, cost, accurancy], feed_dict={x: xTestBatch, y: yTestBatch})\n",
        "    testWriter.add_summary(summary, iteration)\n",
        "      \n",
        "    # Show the result every 100 iterations\n",
        "    if iteration % 100 == 0:\n",
        "        \n",
        "      # Evaluate the list of tensor\n",
        "      valTensorList = sess.run(tensorList, feed_dict={x: xTestBatch})\n",
        "        \n",
        "      # Number of tensors\n",
        "      nTensors = len(valTensorList)\n",
        "        \n",
        "      # Show tensors (take the first filter and all the image --> [0,:,:,0])      \n",
        "      plt.imshow(valTensorList[0][0,:,:,0], cmap='gray') #conv1 pre act\n",
        "      plt.show()\n",
        "      plt.imshow(valTensorList[1][0,:,:,0], cmap='gray') #conv1 post act\n",
        "      plt.show()\n",
        "      plt.imshow(valTensorList[2][0,:,:,0], cmap='gray') #conv1 post maxpolling\n",
        "      plt.show()\n",
        "      plt.imshow(valTensorList[3][0,:,:,0], cmap='gray') #conv2 pre act\n",
        "      plt.show()\n",
        "      plt.imshow(valTensorList[4][0,:,:,0], cmap='gray') #conv2 post act\n",
        "      plt.show()\n",
        "      plt.imshow(valTensorList[5][0,:,:,0], cmap='gray') #conv2 post maxpolling\n",
        "      plt.show()\n",
        "      plt.imshow(valTensorList[6], cmap='gray') #flatten after activation\n",
        "      plt.show()\n",
        "        \n",
        "      # Print results\n",
        "      print('\\nIteration: ', iteration, 'trainCost: ', trainCost, 'trainAccurancy: ', trainAccurancy, 'testCost: ', testCost, 'testAccurancy: ', testAccurancy )\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}