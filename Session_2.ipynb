{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Session 2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahmadhajmosa/Machine-learning-labs/blob/ebrandner/Session_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmSUExzC-ZBV",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgWZldyi-Zef",
        "colab_type": "text"
      },
      "source": [
        "# Lab on Machine Learning and Applications in Intelligent Vehicles\n",
        "## Session 1: Introduction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a75NLOuwKXSs",
        "colab_type": "text"
      },
      "source": [
        "#Session 2: 05.06 - 13:00 - 14:30 :\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciUtGM-W_ehP",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "## Intro:\n",
        "\n",
        "Tensorflow is a powerful framework for implementing and deploying large-scale deep learning models. Recently, it has been widely used in both reasearch and production. TF objective is to combine scale and flexibility.\n",
        "\n",
        "In the past session, we will learning the following:\n",
        "\n",
        "1. TF programming stack\n",
        "2. TF programming concepts including computatoin graphs, operations and sessions. \n",
        "3. Implementation of linear regression\n",
        "4. Implementation of feed-forward neural networks\n",
        "\n",
        "## TF stack:\n",
        "\n",
        "TensorFlow is a framework composed of two core building blocks — a library for defining computational graphs and a runtime for executing such graphs on a variety of different hardware\n",
        "\n",
        "\n",
        "![alt text](https://www.tensorflow.org/images/layers.png)\n",
        "\n",
        "\n",
        "Before goining into details about the stack, let us talk about computational graphs.\n",
        "\n",
        "### Computational Graphs\n",
        "\n",
        "A directed graph is a data structure consisting of nodes (vertices) and edges. It’s a set of vertices connected pairwise by directed edges.\n",
        "\n",
        "Graphs come in many shapes and sizes and are used to solve many real-life problems, such as representing networks including telephone networks, circuit networks, road networks, and even social networks. \n",
        "![alt text](https://cdn-images-1.medium.com/max/800/1*V6aYjD3AxDbEKYahkGqVQw.png)\n",
        "\n",
        "TensorFlow uses directed graphs internally to represent computations, and they call this data flow graphs (or computational graphs).\n",
        "\n",
        "The nodes in TF data flow graph mostly represents operations, variables and placeholders.\n",
        "\n",
        "Take for example the following operation:\n",
        "![alt text](https://cdn-images-1.medium.com/max/800/1*6E3sfit6DCeJ9mOz17g4bA.png)\n",
        "\n",
        "To create a computational graph out of this program, we create nodes for each of the operations in our program, along with the input variables a and b. In fact, a and b could be constants if they don’t change. If one node is used as the input to another operation we draw a directed arrow that goes from one node to another.\n",
        "\n",
        "The computational graph for this program might look like this:\n",
        "![alt text](https://cdn-images-1.medium.com/max/800/1*vPb9E0Yd1QUAD0oFmAgaOw.png)\n",
        "\n",
        "Operations create or manipulate data according to specific rules. In TensorFlow those rules are called Ops, short for operations. Variables on the other hand represent shared, persistent state that can be manipulated by running Ops on those variables.\n",
        "\n",
        "The questions now what are the advantages of representing operations as directed graphs: The main advantage of using directed graphs is the ability to do **parallelism** and what is called **dependency driving scheduling**. \n",
        "For example, consider again the follwoing code:\n",
        "![alt text](https://cdn-images-1.medium.com/max/800/1*6E3sfit6DCeJ9mOz17g4bA.png)\n",
        "At the most fundamental level, most computer programs are mainly composed of two things — primitive operations and an order in which these operations are executed, often sequentially, line by line. This means we would first multiply a and b and only when this expression was evaluated we would take their sum. Computational graphs on the otherhand, exclusively specify the dependencies across the operations.\n",
        "If we look at our computational graph we see that we could execute the multiplication and addition in parallel. That’s because these two operations do not depend on each other.\n",
        " So we can use the topology of the graph to drive the scheduling of operations and execute them in the most efficient manner, e.g. using multiple GPUs on a single machine or even distribute the execution across multiple machines.\n",
        " Another key advantage is portability. The graph is a language-independent representation of our code. So we can build the graph in Python, save the model (TensorFlow uses protocol buffers), and restore the model in a different language, say C++, if you want to go really fast.\n",
        " \n",
        " \n",
        "\n",
        "--------------------------------\n",
        "# References:\n",
        "\n",
        "https://medium.com/@d3lm/understand-tensorflow-by-mimicking-its-api-from-scratch-faa55787170d\n",
        "\n",
        "https://www.tensorflow.org/guide/extend/architecture\n",
        "\n",
        "https://www.tensorflow.org/guide/low_level_intro\n",
        "\n",
        "  \n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-GFJPVDnEwx",
        "colab_type": "text"
      },
      "source": [
        "# placeholder: tensors are feeded externaly for example inputs tensors + output tensors\n",
        "\n",
        "# variables : tensors represent the parameters of the network/graph ie. nn weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlmSCbhtoJBs",
        "colab_type": "code",
        "outputId": "5c48ce27-eecc-4cea-aa52-ef342a3f3809",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.001\n",
        "training_iters = 2000\n",
        "batch_size = 128\n",
        "\n",
        "# Network Parameters\n",
        "\n",
        "num_inputs = 3\n",
        "num_outputs = 4\n",
        "num_samples= 10\n",
        "# Training data\n",
        "x_gr = np.random.rand(num_samples,num_inputs)\n",
        "y_gr = np.random.rand(num_samples,num_outputs)\n",
        "\n",
        "\n",
        "# tf Graph input\n",
        "x = tf.placeholder(tf.float32, [None, num_inputs])\n",
        "y = tf.placeholder(tf.float32, [None, num_outputs])\n",
        "\n",
        "\n",
        "# weights \n",
        "w_1 = tf.Variable(tf.random_normal([num_inputs,num_outputs ]))\n",
        "\n",
        "# model\n",
        "y_p = tf.matmul(x, w_1)\n",
        "\n",
        "\n",
        "# cost\n",
        "\n",
        "cost = tf.reduce_mean(tf.pow(y-y_p,2)) # \n",
        "\n",
        "# optimisation \n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# initalizing the graph and the weights\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Launch the graph\n",
        "with tf.Session() as sess:\n",
        "    \n",
        "    sess.run(init)\n",
        "    \n",
        "    for i in range(10):\n",
        "    \n",
        "        sess.run(optimizer, feed_dict={x: x_gr, y: y_gr}) \n",
        "\n",
        "        pr_cost = sess.run(cost, feed_dict={x: x_gr,y: y_gr})\n",
        "    \n",
        "        print('iter: ',i, 'cost: ', pr_cost)\n",
        "    \n",
        "    y_p_p = sess.run(y_p, feed_dict={x: x_gr, y: y_gr})\n",
        "    \n",
        "    print('predicted ', y_p_p)\n",
        "    print('real ', y_gr)\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0726 17:27:49.395052 140322728843136 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iter:  0 cost:  2.6251776\n",
            "iter:  1 cost:  2.6206334\n",
            "iter:  2 cost:  2.6160946\n",
            "iter:  3 cost:  2.6115618\n",
            "iter:  4 cost:  2.607035\n",
            "iter:  5 cost:  2.6025138\n",
            "iter:  6 cost:  2.597999\n",
            "iter:  7 cost:  2.5934906\n",
            "iter:  8 cost:  2.5889883\n",
            "iter:  9 cost:  2.5844924\n",
            "predicted  [[ 0.35148603 -1.9723994  -0.49707454 -1.5141728 ]\n",
            " [-0.7691761  -0.21237716  0.16914815 -1.0812541 ]\n",
            " [-0.6369749  -1.2598519  -0.6585672  -1.9684923 ]\n",
            " [-0.558677   -0.05562422  0.31569484 -0.6833181 ]\n",
            " [-0.9574977  -0.616953   -0.23733196 -1.7018905 ]\n",
            " [-0.33277634 -2.3779585  -0.93256    -2.706089  ]\n",
            " [-0.41960195 -1.3745198  -0.3222732  -1.8171179 ]\n",
            " [ 0.7563244  -2.123018   -0.93904924 -1.2106719 ]\n",
            " [-0.12352562 -0.528494    0.2179209  -0.6427809 ]\n",
            " [ 0.31966454 -2.2267163  -1.0501024  -1.8158199 ]]\n",
            "real  [[0.82842122 0.03553232 0.11894606 0.93495715]\n",
            " [0.51515637 0.44912359 0.1267139  0.60677052]\n",
            " [0.13615802 0.56200759 0.49221935 0.43973747]\n",
            " [0.6151325  0.15462524 0.59546929 0.32207361]\n",
            " [0.76713646 0.10484171 0.83366571 0.53539064]\n",
            " [0.80082027 0.44647602 0.51550874 0.09378216]\n",
            " [0.53954572 0.03046007 0.46208914 0.81671614]\n",
            " [0.02128246 0.26296001 0.22451513 0.86306776]\n",
            " [0.49749244 0.46554621 0.57992201 0.94749599]\n",
            " [0.04346377 0.89075852 0.5923579  0.74267833]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fv2aqi3Fu-AJ",
        "colab_type": "code",
        "outputId": "300ef36f-6bb0-4833-cbca-e80edeb0d515",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "sess = tf.Session() \n",
        "sess.run(init)\n",
        "    \n",
        "for i in range(10):\n",
        "    \n",
        "        sess.run(optimizer, feed_dict={x: x_gr, y: y_gr}) \n",
        "\n",
        "        pr_cost = sess.run(cost, feed_dict={x: x_gr,y: y_gr})\n",
        "    \n",
        "        print('iter: ',i, 'cost: ', pr_cost)\n",
        "    \n",
        "y_p_p = sess.run(y_p, feed_dict={x: x_gr, y: y_gr})\n",
        "    \n",
        "print('predicted ', y_p_p)\n",
        "print('real ', y_gr)\n",
        "\n",
        "#sess.close()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter:  0 cost:  1.7605293\n",
            "iter:  1 cost:  1.7572906\n",
            "iter:  2 cost:  1.7540576\n",
            "iter:  3 cost:  1.7508309\n",
            "iter:  4 cost:  1.7476097\n",
            "iter:  5 cost:  1.7443945\n",
            "iter:  6 cost:  1.7411855\n",
            "iter:  7 cost:  1.7379825\n",
            "iter:  8 cost:  1.734786\n",
            "iter:  9 cost:  1.7315954\n",
            "predicted  [[-1.5174351   1.6644232   0.5289003  -1.4559686 ]\n",
            " [ 0.08431488  0.76299876  0.45737678 -1.4261276 ]\n",
            " [ 0.8469092   0.5570124  -0.04539706 -1.8491459 ]\n",
            " [-0.3316527   0.7954763   0.5391224  -1.0655446 ]\n",
            " [ 0.86210066  0.55838895  0.16788726 -1.8509395 ]\n",
            " [-0.1440177   1.4963131   0.2538183  -2.5370982 ]\n",
            " [-0.42657202  1.3299171   0.47527432 -1.9399707 ]\n",
            " [-0.9457425   0.9332629  -0.03808715 -0.7468864 ]\n",
            " [-1.1200864   1.171346    0.6574003  -0.9517237 ]\n",
            " [-0.32144517  0.9392726  -0.07929882 -1.380792  ]]\n",
            "real  [[0.82842122 0.03553232 0.11894606 0.93495715]\n",
            " [0.51515637 0.44912359 0.1267139  0.60677052]\n",
            " [0.13615802 0.56200759 0.49221935 0.43973747]\n",
            " [0.6151325  0.15462524 0.59546929 0.32207361]\n",
            " [0.76713646 0.10484171 0.83366571 0.53539064]\n",
            " [0.80082027 0.44647602 0.51550874 0.09378216]\n",
            " [0.53954572 0.03046007 0.46208914 0.81671614]\n",
            " [0.02128246 0.26296001 0.22451513 0.86306776]\n",
            " [0.49749244 0.46554621 0.57992201 0.94749599]\n",
            " [0.04346377 0.89075852 0.5923579  0.74267833]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUPSS03avw5D",
        "colab_type": "code",
        "outputId": "ce43856b-591a-4383-c98b-b4d11d63fdb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.001\n",
        "training_iters = 2000\n",
        "batch_size = 128\n",
        "\n",
        "# Network Parameters\n",
        "\n",
        "num_inputs = 3\n",
        "num_h1_n = 4\n",
        "num_h2_n = 10\n",
        "num_outputs = 4\n",
        "\n",
        "num_samples= 10\n",
        "\n",
        "# Training data\n",
        "x_gr = np.random.rand(num_samples,num_inputs)\n",
        "y_gr = np.random.rand(num_samples,num_outputs)\n",
        "\n",
        "\n",
        "# tf Graph input\n",
        "x = tf.placeholder(tf.float32, [None, num_inputs])\n",
        "y = tf.placeholder(tf.float32, [None, num_outputs])\n",
        "\n",
        "\n",
        "# weights \n",
        "w_1 = tf.Variable(tf.random_normal([num_inputs,num_h1_n ]))\n",
        "w_2 = tf.Variable(tf.random_normal([num_h1_n,num_h2_n ]))\n",
        "w_3 = tf.Variable(tf.random_normal([num_h2_n,num_outputs ]))\n",
        "\n",
        "# bias \n",
        "b_1 = tf.Variable(tf.random_normal([num_h1_n]))\n",
        "b_2 = tf.Variable(tf.random_normal([num_h2_n]))\n",
        "b_3 = tf.Variable(tf.random_normal([num_outputs]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# model\n",
        "\n",
        "h1 = tf.nn.sigmoid(tf.add(tf.matmul(x, w_1),b_1)) # model of hidden layer 1\n",
        "h2 = tf.nn.sigmoid(tf.add(tf.matmul(h1, w_2),b_2)) # model of hidden layer 2\n",
        "y_p = tf.add(tf.matmul(h2, w_3),b_3) # model of the output layer\n",
        "\n",
        "\n",
        "\n",
        "# cost\n",
        "\n",
        "cost = tf.reduce_mean(tf.pow(y-y_p,2)) # \n",
        "\n",
        "# optimisation \n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# initalizing the graph and the weights\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Launch the graph\n",
        "with tf.Session() as sess:\n",
        "    \n",
        "    sess.run(init)\n",
        "    \n",
        "    for i in range(10):\n",
        "    \n",
        "        sess.run(optimizer, feed_dict={x: x_gr, y: y_gr}) \n",
        "\n",
        "        pr_cost = sess.run(cost, feed_dict={x: x_gr,y: y_gr})\n",
        "    \n",
        "        print('iter: ',i, 'cost: ', pr_cost)\n",
        "    \n",
        "    y_p_p = sess.run(y_p, feed_dict={x: x_gr, y: y_gr})\n",
        "    \n",
        "    print('predicted ', y_p_p)\n",
        "    print('real ', y_gr)\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter:  0 cost:  3.2981598\n",
            "iter:  1 cost:  3.272094\n",
            "iter:  2 cost:  3.2461922\n",
            "iter:  3 cost:  3.220456\n",
            "iter:  4 cost:  3.19489\n",
            "iter:  5 cost:  3.169495\n",
            "iter:  6 cost:  3.1442733\n",
            "iter:  7 cost:  3.119227\n",
            "iter:  8 cost:  3.094358\n",
            "iter:  9 cost:  3.069667\n",
            "predicted  [[ 0.7632036  -2.245531    0.46305412 -1.1990601 ]\n",
            " [ 0.66806424 -2.2697988   0.60253644 -1.3658413 ]\n",
            " [ 0.70582604 -2.1666927   0.66242075 -1.3949344 ]\n",
            " [ 0.7241087  -2.3061597   0.57717776 -1.3933644 ]\n",
            " [ 0.8035642  -2.1342535   0.5749593  -1.2716101 ]\n",
            " [ 0.63729846 -2.2384052   0.549793   -1.2492473 ]\n",
            " [ 0.65664804 -2.300055    0.4730385  -1.2098329 ]\n",
            " [ 0.6878375  -2.3213      0.5591388  -1.3556211 ]\n",
            " [ 0.7031281  -2.258948    0.49643356 -1.2336907 ]\n",
            " [ 0.67415845 -2.3418121   0.50732493 -1.290762  ]]\n",
            "real  [[0.34468958 0.59452299 0.10506059 0.38600112]\n",
            " [0.41804311 0.74340296 0.98946272 0.07529672]\n",
            " [0.81281543 0.21513215 0.87435438 0.58145273]\n",
            " [0.18237813 0.77340011 0.36538029 0.92913497]\n",
            " [0.13295164 0.57126419 0.6810241  0.87259032]\n",
            " [0.04875729 0.51191084 0.71068541 0.21620727]\n",
            " [0.80661147 0.5214222  0.77780102 0.31682608]\n",
            " [0.87003065 0.89827082 0.80753898 0.68753404]\n",
            " [0.12465374 0.6157765  0.90840605 0.68577186]\n",
            " [0.72058042 0.94216212 0.53285435 0.93200345]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwhCMk9VyVj-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaeRp0T10834",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training data\n",
        "X_train = mnist.train.images\n",
        "Y_train = mnist.train.labels\n",
        "\n",
        "# training data\n",
        "X_test = mnist.test.images\n",
        "Y_test = mnist.test.labels\n",
        "\n",
        "# training data\n",
        "X_val = mnist.validation.images\n",
        "Y_val = mnist.validation.labels\n",
        "\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.001\n",
        "training_iters = 2000\n",
        "batch_size = 128\n",
        "\n",
        "# Network Parameters\n",
        "\n",
        "num_inputs = 784\n",
        "num_h1_n = 100\n",
        "num_h2_n = 100\n",
        "num_outputs = 10\n",
        "\n",
        "\n",
        "\n",
        "# tf Graph input\n",
        "x = tf.placeholder(tf.float32, [None, num_inputs])\n",
        "y = tf.placeholder(tf.float32, [None, num_outputs])\n",
        "\n",
        "\n",
        "# weights \n",
        "w_1 = tf.Variable(tf.random_normal([num_inputs,num_h1_n ]))\n",
        "w_2 = tf.Variable(tf.random_normal([num_h1_n,num_h2_n ]))\n",
        "w_3 = tf.Variable(tf.random_normal([num_h2_n,num_outputs ]))\n",
        "\n",
        "# bias \n",
        "b_1 = tf.Variable(tf.random_normal([num_h1_n]))\n",
        "b_2 = tf.Variable(tf.random_normal([num_h2_n]))\n",
        "b_3 = tf.Variable(tf.random_normal([num_outputs]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# model\n",
        "\n",
        "h1 = tf.nn.sigmoid(tf.add(tf.matmul(x, w_1),b_1)) # model of hidden layer 1\n",
        "h2 = tf.nn.sigmoid(tf.add(tf.matmul(h1, w_2),b_2)) # model of hidden layer 2\n",
        "y_p = tf.add(tf.matmul(h2, w_3),b_3) # model of the output layer\n",
        "\n",
        "\n",
        "\n",
        "# cost\n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_p, labels=y)) # cross entropy cost\n",
        "\n",
        "\n",
        "# Evaluate model\n",
        "correct_pred = tf.equal(tf.argmax(y_p, 1), tf.argmax(y, 1))\n",
        "\n",
        "## 3 images, y_p=[[0.1,0.0,0,0.9],[0.9,0.1,0,0.],[0,0.9,0,0.1]] \n",
        "\n",
        "# tf.argmax(y_p, 1) [3,0,1] \n",
        "\n",
        "# 3 images, y=[[0,0.0,0,1],[0,1,0,0],[0,1,0,0]] \n",
        "\n",
        "# tf.argmax(y, 1) [3,1,1]\n",
        "\n",
        "# tf.equal [True,False,True]--[1,0,1]--- 2/3 \n",
        "\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "#\n",
        "\n",
        "# optimisation \n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# initalizing the graph and the weights\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Launch the graph\n",
        "with tf.Session() as sess:\n",
        "    \n",
        "    sess.run(init)\n",
        "    \n",
        "    for i in range(1000):\n",
        "        \n",
        "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
        "        \n",
        "        # Run optimization op (backprop)\n",
        "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
        "    \n",
        "\n",
        "        train_cost, train_acc  = sess.run([cost,accuracy], feed_dict={x: batch_x,y: batch_y})\n",
        "    \n",
        "        \n",
        "        test_batch_x, test_batch_y = mnist.test.next_batch(batch_size)\n",
        "\n",
        "        test_cost, test_acc  = sess.run([cost,accuracy], feed_dict={x: test_batch_x,y: test_batch_y})\n",
        "        print('iter: ',i, 'train_cost: ', train_cost, 'train_acc: ', train_acc,'test_cost: ', test_cost, 'test_acc: ', test_acc )\n",
        "\n",
        "    \n",
        "    #y_p_p = sess.run(y_p, feed_dict={x: x_gr, y: y_gr})\n",
        "    \n",
        "    print('predicted ', y_p_p)\n",
        "    print('real ', y_gr)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDkPoIsB90MW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf \n",
        "\n",
        "\n",
        "def variable_summaries(var):\n",
        "  \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
        "  with tf.name_scope('summaries'):\n",
        "    mean = tf.reduce_mean(var)\n",
        "    tf.summary.scalar('mean', mean)\n",
        "    with tf.name_scope('stddev'):\n",
        "      stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
        "    tf.summary.scalar('stddev', stddev)\n",
        "    tf.summary.scalar('max', tf.reduce_max(var))\n",
        "    tf.summary.scalar('min', tf.reduce_min(var))\n",
        "    tf.summary.histogram('histogram', var)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRJsyYmvBE43",
        "colab_type": "code",
        "outputId": "935978b5-00b4-4d7f-c55a-4efef708e571",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "from matplotlib.pyplot import imshow\n",
        "\n",
        "# Import MNIST data\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
        "\n",
        "mnist.train.images.shape\n",
        "\n",
        "image =mnist.train.images[0].reshape((28,28))\n",
        "#MNIST data input (img shape: 28*28)\n",
        "imshow(image)\n",
        "\n",
        "mnist.train.labels[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
            "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCIrwA0ERfiAowg8ERfiBoAg/ENT/AyErW1pw/s8cAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXZtWbWQ92P_",
        "colab_type": "text"
      },
      "source": [
        "# CNN using Tensorflow\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6k9YPZQ95z5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "ede82976-dcfc-465c-c8c6-6799740f048d"
      },
      "source": [
        "n_classes=10\n",
        "learning_rate=0.002\n",
        "batch_size=64\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# Create some wrappers for simplicity\n",
        "def conv2d(x, W, b, strides=1):\n",
        "    # Conv2D wrapper, with bias and relu activation\n",
        "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
        "    x = tf.nn.bias_add(x, b)\n",
        "    return x\n",
        "\n",
        "\n",
        "num_inputs = 784\n",
        "num_outputs= 10\n",
        "\n",
        "# tf Graph input\n",
        "x = tf.placeholder(tf.float32, [None, num_inputs],name = 'input')\n",
        "y = tf.placeholder(tf.float32, [None, num_outputs],name =  'output')\n",
        "\n",
        "\n",
        "def maxpool2d(x, k=2):\n",
        "    # MaxPool2D wrapper\n",
        "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
        "                          padding='SAME')\n",
        "\n",
        "\n",
        "# Create model\n",
        "def conv_net(x, weights, biases):\n",
        "    # Reshape input picture\n",
        "    \n",
        "    with tf.name_scope('input-reshape'):\n",
        "        x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
        "\n",
        "    with tf.name_scope('conv-1'):\n",
        "\n",
        "        # Convolution Layer\n",
        "        conv_pre_actv_1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
        "        conv_actv_1 = tf.nn.relu(conv_pre_actv_1)\n",
        "        \n",
        "        print('conv_actv_1 max',conv_actv_1.get_shape().as_list())\n",
        "\n",
        "    with tf.name_scope('maxpooling-1'):\n",
        "        \n",
        "        # Max Pooling (down-sampling)\n",
        "        conv1 = maxpool2d(conv_actv_1, k=2)\n",
        "        print('con1_after max',conv1.get_shape().as_list())\n",
        "\n",
        "\n",
        "    # Convolution Layer\n",
        "    with tf.name_scope('conv-2'):\n",
        "\n",
        "        conv_pre_actv_2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
        "        conv_actv_2 = tf.nn.relu(conv_pre_actv_2)\n",
        "\n",
        "        \n",
        "        print('con2_before max',conv_actv_2.get_shape().as_list())\n",
        "\n",
        "\n",
        "    with tf.name_scope('maxpooling-2'):\n",
        "        \n",
        "     \n",
        "        # Max Pooling (down-sampling)\n",
        "        conv2 = maxpool2d(conv_actv_2, k=2)\n",
        "        print('con2_after max', conv2.get_shape().as_list())\n",
        "\n",
        "    # Fully connected layer\n",
        "    # Reshape conv2 output to fit fully connected layer input\n",
        "    #wd1 numx3x3  wd1.get_shape() -> numx9 \n",
        "    with tf.name_scope('flatten'):\n",
        "    \n",
        "        fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
        "        fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
        "        fc1 = tf.nn.relu(fc1)\n",
        "    # Apply Dropout\n",
        "    #fc1 = tf.nn.dropout(fc1, dropout)\n",
        "\n",
        "    # Output, class prediction\n",
        "    with tf.name_scope('output'):\n",
        "    \n",
        "        out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
        "        \n",
        "    list_of_tensors = [conv_pre_actv_1,conv_actv_1, conv1,conv_pre_actv_2,conv_actv_2, conv2, fc1]\n",
        "    return out, list_of_tensors\n",
        "\n",
        "\n",
        "# Store layers weight & bias\n",
        "weights = {\n",
        "    # 5x5 conv, 1 input, 32 outputs\n",
        "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32]), name=\"wc1\"),\n",
        "    # 5x5 conv, 32 inputs, 64 outputs\n",
        "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
        "    # fully connected, 7*7*64 inputs, 1024 outputs\n",
        "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
        "    # 1024 inputs, 10 outputs (class prediction)\n",
        "    'out': tf.Variable(tf.random_normal([1024, n_classes]))\n",
        "}\n",
        "\n",
        "biases = {\n",
        "    'bc1': tf.Variable(tf.random_normal([32])),\n",
        "    'bc2': tf.Variable(tf.random_normal([64])),\n",
        "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
        "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
        "}\n",
        "\n",
        "y_p,list_of_tensors = conv_net(x, weights, biases)\n",
        "\n",
        "#crossentropy cost\n",
        "with tf.name_scope('cross_entropy'):\n",
        "\n",
        "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_p, labels=y)) # cross entropy cost\n",
        "\n",
        "#tf.summary.scalar('cross_entropy', cost)\n",
        "\n",
        "\n",
        "# Evaluate model\n",
        "with tf.name_scope('operations'):\n",
        "\n",
        "    correct_pred = tf.equal(tf.argmax(y_p, 1), tf.argmax(y, 1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "#\n",
        "merged = tf.summary.scalar('accuracy', accuracy)\n",
        "\n",
        "\n",
        "\n",
        "# optimisation \n",
        "with tf.name_scope('optimisation'):\n",
        "\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# initalizing the graph and the weights\n",
        "#merged = tf.summary.merge_all()\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "\n",
        "\n",
        "# Launch the graph\n",
        "with tf.Session() as sess:\n",
        "    #graph_writer = tf.summary.FileWriter('./log/train', sess.graph)\n",
        "    #graph_writer.add_summary()\n",
        "    train_writer = tf.summary.FileWriter('./log/train', sess.graph)\n",
        "    test_writer = tf.summary.FileWriter('./log/test')\n",
        "    sess.run(init)\n",
        "    print('started')\n",
        "    \n",
        "    for i in range(10000):\n",
        "        \n",
        "        \n",
        "        \n",
        "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
        "        \n",
        "\n",
        "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
        "\n",
        "\n",
        "        summary,train_cost , train_acc = sess.run([merged,cost,accuracy], feed_dict={x: batch_x,y: batch_y})\n",
        "        train_writer.add_summary(summary, i)\n",
        "\n",
        "        #print('started')\n",
        "\n",
        "        test_batch_x, test_batch_y = mnist.test.next_batch(batch_size)\n",
        "\n",
        "        summary,test_cost, test_acc  = sess.run([merged,cost,accuracy], feed_dict={x: test_batch_x,y: test_batch_y})\n",
        "        train_writer.add_summary(summary, i)\n",
        "        \n",
        "        if i % 100 ==0:\n",
        "            \n",
        "           \n",
        "            val_list_of_tensors = sess.run(list_of_tensors, feed_dict={x: test_batch_x})\n",
        "            num_of_returned_sensors = len(val_list_of_tensors)\n",
        "            \n",
        "            print(val_list_of_tensors[-1].shape)\n",
        "            \n",
        "            imshow(val_list_of_tensors[0][0,:,:,0], cmap='gray')# conv1 pre-activation first input image first filter\n",
        "\n",
        "            plt.show()\n",
        "            imshow(val_list_of_tensors[1][0,:,:,0],  cmap='gray')# conv1 after-activation first input image first filter\n",
        "            plt.show()\n",
        "\n",
        "            \n",
        "            imshow(val_list_of_tensors[2][0,:,:,0], cmap='gray')# conv2 pre-activation first input image first filter\n",
        "\n",
        "            plt.show()\n",
        "            imshow(val_list_of_tensors[3][0,:,:,0], cmap='gray')# conv2 after-activation first input image first filter\n",
        "            plt.show()\n",
        "            \n",
        "            imshow(val_list_of_tensors[-1], cmap='gray')# conv2 pre-activation first input image first filter\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "            print('iter: ',i, 'train_cost: ', train_cost, 'train_acc: ', train_acc,'test_cost: ', test_cost, 'test_acc: ', test_acc, 'num_of_returned_sensors:', num_of_returned_sensors )\n",
        "\n",
        "\n",
        "    #y_p_p = sess.run(y_p, feed_dict={x: x_gr, y: y_gr})\n",
        "    \n",
        "    print('predicted ', y_p_p)\n",
        "    print('real ', y_gr)\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0726 17:28:38.349245 140322728843136 deprecation.py:323] From <ipython-input-4-3ff91ce2e3b9>:113: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "conv_actv_1 max [None, 28, 28, 32]\n",
            "con1_after max [None, 14, 14, 32]\n",
            "con2_before max [None, 14, 14, 64]\n",
            "con2_after max [None, 7, 7, 64]\n",
            "started\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-3ff91ce2e3b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'mnist' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMjcaiZyzJxm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWejYnUSzKh1",
        "colab_type": "text"
      },
      "source": [
        "# Tensorboard viz\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNXn4e82zO1i",
        "colab_type": "code",
        "outputId": "74c2e68c-b61e-4d12-8338-c9a0cb2c59d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        }
      },
      "source": [
        "n_classes=10\n",
        "learning_rate=0.002\n",
        "batch_size=64\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# Create some wrappers for simplicity\n",
        "def conv2d(x, W, b, strides=1):\n",
        "    # Conv2D wrapper, with bias and relu activation\n",
        "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
        "    x = tf.nn.bias_add(x, b)\n",
        "    return x\n",
        "\n",
        "\n",
        "num_inputs = 784\n",
        "num_outputs= 10\n",
        "\n",
        "# tf Graph input\n",
        "x = tf.placeholder(tf.float32, [None, num_inputs],name = 'input')\n",
        "y = tf.placeholder(tf.float32, [None, num_outputs],name =  'output')\n",
        "\n",
        "\n",
        "def maxpool2d(x, k=2):\n",
        "    # MaxPool2D wrapper\n",
        "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
        "                          padding='SAME')\n",
        "\n",
        "\n",
        "# Create model\n",
        "def conv_net(x, weights, biases):\n",
        "    # Reshape input picture\n",
        "    \n",
        "    with tf.name_scope('input-reshape'):\n",
        "        x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
        "\n",
        "    with tf.name_scope('conv-1'):\n",
        "\n",
        "        # Convolution Layer\n",
        "        conv_pre_actv_1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
        "        conv_actv_1 = tf.nn.relu(conv_pre_actv_1)\n",
        "        \n",
        "        print('conv_actv_1 max',conv_actv_1.get_shape().as_list())\n",
        "\n",
        "    with tf.name_scope('maxpooling-1'):\n",
        "        \n",
        "        # Max Pooling (down-sampling)\n",
        "        conv1 = maxpool2d(conv_actv_1, k=2)\n",
        "        print('con1_after max',conv1.get_shape().as_list())\n",
        "\n",
        "\n",
        "    # Convolution Layer\n",
        "    with tf.name_scope('conv-2'):\n",
        "\n",
        "        conv_pre_actv_2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
        "        conv_actv_2 = tf.nn.relu(conv_pre_actv_2)\n",
        "\n",
        "        \n",
        "        print('con2_before max',conv_actv_2.get_shape().as_list())\n",
        "\n",
        "\n",
        "    with tf.name_scope('maxpooling-2'):\n",
        "        \n",
        "     \n",
        "        # Max Pooling (down-sampling)\n",
        "        conv2 = maxpool2d(conv_actv_2, k=2)\n",
        "        print('con2_after max', conv2.get_shape().as_list())\n",
        "\n",
        "    # Fully connected layer\n",
        "    # Reshape conv2 output to fit fully connected layer input\n",
        "    #wd1 numx3x3  wd1.get_shape() -> numx9 \n",
        "    with tf.name_scope('flatten'):\n",
        "    \n",
        "        fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
        "        fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
        "        fc1 = tf.nn.relu(fc1)\n",
        "    # Apply Dropout\n",
        "    #fc1 = tf.nn.dropout(fc1, dropout)\n",
        "\n",
        "    # Output, class prediction\n",
        "    with tf.name_scope('output'):\n",
        "    \n",
        "        out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
        "        \n",
        "    list_of_tensors = [conv_pre_actv_1,conv_actv_1, conv1,conv_pre_actv_2,conv_actv_2, conv2, fc1]\n",
        "    return out, list_of_tensors\n",
        "\n",
        "\n",
        "# Store layers weight & bias\n",
        "weights = {\n",
        "    # 5x5 conv, 1 input, 32 outputs\n",
        "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32]), name=\"wc1\"),\n",
        "    # 5x5 conv, 32 inputs, 64 outputs\n",
        "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
        "    # fully connected, 7*7*64 inputs, 1024 outputs\n",
        "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
        "    # 1024 inputs, 10 outputs (class prediction)\n",
        "    'out': tf.Variable(tf.random_normal([1024, n_classes]))\n",
        "}\n",
        "\n",
        "biases = {\n",
        "    'bc1': tf.Variable(tf.random_normal([32])),\n",
        "    'bc2': tf.Variable(tf.random_normal([64])),\n",
        "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
        "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
        "}\n",
        "\n",
        "y_p,list_of_tensors = conv_net(x, weights, biases)\n",
        "\n",
        "#crossentropy cost\n",
        "with tf.name_scope('cross_entropy'):\n",
        "\n",
        "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_p, labels=y)) # cross entropy cost\n",
        "\n",
        "#tf.summary.scalar('cross_entropy', cost)\n",
        "\n",
        "\n",
        "# Evaluate model\n",
        "with tf.name_scope('accuracy'):\n",
        "\n",
        "    correct_pred = tf.equal(tf.argmax(y_p, 1), tf.argmax(y, 1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "#\n",
        "acc_summary = tf.summary.scalar('accuracy', accuracy)\n",
        "\n",
        "\n",
        "\n",
        "# optimisation \n",
        "with tf.name_scope('optimisation'):\n",
        "\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# initalizing the graph and the weights\n",
        "#merged = tf.summary.merge_all()\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "\n",
        "\n",
        "# Launch the graph\n",
        "with tf.Session() as sess:\n",
        "    #graph_writer = tf.summary.FileWriter('./log/train', sess.graph)\n",
        "    #graph_writer.add_summary()\n",
        "    train_writer = tf.summary.FileWriter('./log2/train', sess.graph)\n",
        "    test_writer = tf.summary.FileWriter('./log2/test')\n",
        "    sess.run(init)\n",
        "    print('started')\n",
        "    \n",
        "    for i in range(10000):\n",
        "        \n",
        "        \n",
        "        \n",
        "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
        "        \n",
        "\n",
        "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
        "\n",
        "\n",
        "        summary,train_cost , train_acc = sess.run([acc_summary,cost,accuracy], feed_dict={x: batch_x,y: batch_y})\n",
        "        train_writer.add_summary(summary, i)\n",
        "\n",
        "        #print('started')\n",
        "\n",
        "        test_batch_x, test_batch_y = mnist.test.next_batch(batch_size)\n",
        "\n",
        "        summary,test_cost, test_acc  = sess.run([acc_summary,cost,accuracy], feed_dict={x: test_batch_x,y: test_batch_y})\n",
        "        test_writer.add_summary(summary, i)\n",
        "\n",
        "        if i % 100 ==0:\n",
        "            \n",
        "           \n",
        "            val_list_of_tensors = sess.run(list_of_tensors, feed_dict={x: test_batch_x})\n",
        "            num_of_returned_sensors = len(val_list_of_tensors)\n",
        "            \n",
        "            print(val_list_of_tensors[-1].shape)\n",
        "            \n",
        "            imshow(val_list_of_tensors[0][0,:,:,0], cmap='gray')# conv1 pre-activation first input image first filter\n",
        "\n",
        "            plt.show()\n",
        "            imshow(val_list_of_tensors[1][0,:,:,0],  cmap='gray')# conv1 after-activation first input image first filter\n",
        "            plt.show()\n",
        "\n",
        "            \n",
        "            imshow(val_list_of_tensors[2][0,:,:,0], cmap='gray')# conv2 pre-activation first input image first filter\n",
        "\n",
        "            plt.show()\n",
        "            imshow(val_list_of_tensors[3][0,:,:,0], cmap='gray')# conv2 after-activation first input image first filter\n",
        "            plt.show()\n",
        "            \n",
        "            imshow(val_list_of_tensors[-1], cmap='gray')# conv2 pre-activation first input image first filter\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "            print('iter: ',i, 'train_cost: ', train_cost, 'train_acc: ', train_acc,'test_cost: ', test_cost, 'test_acc: ', test_acc, 'num_of_returned_sensors:', num_of_returned_sensors )\n",
        "\n",
        "\n",
        "    #y_p_p = sess.run(y_p, feed_dict={x: x_gr, y: y_gr})\n",
        "    \n",
        "    print('predicted ', y_p_p)\n",
        "    print('real ', y_gr)\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv_actv_1 max [None, 28, 28, 32]\n",
            "con1_after max [None, 14, 14, 32]\n",
            "con2_before max [None, 14, 14, 64]\n",
            "con2_after max [None, 7, 7, 64]\n",
            "started\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-0991a7747395>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'mnist' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhbgp1CwE_4t",
        "colab_type": "code",
        "outputId": "9f4c209e-8d92-4064-a120-790fb8c06d5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-07-26 17:29:12--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 35.173.6.94, 3.209.102.29, 34.206.9.96, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|35.173.6.94|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13607069 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  12.98M  18.5MB/s    in 0.7s    \n",
            "\n",
            "2019-07-26 17:29:13 (18.5 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13607069/13607069]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yipQU39JOccV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf log"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96bJem1FFBXx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LOG_DIR = './log2'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faCuFGDZFGCj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_ipython().system_raw('./ngrok http 6006 &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdPYibnJFI1H",
        "colab_type": "code",
        "outputId": "c6de98e4-9f0c-42b6-a59b-5499a17a2ed2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://8ac090bb.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}