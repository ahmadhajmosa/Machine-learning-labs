{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Session 2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahmadhajmosa/Machine-learning-labs/blob/Anna/Session_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmSUExzC-ZBV",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgWZldyi-Zef",
        "colab_type": "text"
      },
      "source": [
        "# Lab on Machine Learning and Applications in Intelligent Vehicles\n",
        "## Session 1: Introduction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a75NLOuwKXSs",
        "colab_type": "text"
      },
      "source": [
        "#Session 2: 05.06 - 13:00 - 14:30 :\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciUtGM-W_ehP",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "## Intro:\n",
        "\n",
        "Tensorflow is a powerful framework for implementing and deploying large-scale deep learning models. Recently, it has been widely used in both reasearch and production. TF objective is to combine scale and flexibility.\n",
        "\n",
        "In the past session, we will learning the following:\n",
        "\n",
        "1. TF programming stack\n",
        "2. TF programming concepts including computatoin graphs, operations and sessions. \n",
        "3. Implementation of linear regression\n",
        "4. Implementation of feed-forward neural networks\n",
        "\n",
        "## TF stack:\n",
        "\n",
        "TensorFlow is a framework composed of two core building blocks — a library for defining computational graphs and a runtime for executing such graphs on a variety of different hardware\n",
        "\n",
        "\n",
        "![alt text](https://www.tensorflow.org/images/layers.png)\n",
        "\n",
        "\n",
        "Before goining into details about the stack, let us talk about computational graphs.\n",
        "\n",
        "### Computational Graphs\n",
        "\n",
        "A directed graph is a data structure consisting of nodes (vertices) and edges. It’s a set of vertices connected pairwise by directed edges.\n",
        "\n",
        "Graphs come in many shapes and sizes and are used to solve many real-life problems, such as representing networks including telephone networks, circuit networks, road networks, and even social networks. \n",
        "![alt text](https://cdn-images-1.medium.com/max/800/1*V6aYjD3AxDbEKYahkGqVQw.png)\n",
        "\n",
        "TensorFlow uses directed graphs internally to represent computations, and they call this data flow graphs (or computational graphs).\n",
        "\n",
        "The nodes in TF data flow graph mostly represents operations, variables and placeholders.\n",
        "\n",
        "Take for example the following operation:\n",
        "![alt text](https://cdn-images-1.medium.com/max/800/1*6E3sfit6DCeJ9mOz17g4bA.png)\n",
        "\n",
        "To create a computational graph out of this program, we create nodes for each of the operations in our program, along with the input variables a and b. In fact, a and b could be constants if they don’t change. If one node is used as the input to another operation we draw a directed arrow that goes from one node to another.\n",
        "\n",
        "The computational graph for this program might look like this:\n",
        "![alt text](https://cdn-images-1.medium.com/max/800/1*vPb9E0Yd1QUAD0oFmAgaOw.png)\n",
        "\n",
        "Operations create or manipulate data according to specific rules. In TensorFlow those rules are called Ops, short for operations. Variables on the other hand represent shared, persistent state that can be manipulated by running Ops on those variables.\n",
        "\n",
        "The questions now what are the advantages of representing operations as directed graphs: The main advantage of using directed graphs is the ability to do **parallelism** and what is called **dependency driving scheduling**. \n",
        "For example, consider again the follwoing code:\n",
        "![alt text](https://cdn-images-1.medium.com/max/800/1*6E3sfit6DCeJ9mOz17g4bA.png)\n",
        "At the most fundamental level, most computer programs are mainly composed of two things — primitive operations and an order in which these operations are executed, often sequentially, line by line. This means we would first multiply a and b and only when this expression was evaluated we would take their sum. Computational graphs on the otherhand, exclusively specify the dependencies across the operations.\n",
        "If we look at our computational graph we see that we could execute the multiplication and addition in parallel. That’s because these two operations do not depend on each other.\n",
        " So we can use the topology of the graph to drive the scheduling of operations and execute them in the most efficient manner, e.g. using multiple GPUs on a single machine or even distribute the execution across multiple machines.\n",
        " Another key advantage is portability. The graph is a language-independent representation of our code. So we can build the graph in Python, save the model (TensorFlow uses protocol buffers), and restore the model in a different language, say C++, if you want to go really fast.\n",
        " \n",
        " \n",
        "\n",
        "--------------------------------\n",
        "# References:\n",
        "\n",
        "https://medium.com/@d3lm/understand-tensorflow-by-mimicking-its-api-from-scratch-faa55787170d\n",
        "\n",
        "https://www.tensorflow.org/guide/extend/architecture\n",
        "\n",
        "https://www.tensorflow.org/guide/low_level_intro\n",
        "\n",
        "  \n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLbVW-Adoko2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2129
        },
        "outputId": "271073f4-d469-47ad-831c-3bac94429964"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "learning_rate = 0.001\n",
        "training_iters = 2000\n",
        "batch_size = 128\n",
        "\n",
        "num_inputs = 3\n",
        "num_outputs = 4\n",
        "\n",
        "num_samples = 10\n",
        "\n",
        "# Training data\n",
        "x_gr = np.random.rand(num_samples, num_inputs)\n",
        "y_gr = np.random.rand(num_samples, num_outputs)\n",
        "\n",
        "# tf Graph input\n",
        "x = tf.placeholder(tf.float32, [None, num_inputs])\n",
        "y = tf.placeholder(tf.float32, [None, num_outputs])\n",
        "\n",
        "# weights\n",
        "w_1 = tf.Variable(tf.random_normal([num_inputs, num_outputs]))\n",
        "\n",
        "# model\n",
        "y_p = tf.matmul(x, w_1)\n",
        "\n",
        "# cost\n",
        "cost = tf.reduce_mean(tf.pow(y - y_p, 2)) # mse fisrt take then minimize\n",
        "\n",
        "# optimization\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
        "\n",
        "# initialize the graph\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(init)\n",
        "  \n",
        "  for i in range(100):\n",
        "    \n",
        "    sess.run(optimizer, feed_dict={x: x_gr, y: y_gr})\n",
        "    pr_cost = sess.run(cost, feed_dict={x: x_gr, y: y_gr})\n",
        "    print('iter', i, 'cost: ', pr_cost)\n",
        "  \n",
        "  y_p_p = sess.run(y_p, feed_dict={x: x_gr, y: y_gr})\n",
        "  print('predcited', y_p_p)\n",
        "  print('real ', y_gr)\n",
        "  "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter 0 cost:  0.61198723\n",
            "iter 1 cost:  0.6100459\n",
            "iter 2 cost:  0.6081093\n",
            "iter 3 cost:  0.6061774\n",
            "iter 4 cost:  0.60425043\n",
            "iter 5 cost:  0.60232836\n",
            "iter 6 cost:  0.60041124\n",
            "iter 7 cost:  0.59849924\n",
            "iter 8 cost:  0.5965924\n",
            "iter 9 cost:  0.5946908\n",
            "iter 10 cost:  0.5927944\n",
            "iter 11 cost:  0.5909034\n",
            "iter 12 cost:  0.58901787\n",
            "iter 13 cost:  0.5871377\n",
            "iter 14 cost:  0.5852631\n",
            "iter 15 cost:  0.58339405\n",
            "iter 16 cost:  0.5815306\n",
            "iter 17 cost:  0.5796728\n",
            "iter 18 cost:  0.5778208\n",
            "iter 19 cost:  0.5759746\n",
            "iter 20 cost:  0.57413405\n",
            "iter 21 cost:  0.57229936\n",
            "iter 22 cost:  0.57047063\n",
            "iter 23 cost:  0.56864774\n",
            "iter 24 cost:  0.5668308\n",
            "iter 25 cost:  0.5650198\n",
            "iter 26 cost:  0.5632149\n",
            "iter 27 cost:  0.5614159\n",
            "iter 28 cost:  0.55962306\n",
            "iter 29 cost:  0.55783606\n",
            "iter 30 cost:  0.55605525\n",
            "iter 31 cost:  0.5542805\n",
            "iter 32 cost:  0.55251193\n",
            "iter 33 cost:  0.5507494\n",
            "iter 34 cost:  0.548993\n",
            "iter 35 cost:  0.5472427\n",
            "iter 36 cost:  0.54549855\n",
            "iter 37 cost:  0.54376054\n",
            "iter 38 cost:  0.5420287\n",
            "iter 39 cost:  0.54030293\n",
            "iter 40 cost:  0.5385834\n",
            "iter 41 cost:  0.53686994\n",
            "iter 42 cost:  0.53516257\n",
            "iter 43 cost:  0.53346145\n",
            "iter 44 cost:  0.5317663\n",
            "iter 45 cost:  0.5300775\n",
            "iter 46 cost:  0.52839464\n",
            "iter 47 cost:  0.526718\n",
            "iter 48 cost:  0.5250474\n",
            "iter 49 cost:  0.52338284\n",
            "iter 50 cost:  0.52172446\n",
            "iter 51 cost:  0.5200721\n",
            "iter 52 cost:  0.5184258\n",
            "iter 53 cost:  0.5167856\n",
            "iter 54 cost:  0.51515144\n",
            "iter 55 cost:  0.5135233\n",
            "iter 56 cost:  0.51190114\n",
            "iter 57 cost:  0.51028496\n",
            "iter 58 cost:  0.5086748\n",
            "iter 59 cost:  0.5070706\n",
            "iter 60 cost:  0.50547236\n",
            "iter 61 cost:  0.5038801\n",
            "iter 62 cost:  0.50229377\n",
            "iter 63 cost:  0.5007132\n",
            "iter 64 cost:  0.49913874\n",
            "iter 65 cost:  0.4975701\n",
            "iter 66 cost:  0.4960073\n",
            "iter 67 cost:  0.49445042\n",
            "iter 68 cost:  0.49289933\n",
            "iter 69 cost:  0.491354\n",
            "iter 70 cost:  0.48981446\n",
            "iter 71 cost:  0.48828077\n",
            "iter 72 cost:  0.4867529\n",
            "iter 73 cost:  0.48523074\n",
            "iter 74 cost:  0.48371428\n",
            "iter 75 cost:  0.48220366\n",
            "iter 76 cost:  0.4806986\n",
            "iter 77 cost:  0.47919932\n",
            "iter 78 cost:  0.47770566\n",
            "iter 79 cost:  0.47621775\n",
            "iter 80 cost:  0.47473535\n",
            "iter 81 cost:  0.47325858\n",
            "iter 82 cost:  0.47178745\n",
            "iter 83 cost:  0.4703219\n",
            "iter 84 cost:  0.4688619\n",
            "iter 85 cost:  0.46740746\n",
            "iter 86 cost:  0.46595854\n",
            "iter 87 cost:  0.46451515\n",
            "iter 88 cost:  0.46307725\n",
            "iter 89 cost:  0.46164483\n",
            "iter 90 cost:  0.4602179\n",
            "iter 91 cost:  0.4587964\n",
            "iter 92 cost:  0.4573803\n",
            "iter 93 cost:  0.45596963\n",
            "iter 94 cost:  0.4545644\n",
            "iter 95 cost:  0.4531645\n",
            "iter 96 cost:  0.4517699\n",
            "iter 97 cost:  0.45038065\n",
            "iter 98 cost:  0.44899672\n",
            "iter 99 cost:  0.4476182\n",
            "predcited [[ 1.0642519  -0.10797907  0.63572466  0.6151705 ]\n",
            " [ 0.95057315 -0.05558784  0.4162824   0.2345316 ]\n",
            " [ 1.0715606  -0.21512848  0.48644888  0.8601454 ]\n",
            " [ 1.1371416   0.03873086  0.66152763  0.05474167]\n",
            " [ 1.719374    0.12379417  1.0996888  -0.05926034]\n",
            " [ 1.1694103  -0.08052979  0.8283477   0.6687256 ]\n",
            " [ 1.6303971  -0.06164222  0.9624437   0.537149  ]\n",
            " [ 1.4713259   0.12522149  0.83886063 -0.23176065]\n",
            " [ 0.8430602   0.12343843  0.6315912  -0.16910896]\n",
            " [ 2.14538    -0.02084586  1.2198906   0.4293001 ]]\n",
            "real  [[0.57394809 0.29101067 0.69905395 0.86220391]\n",
            " [0.02324423 0.22720175 0.75477321 0.58486926]\n",
            " [0.23815138 0.29457941 0.67673838 0.56579858]\n",
            " [0.62120126 0.78150879 0.65749981 0.1883166 ]\n",
            " [0.17328817 0.38847752 0.88352496 0.23752996]\n",
            " [0.91517876 0.97690852 0.96999503 0.09917936]\n",
            " [0.95635655 0.26766364 0.50175673 0.70247451]\n",
            " [0.41062677 0.15770593 0.06247677 0.74438781]\n",
            " [0.57134897 0.28954091 0.35324078 0.80823561]\n",
            " [0.13459438 0.80272035 0.31297211 0.17406621]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXbymsudvVj0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "outputId": "e2663868-11d5-4f39-e7c2-10cc8ee9c557"
      },
      "source": [
        "sess = tf.Session()\n",
        "sess.run(init)\n",
        "\n",
        "for i in range(10):\n",
        "    sess.run(optimizer, feed_dict={x: x_gr, y: y_gr})\n",
        "    pr_cost = sess.run(cost, feed_dict={x: x_gr, y: y_gr})\n",
        "    print('iter', i, 'cost: ', pr_cost)\n",
        "    \n",
        "y_p_p = sess.run(y_p, feed_dict={x: x_gr, y: y_gr})\n",
        "print('predcited', y_p_p)\n",
        "print('real ', y_gr)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter 0 cost:  0.4302662\n",
            "iter 1 cost:  0.42903796\n",
            "iter 2 cost:  0.42781314\n",
            "iter 3 cost:  0.42659205\n",
            "iter 4 cost:  0.4253745\n",
            "iter 5 cost:  0.42416057\n",
            "iter 6 cost:  0.42295036\n",
            "iter 7 cost:  0.4217438\n",
            "iter 8 cost:  0.4205409\n",
            "iter 9 cost:  0.4193418\n",
            "predcited [[ 0.55518985  0.76716846  1.2293353   0.26625723]\n",
            " [ 0.23008093  0.25880137  0.93389106  0.42814687]\n",
            " [ 0.6152188   1.0576633   1.1845642  -0.00955155]\n",
            " [ 0.23310451  0.04499477  1.1799806   0.7821259 ]\n",
            " [ 0.32409704 -0.09890425  1.8213812   1.3506966 ]\n",
            " [ 0.6766422   0.8533402   1.4354147   0.3622901 ]\n",
            " [ 0.60858256  0.6542869   1.793784    0.74282306]\n",
            " [ 0.11631601 -0.33205917  1.4542714   1.2578692 ]\n",
            " [ 0.1278496  -0.21396655  0.92602456  0.8252694 ]\n",
            " [ 0.61402434  0.49834505  2.2734153   1.1878214 ]]\n",
            "real  [[0.57394809 0.29101067 0.69905395 0.86220391]\n",
            " [0.02324423 0.22720175 0.75477321 0.58486926]\n",
            " [0.23815138 0.29457941 0.67673838 0.56579858]\n",
            " [0.62120126 0.78150879 0.65749981 0.1883166 ]\n",
            " [0.17328817 0.38847752 0.88352496 0.23752996]\n",
            " [0.91517876 0.97690852 0.96999503 0.09917936]\n",
            " [0.95635655 0.26766364 0.50175673 0.70247451]\n",
            " [0.41062677 0.15770593 0.06247677 0.74438781]\n",
            " [0.57134897 0.28954091 0.35324078 0.80823561]\n",
            " [0.13459438 0.80272035 0.31297211 0.17406621]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72MhC5Omv0iS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2129
        },
        "outputId": "ff6c8972-3d98-436b-e697-c53c97888b7c"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "learning_rate = 0.001\n",
        "training_iters = 2000\n",
        "batch_size = 128\n",
        "\n",
        "num_inputs = 3\n",
        "num_h1_n = 4\n",
        "num_h2_n = 10\n",
        "num_outputs = 4\n",
        "\n",
        "num_samples = 10\n",
        "\n",
        "# Training data\n",
        "x_gr = np.random.rand(num_samples, num_inputs)\n",
        "y_gr = np.random.rand(num_samples, num_outputs)\n",
        "\n",
        "# tf Graph input\n",
        "x = tf.placeholder(tf.float32, [None, num_inputs])\n",
        "y = tf.placeholder(tf.float32, [None, num_outputs])\n",
        "\n",
        "# weights\n",
        "w_1 = tf.Variable(tf.random_normal([num_inputs, num_h1_n]))\n",
        "w_2 = tf.Variable(tf.random_normal([num_h1_n, num_h2_n]))\n",
        "w_3 = tf.Variable(tf.random_normal([num_h2_n, num_outputs]))\n",
        "\n",
        "# bias\n",
        "b_1 = tf.Variable(tf.random_normal([num_h1_n]))\n",
        "b_2 = tf.Variable(tf.random_normal([num_h2_n]))\n",
        "b_3 = tf.Variable(tf.random_normal([num_outputs]))\n",
        "\n",
        "# model\n",
        "h1 = tf.nn.sigmoid(tf.add(tf.matmul(x, w_1), b_1)) # model of hidden layer 1\n",
        "h2 = tf.nn.sigmoid(tf.add(tf.matmul(h1, w_2), b_2)) # model of hidden layer 2\n",
        "y_p = tf.add(tf.matmul(h2, w_3), b_3)  # model of the output layer\n",
        "\n",
        "# cost\n",
        "cost = tf.reduce_mean(tf.pow(y - y_p, 2)) # mse first take then minimize\n",
        "\n",
        "# optimization\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
        "\n",
        "# initialize the graph\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(init)\n",
        "  \n",
        "  for i in range(100):\n",
        "    \n",
        "    sess.run(optimizer, feed_dict={x: x_gr, y: y_gr})\n",
        "    pr_cost = sess.run(cost, feed_dict={x: x_gr, y: y_gr})\n",
        "    print('iter', i, 'cost: ', pr_cost)\n",
        "  \n",
        "  y_p_p = sess.run(y_p, feed_dict={x: x_gr, y: y_gr})\n",
        "  print('predcited', y_p_p)\n",
        "  print('real ', y_gr)\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter 0 cost:  0.1679424\n",
            "iter 1 cost:  0.16739722\n",
            "iter 2 cost:  0.16685338\n",
            "iter 3 cost:  0.16631092\n",
            "iter 4 cost:  0.16576989\n",
            "iter 5 cost:  0.16523035\n",
            "iter 6 cost:  0.16469231\n",
            "iter 7 cost:  0.16415586\n",
            "iter 8 cost:  0.16362095\n",
            "iter 9 cost:  0.16308767\n",
            "iter 10 cost:  0.16255602\n",
            "iter 11 cost:  0.16202603\n",
            "iter 12 cost:  0.16149776\n",
            "iter 13 cost:  0.1609712\n",
            "iter 14 cost:  0.16044644\n",
            "iter 15 cost:  0.15992345\n",
            "iter 16 cost:  0.1594023\n",
            "iter 17 cost:  0.15888305\n",
            "iter 18 cost:  0.1583657\n",
            "iter 19 cost:  0.1578503\n",
            "iter 20 cost:  0.15733685\n",
            "iter 21 cost:  0.1568254\n",
            "iter 22 cost:  0.15631597\n",
            "iter 23 cost:  0.1558086\n",
            "iter 24 cost:  0.1553033\n",
            "iter 25 cost:  0.15480006\n",
            "iter 26 cost:  0.15429893\n",
            "iter 27 cost:  0.15379997\n",
            "iter 28 cost:  0.15330312\n",
            "iter 29 cost:  0.15280844\n",
            "iter 30 cost:  0.15231594\n",
            "iter 31 cost:  0.15182564\n",
            "iter 32 cost:  0.15133753\n",
            "iter 33 cost:  0.15085164\n",
            "iter 34 cost:  0.15036799\n",
            "iter 35 cost:  0.14988653\n",
            "iter 36 cost:  0.14940731\n",
            "iter 37 cost:  0.14893034\n",
            "iter 38 cost:  0.14845559\n",
            "iter 39 cost:  0.14798304\n",
            "iter 40 cost:  0.14751276\n",
            "iter 41 cost:  0.14704467\n",
            "iter 42 cost:  0.1465788\n",
            "iter 43 cost:  0.14611514\n",
            "iter 44 cost:  0.14565367\n",
            "iter 45 cost:  0.14519437\n",
            "iter 46 cost:  0.14473721\n",
            "iter 47 cost:  0.14428225\n",
            "iter 48 cost:  0.14382938\n",
            "iter 49 cost:  0.14337865\n",
            "iter 50 cost:  0.14293\n",
            "iter 51 cost:  0.1424834\n",
            "iter 52 cost:  0.14203887\n",
            "iter 53 cost:  0.14159635\n",
            "iter 54 cost:  0.14115584\n",
            "iter 55 cost:  0.14071725\n",
            "iter 56 cost:  0.14028063\n",
            "iter 57 cost:  0.13984592\n",
            "iter 58 cost:  0.13941309\n",
            "iter 59 cost:  0.13898206\n",
            "iter 60 cost:  0.13855287\n",
            "iter 61 cost:  0.13812546\n",
            "iter 62 cost:  0.1376998\n",
            "iter 63 cost:  0.1372758\n",
            "iter 64 cost:  0.1368535\n",
            "iter 65 cost:  0.13643284\n",
            "iter 66 cost:  0.13601375\n",
            "iter 67 cost:  0.13559622\n",
            "iter 68 cost:  0.13518019\n",
            "iter 69 cost:  0.13476565\n",
            "iter 70 cost:  0.13435254\n",
            "iter 71 cost:  0.13394085\n",
            "iter 72 cost:  0.13353048\n",
            "iter 73 cost:  0.13312143\n",
            "iter 74 cost:  0.13271365\n",
            "iter 75 cost:  0.13230713\n",
            "iter 76 cost:  0.13190177\n",
            "iter 77 cost:  0.13149759\n",
            "iter 78 cost:  0.13109452\n",
            "iter 79 cost:  0.13069251\n",
            "iter 80 cost:  0.13029154\n",
            "iter 81 cost:  0.12989156\n",
            "iter 82 cost:  0.12949255\n",
            "iter 83 cost:  0.12909445\n",
            "iter 84 cost:  0.12869722\n",
            "iter 85 cost:  0.12830083\n",
            "iter 86 cost:  0.12790525\n",
            "iter 87 cost:  0.12751043\n",
            "iter 88 cost:  0.12711635\n",
            "iter 89 cost:  0.12672296\n",
            "iter 90 cost:  0.12633024\n",
            "iter 91 cost:  0.12593815\n",
            "iter 92 cost:  0.12554666\n",
            "iter 93 cost:  0.12515573\n",
            "iter 94 cost:  0.12476534\n",
            "iter 95 cost:  0.12437544\n",
            "iter 96 cost:  0.12398605\n",
            "iter 97 cost:  0.1235971\n",
            "iter 98 cost:  0.123208545\n",
            "iter 99 cost:  0.122820415\n",
            "predcited [[0.44350305 0.839412   0.49637812 0.47091088]\n",
            " [0.45391217 0.84518296 0.4852562  0.438794  ]\n",
            " [0.4578295  0.83979386 0.5336766  0.48385775]\n",
            " [0.44687054 0.83915615 0.54184246 0.515637  ]\n",
            " [0.42950988 0.8335855  0.46091038 0.46839523]\n",
            " [0.45501864 0.8462041  0.52322924 0.47729155]\n",
            " [0.43466353 0.8379029  0.5199768  0.5102945 ]\n",
            " [0.47085845 0.84148717 0.5451634  0.47265536]\n",
            " [0.46744984 0.8417501  0.54759085 0.48327166]\n",
            " [0.41982082 0.81402737 0.5442217  0.55814195]]\n",
            "real  [[0.75677496 0.86622504 0.24616572 0.08559706]\n",
            " [0.00458467 0.90546897 0.62091264 0.69562129]\n",
            " [0.38832203 0.09411037 0.53453878 0.9388733 ]\n",
            " [0.25782516 0.20914901 0.604711   0.25880953]\n",
            " [0.36600028 0.24471499 0.18936937 0.77867655]\n",
            " [0.42414063 0.98935966 0.41082234 0.72101364]\n",
            " [0.00915616 0.17289262 0.97779326 0.38942187]\n",
            " [0.65362865 0.3688986  0.71169712 0.69685883]\n",
            " [0.17225434 0.01809993 0.8897835  0.09220552]\n",
            " [0.60148096 0.62382816 0.62216857 0.19050468]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9v3V8ztAKDCg",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "\n",
        "Before we jump into Tensorflow, we will implemented our first neural network model using Python Numpy package. NumPy is the fundamental package for scientific computing with Python, such as:\n",
        "\n",
        "1. Linear Algebra\n",
        "2. Statistics\n",
        "3. Calculus\n",
        "\n",
        "## A brief intro to Numpy operations:\n",
        "\n",
        "1. Creating a Vector:\n",
        "Here we use Numpy to create a 1-D Array which we then call a vector.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTf7M4r7Lgj9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load Library\n",
        "import numpy as np\n",
        "\n",
        "#Create a vector as a Row\n",
        "vector_row = np.array([1,2,3])\n",
        "\n",
        "#Create vector as a Column\n",
        "vector_column = np.array([[1],[2],[3]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYFjSo0OLqA3",
        "colab_type": "text"
      },
      "source": [
        "2. Creating a Matrix\n",
        "We Create a 2-D Array in Numpy and call it a Matrix. It contains 2 rows and 3 columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJlDBq5rLmA-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load Library\n",
        "import numpy as np\n",
        "\n",
        "#Create a Matrix\n",
        "matrix = np.array([[1,2,3],[4,5,6]])\n",
        "print(matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wv99hZqULygH",
        "colab_type": "text"
      },
      "source": [
        "3. Selecting Elements\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLQlxFzkPrKM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load Library\n",
        "import numpy as np\n",
        "\n",
        "#Create a vector as a Row\n",
        "vector_row = np.array([ 1,2,3,4,5,6 ])\n",
        "\n",
        "#Create a Matrix\n",
        "matrix = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
        "print(matrix)\n",
        "\n",
        "#Select 3rd element of Vector\n",
        "print(vector_row[2])\n",
        "\n",
        "#Select 2nd row 2nd column\n",
        "print(matrix[1,1])\n",
        "#Select all elements of a vector\n",
        "print(vector_row[:])\n",
        "#Select everything up to and including the 3rd element\n",
        "print(vector_row[:3])\n",
        "#Select the everything after the 3rd element\n",
        "print(vector_row[3:])\n",
        "#Select the last element\n",
        "print(vector_row[-1])\n",
        "#Select the first 2 rows and all the columns of the matrix\n",
        "print(matrix[:2,:])\n",
        "#Select all rows and the 2nd column of the matrix\n",
        "print(matrix[:,1:2])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QO3vTGEKQhm7",
        "colab_type": "text"
      },
      "source": [
        "4. Describing a Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8bDjBhhQpg5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "#Create a Matrix\n",
        "matrix = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
        "#View the Number of Rows and Columns\n",
        "print(matrix.shape)\n",
        "#View the number of elements (rows*columns)\n",
        "print(matrix.size)\n",
        "#View the number of Dimensions(2 in this case)\n",
        "print(matrix.ndim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKISvY8kQtA0",
        "colab_type": "text"
      },
      "source": [
        "5. Finding the max and min values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abPJd0JrQ4mM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load Library\n",
        "import numpy as np\n",
        "\n",
        "#Create a Matrix\n",
        "matrix = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
        "print(matrix)\n",
        "#Return the max element\n",
        "print(np.max(matrix))\n",
        "#Return the min element\n",
        "print(np.min(matrix))\n",
        "#To find the max element in each column\n",
        "print(np.max(matrix,axis=0))\n",
        "#To find the max element in each row\n",
        "print(np.max(matrix,axis=1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Qm64s_eR0zQ",
        "colab_type": "text"
      },
      "source": [
        "6. Reshaping Arrays\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pwepq7h_SBBD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load Library\n",
        "import numpy as np\n",
        "\n",
        "#Create a Matrix\n",
        "matrix = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
        "print(matrix)\n",
        "#Reshape\n",
        "print(matrix.reshape(9,1))\n",
        "#Here -1 says as many columns as needed and 1 row\n",
        "print(matrix.reshape(1,-1))\n",
        "#If we provide only 1 value Reshape would return a 1-d array of that length\n",
        "print(matrix.reshape(9))\n",
        "#We can also use the Flatten method to convert a matrix to 1-d array\n",
        "print(matrix.flatten())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJU3xABuVem_",
        "colab_type": "text"
      },
      "source": [
        "7. Calculating Dot Products"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPKg382VVivy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load Library\n",
        "import numpy as np\n",
        "\n",
        "#Create vector-1\n",
        "vector_1 = np.array([ 1,2,3 ])\n",
        "#Create vector-2\n",
        "vector_2 = np.array([ 4,5,6 ])\n",
        "#Calculate Dot Product\n",
        "print(np.dot(vector_1,vector_2))\n",
        "#Alternatively you can use @ to calculate dot products\n",
        "print(vector_1 @ vector_2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cB-jK7jEXY7F",
        "colab_type": "text"
      },
      "source": [
        "##Linear regression in Numpy:\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Write the numpy code for the following model:\n",
        "\n",
        "$Y=WX+B$\n",
        "\n",
        "where $X$ is 3x10 matrix:  10 samples and 3 features\n",
        "\n",
        "$Y$ is 4x10 matrix: 10 samples and 4 outputs\n",
        "\n",
        "$W$ is the weights matrix with the shape 4x3: connecting 3 inputs to 4 outputs\n",
        "\n",
        "$b$ is a vector with a size 4 ( one bias per output)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EtM5LVtWCpm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load Library\n",
        "import numpy as np\n",
        "\n",
        "# Generate a random X (we do not have a real data)\n",
        "X = np.random.rand(3,10)\n",
        "display(X.shape)\n",
        "\n",
        "# Generate a random weights vector\n",
        "W = np.random.rand(4,3)\n",
        "\n",
        "# Generate a random bias \n",
        "b = np.random.rand(4,1)\n",
        "\n",
        "# Calculate Y\n",
        "Y= np.dot(W,X) + b\n",
        "display(Y.shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMIoucH9hFfr",
        "colab_type": "text"
      },
      "source": [
        "## One neuron model in numpy:\n",
        "\n",
        "A single neuron has multiple inputs and one output, in addition to the linear regression model, we need to add non linearity through an activation function:\n",
        "\n",
        "$Y= f(WX+B)$\n",
        "\n",
        "where $X$ is n x m matrix:  m samples and n features/inputs\n",
        "\n",
        "$f(g)= \\frac{1}{1+\\exp(-g)}$  is a sigmoid acitavation function\n",
        "\n",
        "$Y$ is nh1 x m matrix: m samples and ny outputs\n",
        "\n",
        "$W$ is the weights matrix with the shape nh1 x n: connecting 3 inputs to 4 outputs\n",
        "\n",
        "$b$ is a vector with a size nh1 ( one bias per output)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qry1JDGEiLmx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load Library\n",
        "import numpy as np \n",
        "\n",
        "f = lambda x: 1.0/(1.0 + np.exp(-x)) # activation function (use sigmoid)\n",
        "\n",
        "# Generate a random X (we do not have a real data)\n",
        "X = np.random.rand(3,10)\n",
        "\n",
        "\n",
        "# Generate a random weights vector\n",
        "W = np.random.rand(1,3)\n",
        "\n",
        "\n",
        "# Generate a random bias \n",
        "b = np.random.rand()\n",
        "\n",
        "# Calculate Y\n",
        "Y= f(np.dot(W,X) + b)\n",
        "display(Y)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSnbti9ooIIs",
        "colab_type": "text"
      },
      "source": [
        "## One hidden layer model in numpy:\n",
        "\n",
        "The difference from the one neuron model is simple:  we need only to change the number of output \"ny\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAY3o6zBnpA0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load Library\n",
        "import numpy as np \n",
        "\n",
        "#Suppose we have the following NN architecture\n",
        "\n",
        "m = 10 # Number of samples\n",
        "ni= 3 # Number of input neurons\n",
        "h = 1 # Number of hidden layers\n",
        "nh1 = 4 # Number of neurons in the hidden layer 1\n",
        "no =1 # Number of neurons in the output layer\n",
        "\n",
        "\n",
        "\n",
        "f = lambda x: 1.0/(1.0 + np.exp(-x)) # activation function (use sigmoid)\n",
        "\n",
        "# Generate a random X (we do not have a real data)\n",
        "X = np.random.rand(ni,m)\n",
        "\n",
        "\n",
        "# Generate a random weights vector for the first hidden layer\n",
        "W1 = np.random.rand(nh1,ni)\n",
        "\n",
        "\n",
        "# Generate a random bias for the first hidden layer \n",
        "b1 = np.random.rand(nh1,1)\n",
        "\n",
        "# Generate a random weights vector for the output layer\n",
        "W2 = np.random.rand(no,nh1)\n",
        "\n",
        "# Generate a random bias for the output layer \n",
        "b2 = np.random.rand(no,1)\n",
        "\n",
        "# Calculate output of the first hidden layer\n",
        "Yh1= f(np.dot(W1,X) + b1)\n",
        "\n",
        "# Calculate output of the output layer\n",
        "\n",
        "Y= f(np.dot(W2,Yh1) + b2)\n",
        "\n",
        "display(Yh1.shape)\n",
        "display(Y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11fqb_bQvIEi",
        "colab_type": "text"
      },
      "source": [
        "## Gradient descent in Numpy:\n",
        "Let us now start training a neural network\n",
        "We start by implementing a simple gradient descent for linear regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzBJxwb7FFZ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaQyLoxk2FyG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "converged = False\n",
        "iter = 0\n",
        "m = 10 # Number of samples\n",
        "ni= 1 # Number of input neurons\n",
        "h = 1 # Number of hidden layers\n",
        "no =1 # Number of neurons in the output layer\n",
        "\n",
        "# Generate a random X (we do not have a real data)\n",
        "X = np.random.rand(m)\n",
        "display(X)\n",
        "\n",
        "# learning rate\n",
        "alpha =0.01\n",
        "\n",
        "# early stop criteria \n",
        "ep=0.001\n",
        "\n",
        "# maximum number of training iterations\n",
        "max_iter=100\n",
        "\n",
        "# Generate a random weights vector for the output layer\n",
        "W1 = np.random.rand()\n",
        "\n",
        "# Generate a random bias for the output layer \n",
        "b1 = np.random.rand()\n",
        "\n",
        "# Generate a random ground truth\n",
        "Y_gr = np.random.rand(m)\n",
        "\n",
        "\n",
        "J = sum([(b1 + W1*X[i] - Y_gr[i])**2 for i in range(m)])\n",
        "\n",
        "while not converged:\n",
        "        # for each training sample, compute the gradient (d/d_theta j(theta))\n",
        "        grad0 = 1.0/m * sum([(b1 + W1*X[i] - Y_gr[i]) for i in range(m)]) \n",
        "        grad1 = 1.0/m * sum([(b1 + W1*X[i] - Y_gr[i])*X[i] for i in range(m)])\n",
        "        \n",
        "        # update the theta_temp\n",
        "        temp0 = W1 - alpha * grad0\n",
        "        temp1 = b1 - alpha * grad1\n",
        "        # update theta\n",
        "        W1 = temp0\n",
        "        b1 = temp1\n",
        "        \n",
        "        # sum squared error\n",
        "        e = sum([(b1 + W1*X[i] - Y_gr[i])**2 for i in range(m)]) \n",
        "\n",
        "        if abs(J-e) <= ep:\n",
        "            print('Converged, iterations: ', iter, '!!!')\n",
        "            converged = True\n",
        "    \n",
        "        J = e   # update error \n",
        "        iter += 1  # update iter\n",
        "    \n",
        "        if iter == max_iter:\n",
        "            print('Max interactions exceeded!')\n",
        "            converged = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRqCCVlmFIZl",
        "colab_type": "text"
      },
      "source": [
        "##Assignment 1\n",
        "### Backpropagation in Numpy:\n"
      ]
    }
  ]
}