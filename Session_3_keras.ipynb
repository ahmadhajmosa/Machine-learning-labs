{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Session 3 kras.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahmadhajmosa/Machine-learning-labs/blob/ebrandner/Session_3_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmSUExzC-ZBV",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1MeHqHUMkiD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgWZldyi-Zef",
        "colab_type": "text"
      },
      "source": [
        "# Lab on Machine Learning and Applications in Intelligent Vehicles\n",
        "## Session 3: Keras\n",
        "\n",
        "### * Intro into Keras\n",
        "\n",
        "### * CNN using Keras\n",
        "\n",
        "### * LSTM using Keras\n",
        "\n",
        "### * Variational Auto encoder using Keras\n",
        "\n",
        "### * Seq2Seq Model with keras\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uf8EYKGvNX-M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d3aa7aab-bea2-42bf-b1df-855bfcc9473f"
      },
      "source": [
        "!pip install pydot\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pydot in /usr/local/lib/python3.6/dist-packages (1.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.6/dist-packages (from pydot) (2.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8ccyZJfMlFr",
        "colab_type": "code",
        "outputId": "c4e6ada6-8491-42ae-cf3c-ba5ed85ac339",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        }
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Input\n",
        "\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(32, input_shape=(16,),name=\"input_layer\"))\n",
        "model.add(Dense(100,activation=\"relu\",name=\"hidden_layer_1\"))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(200,activation=\"sigmoid\",name=\"hidden_layer_2\"))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(30,name=\"hidden_layer_3\"))\n",
        "model.add(Dense(100,name=\"hidden_layer_4\"))\n",
        "\n",
        "# now the model will take as input arrays of shape (*, 16)\n",
        "# and output arrays of shape (*, 32)\n",
        "\n",
        "# after the first layer, you don't need to specify\n",
        "# the size of the input anymore:\n",
        "model.add(Dense(32,name=\"output_layer\"))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "\n",
        "SVG(model_to_dot(model).create(prog='dot', format='svg'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0726 17:09:49.953208 140441212782464 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0726 17:09:49.986187 140441212782464 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0726 17:09:49.992414 140441212782464 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0726 17:09:50.020772 140441212782464 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0726 17:09:50.030345 140441212782464 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_layer (Dense)          (None, 32)                544       \n",
            "_________________________________________________________________\n",
            "hidden_layer_1 (Dense)       (None, 100)               3300      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "hidden_layer_2 (Dense)       (None, 200)               20200     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "hidden_layer_3 (Dense)       (None, 30)                6030      \n",
            "_________________________________________________________________\n",
            "hidden_layer_4 (Dense)       (None, 100)               3100      \n",
            "_________________________________________________________________\n",
            "output_layer (Dense)         (None, 32)                3232      \n",
            "=================================================================\n",
            "Total params: 36,406\n",
            "Trainable params: 36,406\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KX9SVeTCPvuD",
        "colab_type": "code",
        "outputId": "b7a4f67d-46cd-4521-ea93-4bfcaa69ea3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "from keras.layers import Conv2D, Flatten\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(64, (3, 3),\n",
        "                 input_shape=(3, 32, 32), padding='same',))\n",
        "model.add(Conv2D(20, (3, 3),padding='same'))\n",
        "\n",
        "\n",
        "# now: model.output_shape == (None, 64, 32, 32)\n",
        "# now: model.input_shape == (None, 3, 32, 32)\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "\n",
        "SVG(model_to_dot(model).create(prog='dot', format='svg'))\n",
        "\n",
        "display(model.summary())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_3 (Conv2D)            (None, 3, 32, 64)         18496     \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 3, 32, 20)         11540     \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 1920)              0         \n",
            "=================================================================\n",
            "Total params: 30,036\n",
            "Trainable params: 30,036\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANQ6jNhgSgXc",
        "colab_type": "code",
        "outputId": "a39b10ba-ca60-459b-b548-ec24f7a1a57c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "import os\n",
        "\n",
        "batch_size = 32\n",
        "num_classes = 10\n",
        "epochs = 100\n",
        "data_augmentation = True\n",
        "num_predictions = 20\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "model_name = 'keras_cifar10_trained_model.h5'\n",
        "\n",
        "# The data, split between train and test sets:\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# Convert class vectors to binary class matrices.\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)# labels =[1,2,1,3] --> [[1,0,0],[0,1,0],[0,1,0],[0,0,1]]\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "# Model creation\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), padding='same',\n",
        "                 input_shape=x_train.shape[1:]))\n",
        "\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "# initiate RMSprop optimizer\n",
        "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
        "\n",
        "# Let's train the model using RMSprop\n",
        "\n",
        "# binary_corrsentropy for binary classificaiton 2 classes\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# scaling from 0-255 to 0-1\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              validation_data=(x_test, y_test),\n",
        "              shuffle=True)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 4s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0726 17:10:29.469294 140441212782464 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (50000, 32, 32, 3)\n",
            "50000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0726 17:10:29.718084 140441212782464 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0726 17:10:29.725692 140441212782464 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0726 17:10:30.323981 140441212782464 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/100\n",
            "50000/50000 [==============================] - 18s 361us/step - loss: 1.8267 - acc: 0.3296 - val_loss: 1.6122 - val_acc: 0.4215\n",
            "Epoch 2/100\n",
            "50000/50000 [==============================] - 11s 215us/step - loss: 1.4962 - acc: 0.4582 - val_loss: 1.3671 - val_acc: 0.5051\n",
            "Epoch 3/100\n",
            "50000/50000 [==============================] - 11s 215us/step - loss: 1.3586 - acc: 0.5163 - val_loss: 1.2357 - val_acc: 0.5660\n",
            "Epoch 4/100\n",
            "50000/50000 [==============================] - 11s 215us/step - loss: 1.2546 - acc: 0.5566 - val_loss: 1.1378 - val_acc: 0.6031\n",
            "Epoch 5/100\n",
            "50000/50000 [==============================] - 11s 216us/step - loss: 1.1756 - acc: 0.5839 - val_loss: 1.1002 - val_acc: 0.6118\n",
            "Epoch 6/100\n",
            "50000/50000 [==============================] - 11s 214us/step - loss: 1.1100 - acc: 0.6091 - val_loss: 1.0558 - val_acc: 0.6294\n",
            "Epoch 7/100\n",
            "50000/50000 [==============================] - 11s 212us/step - loss: 1.0574 - acc: 0.6276 - val_loss: 1.0918 - val_acc: 0.6190\n",
            "Epoch 8/100\n",
            "50000/50000 [==============================] - 11s 213us/step - loss: 1.0155 - acc: 0.6460 - val_loss: 0.9866 - val_acc: 0.6582\n",
            "Epoch 9/100\n",
            "50000/50000 [==============================] - 11s 213us/step - loss: 0.9784 - acc: 0.6553 - val_loss: 0.9372 - val_acc: 0.6736\n",
            "Epoch 10/100\n",
            "50000/50000 [==============================] - 11s 211us/step - loss: 0.9434 - acc: 0.6708 - val_loss: 0.8786 - val_acc: 0.6917\n",
            "Epoch 11/100\n",
            "50000/50000 [==============================] - 11s 211us/step - loss: 0.9108 - acc: 0.6845 - val_loss: 0.9020 - val_acc: 0.6801\n",
            "Epoch 12/100\n",
            "50000/50000 [==============================] - 11s 211us/step - loss: 0.8884 - acc: 0.6904 - val_loss: 0.9129 - val_acc: 0.6856\n",
            "Epoch 13/100\n",
            "50000/50000 [==============================] - 11s 213us/step - loss: 0.8612 - acc: 0.6995 - val_loss: 0.8216 - val_acc: 0.7160\n",
            "Epoch 14/100\n",
            "50000/50000 [==============================] - 11s 210us/step - loss: 0.8394 - acc: 0.7073 - val_loss: 0.8022 - val_acc: 0.7210\n",
            "Epoch 15/100\n",
            "50000/50000 [==============================] - 11s 212us/step - loss: 0.8225 - acc: 0.7163 - val_loss: 0.8729 - val_acc: 0.7055\n",
            "Epoch 16/100\n",
            "50000/50000 [==============================] - 11s 212us/step - loss: 0.8068 - acc: 0.7207 - val_loss: 0.7740 - val_acc: 0.7358\n",
            "Epoch 17/100\n",
            "50000/50000 [==============================] - 11s 211us/step - loss: 0.7892 - acc: 0.7253 - val_loss: 0.7618 - val_acc: 0.7383\n",
            "Epoch 18/100\n",
            "50000/50000 [==============================] - 10s 210us/step - loss: 0.7825 - acc: 0.7275 - val_loss: 0.7574 - val_acc: 0.7387\n",
            "Epoch 19/100\n",
            "50000/50000 [==============================] - 11s 217us/step - loss: 0.7649 - acc: 0.7363 - val_loss: 0.7518 - val_acc: 0.7393\n",
            "Epoch 20/100\n",
            "50000/50000 [==============================] - 11s 210us/step - loss: 0.7569 - acc: 0.7378 - val_loss: 0.7284 - val_acc: 0.7502\n",
            "Epoch 21/100\n",
            "50000/50000 [==============================] - 11s 211us/step - loss: 0.7504 - acc: 0.7431 - val_loss: 0.7396 - val_acc: 0.7419\n",
            "Epoch 22/100\n",
            "50000/50000 [==============================] - 10s 209us/step - loss: 0.7410 - acc: 0.7449 - val_loss: 0.7451 - val_acc: 0.7452\n",
            "Epoch 23/100\n",
            "50000/50000 [==============================] - 11s 214us/step - loss: 0.7337 - acc: 0.7494 - val_loss: 0.7435 - val_acc: 0.7428\n",
            "Epoch 24/100\n",
            "50000/50000 [==============================] - 10s 210us/step - loss: 0.7249 - acc: 0.7520 - val_loss: 0.7136 - val_acc: 0.7544\n",
            "Epoch 25/100\n",
            "50000/50000 [==============================] - 10s 209us/step - loss: 0.7179 - acc: 0.7548 - val_loss: 0.6960 - val_acc: 0.7595\n",
            "Epoch 26/100\n",
            "50000/50000 [==============================] - 11s 212us/step - loss: 0.7109 - acc: 0.7595 - val_loss: 0.7245 - val_acc: 0.7515\n",
            "Epoch 27/100\n",
            "50000/50000 [==============================] - 11s 213us/step - loss: 0.7092 - acc: 0.7596 - val_loss: 0.7148 - val_acc: 0.7574\n",
            "Epoch 28/100\n",
            "50000/50000 [==============================] - 11s 212us/step - loss: 0.7021 - acc: 0.7616 - val_loss: 0.7389 - val_acc: 0.7484\n",
            "Epoch 29/100\n",
            "50000/50000 [==============================] - 10s 209us/step - loss: 0.7003 - acc: 0.7618 - val_loss: 0.6906 - val_acc: 0.7632\n",
            "Epoch 30/100\n",
            "50000/50000 [==============================] - 10s 209us/step - loss: 0.6957 - acc: 0.7633 - val_loss: 0.6824 - val_acc: 0.7679\n",
            "Epoch 31/100\n",
            "50000/50000 [==============================] - 10s 209us/step - loss: 0.6923 - acc: 0.7651 - val_loss: 0.6884 - val_acc: 0.7673\n",
            "Epoch 32/100\n",
            "50000/50000 [==============================] - 10s 208us/step - loss: 0.6865 - acc: 0.7664 - val_loss: 0.7182 - val_acc: 0.7599\n",
            "Epoch 33/100\n",
            "50000/50000 [==============================] - 10s 209us/step - loss: 0.6800 - acc: 0.7702 - val_loss: 0.7082 - val_acc: 0.7621\n",
            "Epoch 34/100\n",
            "50000/50000 [==============================] - 10s 210us/step - loss: 0.6771 - acc: 0.7713 - val_loss: 0.7121 - val_acc: 0.7591\n",
            "Epoch 35/100\n",
            "50000/50000 [==============================] - 10s 209us/step - loss: 0.6793 - acc: 0.7696 - val_loss: 0.6739 - val_acc: 0.7704\n",
            "Epoch 36/100\n",
            "50000/50000 [==============================] - 11s 211us/step - loss: 0.6731 - acc: 0.7737 - val_loss: 0.6675 - val_acc: 0.7715\n",
            "Epoch 37/100\n",
            "50000/50000 [==============================] - 10s 209us/step - loss: 0.6668 - acc: 0.7741 - val_loss: 0.6725 - val_acc: 0.7732\n",
            "Epoch 38/100\n",
            "50000/50000 [==============================] - 10s 208us/step - loss: 0.6680 - acc: 0.7738 - val_loss: 0.6738 - val_acc: 0.7706\n",
            "Epoch 39/100\n",
            "50000/50000 [==============================] - 10s 209us/step - loss: 0.6620 - acc: 0.7781 - val_loss: 0.7027 - val_acc: 0.7630\n",
            "Epoch 40/100\n",
            "50000/50000 [==============================] - 10s 208us/step - loss: 0.6612 - acc: 0.7786 - val_loss: 0.6733 - val_acc: 0.7753\n",
            "Epoch 41/100\n",
            "50000/50000 [==============================] - 10s 208us/step - loss: 0.6594 - acc: 0.7775 - val_loss: 0.6824 - val_acc: 0.7694\n",
            "Epoch 42/100\n",
            "50000/50000 [==============================] - 10s 208us/step - loss: 0.6530 - acc: 0.7803 - val_loss: 0.6718 - val_acc: 0.7736\n",
            "Epoch 43/100\n",
            "50000/50000 [==============================] - 10s 209us/step - loss: 0.6459 - acc: 0.7810 - val_loss: 0.6582 - val_acc: 0.7790\n",
            "Epoch 44/100\n",
            "50000/50000 [==============================] - 10s 209us/step - loss: 0.6484 - acc: 0.7825 - val_loss: 0.7341 - val_acc: 0.7632\n",
            "Epoch 45/100\n",
            "50000/50000 [==============================] - 10s 208us/step - loss: 0.6504 - acc: 0.7815 - val_loss: 0.7189 - val_acc: 0.7657\n",
            "Epoch 46/100\n",
            "50000/50000 [==============================] - 10s 208us/step - loss: 0.6423 - acc: 0.7836 - val_loss: 0.6517 - val_acc: 0.7793\n",
            "Epoch 47/100\n",
            "50000/50000 [==============================] - 10s 208us/step - loss: 0.6447 - acc: 0.7828 - val_loss: 0.6685 - val_acc: 0.7748\n",
            "Epoch 48/100\n",
            "50000/50000 [==============================] - 11s 211us/step - loss: 0.6399 - acc: 0.7853 - val_loss: 0.7180 - val_acc: 0.7666\n",
            "Epoch 49/100\n",
            "50000/50000 [==============================] - 11s 212us/step - loss: 0.6409 - acc: 0.7863 - val_loss: 0.6530 - val_acc: 0.7826\n",
            "Epoch 50/100\n",
            "50000/50000 [==============================] - 10s 208us/step - loss: 0.6386 - acc: 0.7870 - val_loss: 0.6957 - val_acc: 0.7609\n",
            "Epoch 51/100\n",
            "50000/50000 [==============================] - 10s 209us/step - loss: 0.6423 - acc: 0.7851 - val_loss: 0.6442 - val_acc: 0.7813\n",
            "Epoch 52/100\n",
            "50000/50000 [==============================] - 11s 212us/step - loss: 0.6306 - acc: 0.7877 - val_loss: 0.7010 - val_acc: 0.7606\n",
            "Epoch 53/100\n",
            "50000/50000 [==============================] - 10s 207us/step - loss: 0.6309 - acc: 0.7892 - val_loss: 0.6687 - val_acc: 0.7739\n",
            "Epoch 54/100\n",
            "50000/50000 [==============================] - 10s 207us/step - loss: 0.6296 - acc: 0.7899 - val_loss: 0.6691 - val_acc: 0.7794\n",
            "Epoch 55/100\n",
            "50000/50000 [==============================] - 10s 207us/step - loss: 0.6322 - acc: 0.7890 - val_loss: 0.6686 - val_acc: 0.7744\n",
            "Epoch 56/100\n",
            "50000/50000 [==============================] - 10s 208us/step - loss: 0.6248 - acc: 0.7933 - val_loss: 0.6616 - val_acc: 0.7795\n",
            "Epoch 57/100\n",
            "50000/50000 [==============================] - 10s 208us/step - loss: 0.6228 - acc: 0.7919 - val_loss: 0.7026 - val_acc: 0.7790\n",
            "Epoch 58/100\n",
            "50000/50000 [==============================] - 10s 207us/step - loss: 0.6255 - acc: 0.7917 - val_loss: 0.6615 - val_acc: 0.7806\n",
            "Epoch 59/100\n",
            "50000/50000 [==============================] - 10s 210us/step - loss: 0.6271 - acc: 0.7937 - val_loss: 0.7006 - val_acc: 0.7792\n",
            "Epoch 60/100\n",
            "50000/50000 [==============================] - 10s 207us/step - loss: 0.6267 - acc: 0.7886 - val_loss: 0.6971 - val_acc: 0.7729\n",
            "Epoch 61/100\n",
            "50000/50000 [==============================] - 10s 208us/step - loss: 0.6210 - acc: 0.7920 - val_loss: 0.6860 - val_acc: 0.7720\n",
            "Epoch 62/100\n",
            "50000/50000 [==============================] - 10s 208us/step - loss: 0.6238 - acc: 0.7923 - val_loss: 0.6516 - val_acc: 0.7822\n",
            "Epoch 63/100\n",
            "50000/50000 [==============================] - 10s 208us/step - loss: 0.6237 - acc: 0.7907 - val_loss: 0.6686 - val_acc: 0.7847\n",
            "Epoch 64/100\n",
            "50000/50000 [==============================] - 10s 207us/step - loss: 0.6250 - acc: 0.7910 - val_loss: 0.6492 - val_acc: 0.7843\n",
            "Epoch 65/100\n",
            "50000/50000 [==============================] - 10s 207us/step - loss: 0.6178 - acc: 0.7939 - val_loss: 0.7064 - val_acc: 0.7674\n",
            "Epoch 66/100\n",
            "50000/50000 [==============================] - 10s 208us/step - loss: 0.6158 - acc: 0.7965 - val_loss: 0.6639 - val_acc: 0.7735\n",
            "Epoch 67/100\n",
            "50000/50000 [==============================] - 10s 209us/step - loss: 0.6149 - acc: 0.7943 - val_loss: 0.7073 - val_acc: 0.7625\n",
            "Epoch 68/100\n",
            "50000/50000 [==============================] - 10s 207us/step - loss: 0.6193 - acc: 0.7937 - val_loss: 0.6662 - val_acc: 0.7814\n",
            "Epoch 69/100\n",
            "50000/50000 [==============================] - 10s 207us/step - loss: 0.6177 - acc: 0.7949 - val_loss: 0.6905 - val_acc: 0.7745\n",
            "Epoch 70/100\n",
            "50000/50000 [==============================] - 10s 207us/step - loss: 0.6171 - acc: 0.7955 - val_loss: 0.7178 - val_acc: 0.7836\n",
            "Epoch 71/100\n",
            "50000/50000 [==============================] - 10s 208us/step - loss: 0.6136 - acc: 0.7955 - val_loss: 0.6893 - val_acc: 0.7731\n",
            "Epoch 72/100\n",
            "50000/50000 [==============================] - 10s 207us/step - loss: 0.6119 - acc: 0.7959 - val_loss: 0.6986 - val_acc: 0.7643\n",
            "Epoch 73/100\n",
            "50000/50000 [==============================] - 10s 208us/step - loss: 0.6149 - acc: 0.7954 - val_loss: 0.6915 - val_acc: 0.7756\n",
            "Epoch 74/100\n",
            "50000/50000 [==============================] - 10s 208us/step - loss: 0.6130 - acc: 0.7956 - val_loss: 0.6721 - val_acc: 0.7757\n",
            "Epoch 75/100\n",
            "50000/50000 [==============================] - 10s 209us/step - loss: 0.6158 - acc: 0.7961 - val_loss: 0.6723 - val_acc: 0.7734\n",
            "Epoch 76/100\n",
            "50000/50000 [==============================] - 10s 207us/step - loss: 0.6185 - acc: 0.7940 - val_loss: 0.6548 - val_acc: 0.7848\n",
            "Epoch 77/100\n",
            "50000/50000 [==============================] - 10s 208us/step - loss: 0.6135 - acc: 0.7961 - val_loss: 0.6478 - val_acc: 0.7815\n",
            "Epoch 78/100\n",
            "50000/50000 [==============================] - 11s 212us/step - loss: 0.6150 - acc: 0.7944 - val_loss: 0.6675 - val_acc: 0.7730\n",
            "Epoch 79/100\n",
            "50000/50000 [==============================] - 11s 210us/step - loss: 0.6145 - acc: 0.7962 - val_loss: 0.6539 - val_acc: 0.7836\n",
            "Epoch 80/100\n",
            "50000/50000 [==============================] - 10s 208us/step - loss: 0.6158 - acc: 0.7959 - val_loss: 0.7282 - val_acc: 0.7784\n",
            "Epoch 81/100\n",
            "16096/50000 [========>.....................] - ETA: 6s - loss: 0.5905 - acc: 0.8008"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-01b9c1eb1220>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m               \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m               shuffle=True)\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B99kVR80R01G",
        "colab_type": "code",
        "outputId": "6d8caa13-24ac-4a0a-92b5-128392b42b09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        }
      },
      "source": [
        "from keras.layers import Input, Dense, Concatenate\n",
        "from keras.models import Model\n",
        "\n",
        "# This returns a tensor\n",
        "inputs = Input(shape=(784,))\n",
        "\n",
        "# a layer instance is callable on a tensor, and returns a tensor\n",
        "x = Dense(64, activation='relu')(inputs)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "\n",
        "\n",
        "# This returns a tensor\n",
        "inputs_1 = Input(shape=(10,))\n",
        "\n",
        "# a layer instance is callable on a tensor, and returns a tensor\n",
        "x_1 = Dense(64, activation='relu')(inputs_1)\n",
        "x_1 = Dense(64, activation='relu')(x_1)\n",
        "\n",
        "\n",
        "merged = Concatenate()([x,x_1])  # x=[1,2,3]  x_1=[4,3,1] Concat->[1,2,4,4,3,1]\n",
        "\n",
        "y = Dense(64, activation='relu')(merged)\n",
        "y = Dense(64, activation='relu')(y)\n",
        "\n",
        "y_plus_1 =Dense(10, activation='softmax',name='output1')(y)\n",
        "y_plus_2 =Dense(20, activation='relu', name='output2')(y)\n",
        "\n",
        "losses=dict()\n",
        "losses['output1'] ='categorical_crossentropy'\n",
        "losses['output2'] ='mse'\n",
        "\n",
        "\n",
        "metrics=dict()\n",
        "metrics['output1'] ='acc'\n",
        "metrics['output2'] ='mse'\n",
        "\n",
        "model = Model([inputs,inputs_1],[y_plus_1,y_plus_2])\n",
        "\n",
        "model.compile(optimizer='adam',loss=losses, metrics=metrics)\n",
        "\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "\n",
        "SVG(model_to_dot(model).create(prog='dot', format='svg'))\n",
        "model.summary()\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "input1= np.random.rand(1,784)\n",
        "input2= np.random.rand(1,10)\n",
        "\n",
        "\n",
        "output1,output2 = model.predict([input1,input2])\n",
        "\n",
        "display(output1.shape)\n",
        "display(output2.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_18 (InputLayer)           (None, 784)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_19 (InputLayer)           (None, 10)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_55 (Dense)                (None, 64)           50240       input_18[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_57 (Dense)                (None, 64)           704         input_19[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_56 (Dense)                (None, 64)           4160        dense_55[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_58 (Dense)                (None, 64)           4160        dense_57[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 128)          0           dense_56[0][0]                   \n",
            "                                                                 dense_58[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_59 (Dense)                (None, 64)           8256        concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_60 (Dense)                (None, 64)           4160        dense_59[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "output1 (Dense)                 (None, 10)           650         dense_60[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "output2 (Dense)                 (None, 20)           1300        dense_60[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 73,630\n",
            "Trainable params: 73,630\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(1, 10)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(1, 20)"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2l8U7xPUSer1",
        "colab_type": "code",
        "outputId": "c474f352-63a8-49b2-cb71-d09dbe037fae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "import numpy as np\n",
        "\n",
        "data_dim = 16\n",
        "timesteps = 8\n",
        "num_classes = 10\n",
        "\n",
        "# expected input data shape: (batch_size, timesteps, data_dim)\n",
        "model = Sequential()\n",
        "model.add(LSTM(32, return_sequences=True,\n",
        "               input_shape=(timesteps, data_dim)))  # returns a sequence of vectors of dimension 32\n",
        "model.add(LSTM(32, return_sequences=True))  # returns a sequence of vectors of dimension 32\n",
        "model.add(LSTM(32))  # return a single vector of dimension 32\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_1 (LSTM)                (None, 8, 32)             6272      \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 8, 32)             8320      \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 32)                8320      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                330       \n",
            "=================================================================\n",
            "Total params: 23,242\n",
            "Trainable params: 23,242\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciUtGM-W_ehP",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}