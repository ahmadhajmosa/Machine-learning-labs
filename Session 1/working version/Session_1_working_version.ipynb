{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Session 1 working version.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgWZldyi-Zef",
        "colab_type": "text"
      },
      "source": [
        "# Lab on Machine Learning and Applications in Intelligent Vehicles\n",
        "## Session 1: Introduction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1T4XAd1_fJk",
        "colab_type": "text"
      },
      "source": [
        "### Course Plan:\n",
        "\n",
        "This lab is a continuation to the Machine learning lecture, the objective of this course is to learn how to model and implement neural networks using deep learning python frameworks: \n",
        "1. Tensorflow\n",
        "2. Keras\n",
        "3. Pytorch\n",
        "\n",
        "Useing these frameworks, we will go through the following topics and use cases:\n",
        "\n",
        "#### Session 1: 05.06 - 09:00 - 11:00  :\n",
        "\n",
        ">#### Presentation:  05.06 - 09:00 - 10:30:\n",
        ">1. Introduction to deep learning frameworks\n",
        "2. Deep learning in Numpy\n",
        "\n",
        ">#### Break:  05.06 - 10:30 - 10:45\n",
        ">#### Assignment:  05.06 - 10:45 - 11:45 : \n",
        ">* Implementation of backprop using numpy\n",
        "\n",
        ">1. Tensorflow backround \n",
        "2. Implementation of feedforward neural networks using Tensorflow\n",
        "\n",
        ">#### Break:  05.06 - 10:00 - 10:15\n",
        ">#### Assignment:  05.06 - 10:15 - 10:45 : \n",
        "  \n",
        "  >* play around with tensorflow and build your first neural network\n",
        "\n",
        "\n",
        "\n",
        "#### Session 2:  05.06 - 11:00 - 12:00  :\n",
        "\n",
        ">#### Presentation:  05.06 - 11:00 - 12:00:\n",
        "\n",
        "\n",
        ">3. Implementation of CNN useing Tensorflow\n",
        "4. Tensorboard\n",
        "\n",
        ">#### Break:  05.06 - 12:00 - 13:00\n",
        ">#### Assignment:  05.06 - 13:00 - 13:30 :\n",
        "\n",
        "\n",
        " #### Session 3:  05.06 - 13:30 - 15:00  :\n",
        "\n",
        ">#### Presentation:  05.06 - 13:00 - 14:00:\n",
        "\n",
        ">5. Introduction to Keras\n",
        "6. CNN using Keras\n",
        ">#### Break:  05.06 - 14:00 - 14:15\n",
        ">#### Assignment:  05.06 - 14:15 - 14:45 :\n",
        "\n",
        " #### Session 4:  05.06 - 15:00 - 16:30  :\n",
        "\n",
        ">#### Presentation:  05.06 - 15:00 - 15:45:\n",
        "\n",
        ">7. LSTM using Keras\n",
        "\n",
        ">#### Break:  05.06 - 15:45 - 16:00\n",
        ">#### Assignment:  05.06 - 16:00 - 16:30 :\n",
        "\n",
        "\n",
        "8. VGG, Inceptaion, ResNet models using Keras\n",
        "\n",
        "9. Autoencoders using Keras\n",
        "\n",
        "10. Sequence to Sequence Models using Keras \n",
        "\n",
        "11. Attention Mechanism using Keras\n",
        "12. GANS \n",
        "13. Introduction to Pytorch\n",
        "14. Machine translation using Pytorch\n",
        "15. Introduction to Allennlp\n",
        "16. Deep Reinforcment Learning using Keras\n",
        "17. Use case: building self driving car using Unity and tensorflow\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a75NLOuwKXSs",
        "colab_type": "text"
      },
      "source": [
        "#Session 1: 05.06 - 09:00 - 11:00 :\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciUtGM-W_ehP",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "## Deep learning frameworks:\n",
        "\n",
        "In the past decade, many deep learning frameworks have been developed to ease and scale the research and development of AI. Many big technology providers including Google, IBM, Microsoft and Facebook have entered the race to provide the best and most popluar frameworks. To enter such a race, mainly four features are considered in the provided framework:\n",
        "\n",
        "1. Uses a popular language for data scientists (Python, Scale, C++ or R)\n",
        "2. Flexible in creating and adjusting deep learning architectures -> Functional programming\n",
        "3. Easy for computing gradients\n",
        "4. Interface with GPUs for parallel processing\n",
        "\n",
        "\n",
        "In the following we see the most popular provided deep learning frameworks with their providers\n",
        "https://towardsdatascience.com/deep-learning-framework-power-scores-2018-23607ddf297a\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/1600/1*dYjDEI0mLpsCOySKUuX1VA.png)\n",
        "\n",
        "## Tensorflow\n",
        "TensorFlow is the undisputed heavyweight champion. It has the most GitHub activity, Google searches, Medium articles, books on Amazon and ArXiv articles. It also has the most developers using it and is listed in the most online job descriptions. TensorFlow is backed by Google.\n",
        "\n",
        "## Keras\n",
        "Keras has an “API designed for human beings, not machines.” It is the second most popular framework in nearly all evaluation areas. Keras sits on top of TensorFlow, Theano, or CNTK. Start with Keras if you are new to deep learning.\n",
        "\n",
        "## Pytorch\n",
        "\n",
        "PyTorch is the third most popular overall framework and the second most popular stand-alone framework. It is younger than TensorFlow and has grown rapidly in popularity. It allows customization that TensorFlow does not. It has the backing of Facebook.\n",
        "\n",
        "## Theano\n",
        "\n",
        "Theano was developed at the University of Montreal in 2007 and is the oldest significant Python deep learning framework. It has lost much of its popularity and its leader stated that major releases were no longer on the roadmap. However, updates continue to be made. Theano still the fifth highest scoring framework.\n",
        "\n",
        "# Comparision:\n",
        "\n",
        "## Criteria 1: Online Job Listings\n",
        "\n",
        "TensorFlow is the clear winner when it comes to frameworks mentioned in job listings. Learn it if you want a job doing deep learning.\n",
        "\n",
        ">> ![alt text](https://cdn-images-1.medium.com/max/800/1*kA8dIgRqLdWgKgs2sd7clA.png)\n",
        "\n",
        "## Criteria 2: Usage\n",
        "\n",
        "\n",
        "Keras showed a surprising amount of use — nearly as much as TensorFlow. It’s interesting that US employers are overwhelmingly looking for TensorFlow skills, when — at least internationally — Keras is used almost as frequently.\n",
        "\n",
        "  >> ![alt text](https://cdn-images-1.medium.com/max/800/1*6b2Tm0oFr99zKaxkr4g52w.png)\n",
        "  \n",
        "  \n",
        "## Criteria 4: Google Search Activity\n",
        "\n",
        ">> ![alt text](https://cdn-images-1.medium.com/max/800/1*wpA6C9x_Nri42EzbqnS1fQ.png)\n",
        "\n",
        "## Criteria 5: Medium Articles\n",
        "\n",
        ">> ![alt text](https://cdn-images-1.medium.com/max/800/1*Y1cJUgZTC3u_hU1lmdsroQ.png)\n",
        "\n",
        "\n",
        "## Criteria 6: Amazon Books\n",
        "\n",
        ">> ![alt text](https://cdn-images-1.medium.com/max/800/1*mtP47q29T6agdkEoJcCw-w.png)\n",
        "\n",
        "## Criteria 7 : ArXiv Articles\n",
        "\n",
        "  >> ![alt text](https://cdn-images-1.medium.com/max/800/1*7jdNoNkGlx_1EnfevQXRvw.png)\n",
        "  \n",
        "## Criteria 8: GitHub Activity\n",
        "\n",
        "  >> ![alt text](https://cdn-images-1.medium.com/max/800/1*eOOYV3C5klsuVBMX31ngxw.png)\n",
        "  \n",
        " --------------------------------\n",
        " \n",
        "\n",
        "\n",
        "  \n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9v3V8ztAKDCg",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "\n",
        "Before we jump into Tensorflow, we will implemented our first neural network model using Python Numpy package. NumPy is the fundamental package for scientific computing with Python, such as:\n",
        "\n",
        "1. Linear Algebra\n",
        "2. Statistics\n",
        "3. Calculus\n",
        "\n",
        "## A brief intro to Numpy operations:\n",
        "\n",
        "1. Creating a Vector:\n",
        "Here we use Numpy to create a 1-D Array which we then call a vector.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTf7M4r7Lgj9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "1b05a6d9-8721-4460-ccf5-422e112d7ebe"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "v1=np.zeros((1,100))\n",
        "#display(v1)\n",
        "#display(v1.shape)   \n",
        "#display(v1.T.shape) #transpose\n",
        "\n",
        "v1=np.ones((1,100))\n",
        "#display(v1)\n",
        "\n",
        "v1=np.random.randn(100,1) #normal distributed random variable 0 mean, 1 standard deviation\n",
        "v1=np.random.rand(100,1)\n",
        "#display(v1)\n",
        "\n",
        "display('mean:',v1.mean())\n",
        "display('std: ',v1.std())\n",
        "display('max: ',v1.max())\n",
        "display('min: ',v1.min())\n",
        "\n",
        "\n",
        "     "
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'mean:'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.5162576846562591"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'std: '"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.29658603209996637"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'max: '"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.9949323699475169"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'min: '"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.0014838949064867046"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYFjSo0OLqA3",
        "colab_type": "text"
      },
      "source": [
        "2. Creating a Matrix\n",
        "We Create a 2-D Array in Numpy and call it a Matrix. It contains 2 rows and 3 columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJlDBq5rLmA-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ad36345d-a0da-419a-b519-795b222a9049"
      },
      "source": [
        "m1=np.zeros((2,3))\n",
        "m1=np.array([[1,2,3],[4,5,6]])\n",
        "display(m1)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[1, 2, 3],\n",
              "       [4, 5, 6]])"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wv99hZqULygH",
        "colab_type": "text"
      },
      "source": [
        "3. Selecting Elements\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLQlxFzkPrKM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "f6e7f8d0-09cf-48ac-9e20-5cb138769d94"
      },
      "source": [
        "display(m1[0]) #first row\n",
        "\n",
        "display(m1[1,1]) #element at 1,1\n",
        "\n",
        "display(m1[0,-1]) #last element of the first row\n",
        "\n",
        "display(m1[0,-2]) #second last element of the first row\n",
        "\n",
        "display(m1[0,1:]) #last two elements of the first row\n",
        "\n",
        "display(m1[0,:-1]) #first tow elements of the first row\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([1, 2, 3])"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([2, 3])"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([1, 2])"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QO3vTGEKQhm7",
        "colab_type": "text"
      },
      "source": [
        "4. Describing a Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8bDjBhhQpg5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "191e4fc6-944c-4184-9a0d-1a3a4975ab42"
      },
      "source": [
        "display(m1.shape)\n",
        "display(m1.size) #number of elements\n",
        "display(m1.ndim) #dimensions"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(2, 3)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKISvY8kQtA0",
        "colab_type": "text"
      },
      "source": [
        "5. Finding the max and min values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abPJd0JrQ4mM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "6e3172e7-c99d-497c-d282-b5db203cc096"
      },
      "source": [
        "#same as for vectors\n",
        "m1.min(axis=0) #smallest of each row\n",
        "\n",
        "#or\n",
        "\n",
        "np.min(m1,axis=0)\n",
        "np.max(m1,axis=0)\n",
        "\n",
        "#to get the index of the row\n",
        "np.argmax(m1,axis=0)\n",
        "\n",
        "#to get the index of the max value\n",
        "np.argmax(m1)\n",
        "\n",
        "#search for condition\n",
        "np.argwhere(m1<np.mean(m1))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0],\n",
              "       [0, 1],\n",
              "       [0, 2]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Qm64s_eR0zQ",
        "colab_type": "text"
      },
      "source": [
        "6. Reshaping Arrays\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pwepq7h_SBBD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3873e17b-5216-49b9-856e-986d679ca5c5"
      },
      "source": [
        "m2=np.array([[1,2,3],[4,5,6]])\n",
        "m2.shape\n",
        "m2.reshape((3,2)) #same as transpose\n",
        "m2.reshape((6,1)) #flatten (column)\n",
        "m2.reshape((1,6)) #also flatten but row vector\n",
        "m2.reshape((1,2,3)) #dim 0 size 1 dim 1 size 2 dim 2 size 3\n",
        "m2.reshape((1,-1,3)) #hmmm"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[1, 2, 3],\n",
              "        [4, 5, 6]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJU3xABuVem_",
        "colab_type": "text"
      },
      "source": [
        "7. Calculating Dot Products"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPKg382VVivy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c5600950-a86d-45bc-c81f-8a1ae7f8f206"
      },
      "source": [
        "v1=np.array([1,2,3])\n",
        "v2=np.array([4,5,6])\n",
        "\n",
        "display(np.dot(v1,v2)) #dot product\n",
        "display(v1 @ v2) #also dot product but shorter"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cB-jK7jEXY7F",
        "colab_type": "text"
      },
      "source": [
        "##Linear regression in Numpy:\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Write the numpy code for the following model:\n",
        "\n",
        "$Y=WX+B$\n",
        "\n",
        "where $X$ is 3x10 matrix:  10 samples and 3 features\n",
        "\n",
        "$Y$ is 4x10 matrix: 10 samples and 4 outputs\n",
        "\n",
        "$W$ is the weights matrix with the shape 4x3: connecting 3 inputs to 4 outputs\n",
        "\n",
        "$b$ is a vector with a size 4 ( one bias per output)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EtM5LVtWCpm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2eee4d86-b69c-44c1-bb5b-030689fcc040"
      },
      "source": [
        "#input for feature matrix\n",
        "X=np.random.rand(3,10)\n",
        "\n",
        "#weight matrix\n",
        "W=np.random.rand(4,3)\n",
        "\n",
        "#bias vector\n",
        "b=np.zeros((4,1))   # not the same as np.zeros(4,) is a vector than\n",
        "\n",
        "#regression model\n",
        "Y=np.dot(W,X)+b\n",
        "display('Y shape: ',Y.shape)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Y shape: '"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(4, 10)"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMIoucH9hFfr",
        "colab_type": "text"
      },
      "source": [
        "## One neuron model in numpy:\n",
        "\n",
        "A single neuron has multiple inputs and one output, in addition to the linear regression model, we need to add non linearity through an activation function:\n",
        "\n",
        "$Y= f(WX+B)$\n",
        "\n",
        "where $X$ is n x m matrix:  m samples and n features/inputs\n",
        "\n",
        "$f(g)= \\frac{1}{1+\\exp(-g)}$  is a sigmoid acitavation function\n",
        "\n",
        "$Y$ is nh1 x m matrix: m samples and ny outputs\n",
        "\n",
        "$W$ is the weights matrix with the shape nh1 x n: connecting 3 inputs to 4 outputs\n",
        "\n",
        "$b$ is a vector with a size nh1 ( one bias per output)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qry1JDGEiLmx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#each neuron has one output but three inputs therefore we need four neurons\n",
        "\n",
        "num_inputs=3\n",
        "num_outputs=4\n",
        "num_samples=100\n",
        "\n",
        "f= lambda x: 1.0/(1.0+np.exp(-x)) # activation function\n",
        "\n",
        "#or we can write like that\n",
        "#def f(x):\n",
        "#  return 1.0/(1.0+np.exp(-x))\n",
        "\n",
        "#input for feature matrix\n",
        "X=np.random.rand(num_inputs,num_samples)\n",
        "\n",
        "#weight matrix\n",
        "W=np.random.rand(num_outputs,num_inputs)\n",
        "\n",
        "#bias vector\n",
        "b=np.random.rand(num_outputs,1)   # not the same as np.zeros(4,) is a vector than\n",
        "\n",
        "#regression model\n",
        "Y=f(np.dot(W,X)+b)   #only difference to before\n",
        "display('Y shape: ',Y.shape)\n",
        "display('Y: ',Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSnbti9ooIIs",
        "colab_type": "text"
      },
      "source": [
        "## Two hidden layer model in numpy:\n",
        "\n",
        "The difference from the one neuron model is simple:  we need only to change the number of output \"ny\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAY3o6zBnpA0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "195f9c76-6846-4095-caf4-0e6604c0bb89"
      },
      "source": [
        "#each neuron has one output but three inputs therefore we need four neurons\n",
        "\n",
        "num_inputs=3\n",
        "num_hidden_outputs=4\n",
        "num_outputs=3\n",
        "num_samples=100\n",
        "\n",
        "f= lambda x: 1.0/(1.0+np.exp(-x)) # activation function\n",
        "\n",
        "#or we can write like that\n",
        "#def f(x):\n",
        "#  return 1.0/(1.0+np.exp(-x))\n",
        "\n",
        "#input for feature matrix\n",
        "X=np.random.rand(num_inputs,num_samples)\n",
        "\n",
        "#weight matrix\n",
        "W1=np.random.rand(num_hidden_outputs,num_inputs)\n",
        "W2=np.random.rand(num_outputs,num_hidden)\n",
        "\n",
        "#bias vector\n",
        "b1=np.random.rand(num_hidden_outputs,1)   # not the same as np.zeros(4,) is a vector than\n",
        "b2=np.random.rand(num_outputs,1)\n",
        "\n",
        "#hidden layer 1 output\n",
        "Y1=f(np.dot(W1,X)+b1)   #only difference to before\n",
        "display('Y1 shape: ',Y1.shape)\n",
        "#display('Y1: ',Y1)\n",
        "\n",
        "#output (layer 2)\n",
        "Y=f(np.dot(W2,Y1)+b2)\n",
        "display('Y shape: ',Y.shape)\n",
        "#display('Y: ',Y)\n"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Y1 shape: '"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(4, 100)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Y shape: '"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(3, 100)"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11fqb_bQvIEi",
        "colab_type": "text"
      },
      "source": [
        "## Gradient descent in Numpy:\n",
        "Let us now start training a neural network\n",
        "We start by implementing a simple gradient descent for linear regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzBJxwb7FFZ2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2941
        },
        "outputId": "bbe53485-99c8-4ff5-c656-b96d1a4eded0"
      },
      "source": [
        "m = 10 #number of samples\n",
        "ni=1   #num of input layers\n",
        "h=1    #num of hidden layers\n",
        "no=1   #num of output layers\n",
        "\n",
        "#Input vector\n",
        "X=np.random.rand(m)\n",
        "\n",
        "#Learning rate\n",
        "alpha=0.01\n",
        "\n",
        "#Early stop criteria\n",
        "ep=0.001\n",
        "\n",
        "#Maximum iteration\n",
        "max_iter=1000\n",
        "\n",
        "#Generate a random wight for output\n",
        "W1=np.random.rand()\n",
        "\n",
        "#Generate the bias\n",
        "b1=np.random.rand()\n",
        "\n",
        "#Generate a random ground truth\n",
        "Y_gr=np.random.rand(m)\n",
        "\n",
        "J=sum([(b1+W1*X[i]-Y_gr[i])**2 for i in range(m)]) #Cost function sum of \n",
        "\n",
        "converged=False\n",
        "iter=0\n",
        "\n",
        "while not converged:\n",
        "  grad0=1.0/m*sum([(b1+W1*X[i]-Y_gr[i]) for i in range(m)])\n",
        "  grad1=1.0/m*sum([(b1+W1*X[i]-Y_gr[i])*X[i] for i in range(m)])\n",
        "  \n",
        "  #update the theta_temp\n",
        "  temp0=b1-alpha*grad0\n",
        "  temp1=W1-alpha*grad1\n",
        "  \n",
        "  #update theta\n",
        "  b1=temp0\n",
        "  W1=temp1\n",
        "  \n",
        "  #sum squared error\n",
        "  e=sum([(b1+W1*X[i]-Y_gr[i])**2 for i in range(m)])\n",
        "  \n",
        "  if abs(J-e)<=ep:\n",
        "    print('Converged, iterations: ',iter,'!!!')\n",
        "    converged=True;\n",
        "    \n",
        "  J=e #update error\n",
        "  print('Error at iteration ',iter,' is ',J)\n",
        "  iter +=1\n",
        "  \n",
        "  if iter==max_iter:\n",
        "    print('Maximum iterations reached!')\n",
        "    converged=True"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error at iteration  0  is  3.551982878625796\n",
            "Error at iteration  1  is  3.4753354358128856\n",
            "Error at iteration  2  is  3.400659250043203\n",
            "Error at iteration  3  is  3.3279035876289145\n",
            "Error at iteration  4  is  3.257019020632688\n",
            "Error at iteration  5  is  3.1879573932611094\n",
            "Error at iteration  6  is  3.120671789123052\n",
            "Error at iteration  7  is  3.0551164993307154\n",
            "Error at iteration  8  is  2.9912469914216744\n",
            "Error at iteration  9  is  2.9290198790807827\n",
            "Error at iteration  10  is  2.868392892641361\n",
            "Error at iteration  11  is  2.809324850345604\n",
            "Error at iteration  12  is  2.7517756303446683\n",
            "Error at iteration  13  is  2.6957061434194065\n",
            "Error at iteration  14  is  2.6410783064031955\n",
            "Error at iteration  15  is  2.5878550162887954\n",
            "Error at iteration  16  is  2.536000125001623\n",
            "Error at iteration  17  is  2.4854784148223024\n",
            "Error at iteration  18  is  2.4362555744417698\n",
            "Error at iteration  19  is  2.3882981756326562\n",
            "Error at iteration  20  is  2.3415736505210902\n",
            "Error at iteration  21  is  2.296050269443465\n",
            "Error at iteration  22  is  2.251697119373112\n",
            "Error at iteration  23  is  2.2084840829022134\n",
            "Error at iteration  24  is  2.1663818177646705\n",
            "Error at iteration  25  is  2.1253617368859956\n",
            "Error at iteration  26  is  2.085395988946666\n",
            "Error at iteration  27  is  2.0464574394457316\n",
            "Error at iteration  28  is  2.0085196522517865\n",
            "Error at iteration  29  is  1.9715568716287817\n",
            "Error at iteration  30  is  1.9355440047244352\n",
            "Error at iteration  31  is  1.900456604509352\n",
            "Error at iteration  32  is  1.8662708531552417\n",
            "Error at iteration  33  is  1.8329635458409363\n",
            "Error at iteration  34  is  1.8005120749752055\n",
            "Error at iteration  35  is  1.7688944148256276\n",
            "Error at iteration  36  is  1.738089106543077\n",
            "Error at iteration  37  is  1.7080752435716382\n",
            "Error at iteration  38  is  1.678832457434036\n",
            "Error at iteration  39  is  1.6503409038829064\n",
            "Error at iteration  40  is  1.6225812494084986\n",
            "Error at iteration  41  is  1.5955346580936338\n",
            "Error at iteration  42  is  1.5691827788069843\n",
            "Error at iteration  43  is  1.5435077327259608\n",
            "Error at iteration  44  is  1.5184921011807275\n",
            "Error at iteration  45  is  1.4941189138110849\n",
            "Error at iteration  46  is  1.4703716370281597\n",
            "Error at iteration  47  is  1.4472341627730583\n",
            "Error at iteration  48  is  1.42469079756485\n",
            "Error at iteration  49  is  1.402726251830419\n",
            "Error at iteration  50  is  1.381325629508943\n",
            "Error at iteration  51  is  1.3604744179239197\n",
            "Error at iteration  52  is  1.3401584779158675\n",
            "Error at iteration  53  is  1.3203640342289804\n",
            "Error at iteration  54  is  1.3010776661452057\n",
            "Error at iteration  55  is  1.282286298359374\n",
            "Error at iteration  56  is  1.2639771920891856\n",
            "Error at iteration  57  is  1.2461379364139935\n",
            "Error at iteration  58  is  1.2287564398365118\n",
            "Error at iteration  59  is  1.2118209220616973\n",
            "Error at iteration  60  is  1.1953199059872273\n",
            "Error at iteration  61  is  1.1792422099001105\n",
            "Error at iteration  62  is  1.1635769398741522\n",
            "Error at iteration  63  is  1.1483134823630747\n",
            "Error at iteration  64  is  1.1334414969842785\n",
            "Error at iteration  65  is  1.1189509094883299\n",
            "Error at iteration  66  is  1.1048319049093926\n",
            "Error at iteration  67  is  1.091074920891953\n",
            "Error at iteration  68  is  1.0776706411892947\n",
            "Error at iteration  69  is  1.0646099893293073\n",
            "Error at iteration  70  is  1.0518841224433189\n",
            "Error at iteration  71  is  1.0394844252537614\n",
            "Error at iteration  72  is  1.0274025042165722\n",
            "Error at iteration  73  is  1.0156301818143607\n",
            "Error at iteration  74  is  1.0041594909964502\n",
            "Error at iteration  75  is  0.9929826697620192\n",
            "Error at iteration  76  is  0.982092155882658\n",
            "Error at iteration  77  is  0.9714805817607545\n",
            "Error at iteration  78  is  0.9611407694202108\n",
            "Error at iteration  79  is  0.9510657256260839\n",
            "Error at iteration  80  is  0.9412486371298363\n",
            "Error at iteration  81  is  0.9316828660369603\n",
            "Error at iteration  82  is  0.9223619452938279\n",
            "Error at iteration  83  is  0.9132795742906957\n",
            "Error at iteration  84  is  0.9044296145778801\n",
            "Error at iteration  85  is  0.8958060856921841\n",
            "Error at iteration  86  is  0.8874031610907409\n",
            "Error at iteration  87  is  0.8792151641895112\n",
            "Error at iteration  88  is  0.8712365645037393\n",
            "Error at iteration  89  is  0.8634619738877419\n",
            "Error at iteration  90  is  0.8558861428714764\n",
            "Error at iteration  91  is  0.8485039570913945\n",
            "Error at iteration  92  is  0.8413104338131591\n",
            "Error at iteration  93  is  0.8343007185438548\n",
            "Error at iteration  94  is  0.8274700817313925\n",
            "Error at iteration  95  is  0.8208139155488605\n",
            "Error at iteration  96  is  0.8143277307616432\n",
            "Error at iteration  97  is  0.8080071536751642\n",
            "Error at iteration  98  is  0.8018479231611937\n",
            "Error at iteration  99  is  0.7958458877606869\n",
            "Error at iteration  100  is  0.7899970028611875\n",
            "Error at iteration  101  is  0.7842973279468803\n",
            "Error at iteration  102  is  0.7787430239194182\n",
            "Error at iteration  103  is  0.7733303504877023\n",
            "Error at iteration  104  is  0.7680556636248478\n",
            "Error at iteration  105  is  0.762915413090594\n",
            "Error at iteration  106  is  0.757906140017486\n",
            "Error at iteration  107  is  0.7530244745591801\n",
            "Error at iteration  108  is  0.7482671335992739\n",
            "Error at iteration  109  is  0.7436309185191096\n",
            "Error at iteration  110  is  0.7391127130230221\n",
            "Error at iteration  111  is  0.7347094810195655\n",
            "Error at iteration  112  is  0.7304182645572658\n",
            "Error at iteration  113  is  0.7262361818135075\n",
            "Error at iteration  114  is  0.7221604251351748\n",
            "Error at iteration  115  is  0.7181882591297275\n",
            "Error at iteration  116  is  0.7143170188054028\n",
            "Error at iteration  117  is  0.7105441077592872\n",
            "Error at iteration  118  is  0.7068669964120194\n",
            "Error at iteration  119  is  0.7032832202879293\n",
            "Error at iteration  120  is  0.6997903783394419\n",
            "Error at iteration  121  is  0.6963861313146045\n",
            "Error at iteration  122  is  0.6930682001666324\n",
            "Error at iteration  123  is  0.6898343645043863\n",
            "Error at iteration  124  is  0.6866824610827313\n",
            "Error at iteration  125  is  0.68361038233175\n",
            "Error at iteration  126  is  0.6806160749238105\n",
            "Error at iteration  127  is  0.6776975383775123\n",
            "Error at iteration  128  is  0.6748528236975655\n",
            "Error at iteration  129  is  0.6720800320496738\n",
            "Error at iteration  130  is  0.6693773134695244\n",
            "Error at iteration  131  is  0.6667428656050024\n",
            "Error at iteration  132  is  0.6641749324907811\n",
            "Error at iteration  133  is  0.6616718033544478\n",
            "Error at iteration  134  is  0.6592318114533573\n",
            "Error at iteration  135  is  0.6568533329414245\n",
            "Error at iteration  136  is  0.6545347857650791\n",
            "Error at iteration  137  is  0.6522746285876373\n",
            "Error at iteration  138  is  0.6500713597413577\n",
            "Error at iteration  139  is  0.6479235162064676\n",
            "Error at iteration  140  is  0.6458296726164674\n",
            "Error at iteration  141  is  0.6437884402890343\n",
            "Error at iteration  142  is  0.6417984662818695\n",
            "Error at iteration  143  is  0.6398584324728459\n",
            "Error at iteration  144  is  0.6379670546638279\n",
            "Error at iteration  145  is  0.6361230817075585\n",
            "Error at iteration  146  is  0.634325294657018\n",
            "Error at iteration  147  is  0.6325725059366749\n",
            "Error at iteration  148  is  0.6308635585350675\n",
            "Error at iteration  149  is  0.6291973252181663\n",
            "Error at iteration  150  is  0.6275727077629814\n",
            "Error at iteration  151  is  0.6259886362108962\n",
            "Error at iteration  152  is  0.6244440681402176\n",
            "Error at iteration  153  is  0.6229379879574487\n",
            "Error at iteration  154  is  0.6214694062068027\n",
            "Error at iteration  155  is  0.6200373588974877\n",
            "Error at iteration  156  is  0.618640906848306\n",
            "Error at iteration  157  is  0.6172791350491205\n",
            "Error at iteration  158  is  0.6159511520387568\n",
            "Error at iteration  159  is  0.6146560892989162\n",
            "Error at iteration  160  is  0.6133931006636857\n",
            "Error at iteration  161  is  0.6121613617442488\n",
            "Error at iteration  162  is  0.6109600693684006\n",
            "Error at iteration  163  is  0.609788441034489\n",
            "Error at iteration  164  is  0.6086457143794093\n",
            "Error at iteration  165  is  0.6075311466602935\n",
            "Error at iteration  166  is  0.6064440142495364\n",
            "Error at iteration  167  is  0.6053836121428198\n",
            "Error at iteration  168  is  0.6043492534798003\n",
            "Error at iteration  169  is  0.603340269077131\n",
            "Converged, iterations:  170 !!!\n",
            "Error at iteration  170  is  0.6023560069735032\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaQyLoxk2FyG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRqCCVlmFIZl",
        "colab_type": "text"
      },
      "source": [
        "##Assignment 1\n",
        "### Backpropagation in Numpy:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCONVhXJSFNN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3468
        },
        "outputId": "f2182398-da44-4337-f6ad-6fd66b095350"
      },
      "source": [
        "num_inputs = 4\n",
        "hidden_layer_1_outputs = 4\n",
        "hidden_layer_2_outputs = 3\n",
        "\n",
        "num_samples = 10\n",
        "\n",
        "f = lambda x: 1.0/(1.0 + np.exp(-x)) # activation function (use sigmoid)\n",
        "\n",
        "\n",
        "X = np.random.rand(num_samples,num_inputs-1) # 10x4\n",
        "\n",
        "X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
        "\n",
        "display(X.shape)\n",
        "\n",
        "# Generate a random weights vector for the first hidden layer\n",
        "W_h1 = np.random.rand(num_inputs,hidden_layer_1_outputs) # 4x4\n",
        "\n",
        "# Generate a random weights vector for the second hidden layer\n",
        "W_h2 = np.random.rand(hidden_layer_1_outputs+1,hidden_layer_2_outputs)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Calculate output of hidden layer 1\n",
        "h1= f(np.dot(X,W_h1))\n",
        "display(h1.shape)\n",
        "\n",
        "# Calculate output of hidden layer 2\n",
        "#h2= f(np.dot(h1,W_h2))\n",
        "#display(h2.shape)\n",
        "\n",
        "\n",
        "# learning rate\n",
        "alpha =0.01\n",
        "\n",
        "# early stop criteria \n",
        "ep=0.001\n",
        "\n",
        "\n",
        "# maximum number of training iterations\n",
        "max_iter=100\n",
        "\n",
        "# Generate a random ground truth\n",
        "Y_gr = np.random.rand(hidden_layer_2_outputs,num_samples)\n",
        "\n",
        "#J = sum([(h2 - Y_gr[:,i])**2 for i in range(num_samples)]) #cost function sum of squared error\n",
        "print(J)\n",
        "\n",
        "#while not converged:\n",
        "for i in range(100):\n",
        "        #print('Interations:' , iter)\n",
        "    \n",
        "        # forward prop   \n",
        "        # Calculate output of hidden layer 1\n",
        "        h1= f(np.dot(X,W_h1))\n",
        "        \n",
        "        # append a column with ones representing bias inputs\n",
        "        h1 = np.hstack((np.ones((h1.shape[0], 1)), h1))\n",
        "        # Calculate output of hidden layer 2\n",
        "        h2= f(np.dot(h1,W_h2))\n",
        "        \n",
        "        \n",
        "        #error/error \n",
        "               \n",
        "        J = np.sum(np.square(h2- Y_gr.T))\n",
        "        \n",
        "        # gradient of the output layer\n",
        "        grad_h2 = h2*J\n",
        "        \n",
        "       \n",
        "      \n",
        "        \n",
        "        # error\n",
        "        \n",
        "        #display('h1', (h1[:, 1:] * (1 - h1[:, 1:])).shape)\n",
        "        \n",
        "        \n",
        "        #display('dot ', np.dot(grad_h2, W_h2.T[:, 1:]).shape)\n",
        "\n",
        "\n",
        "        hidden_error = (h1[:, 1:] * (1 - h1[:, 1:])) * np.dot(grad_h2, W_h2.T[:, 1:]) \n",
        "\n",
        "        #print(h1.shape)\n",
        "        #print(hidden_error.shape)\n",
        "        grad_h1= h1[:, :, np.newaxis]*hidden_error[:, np.newaxis, :]\n",
        "\n",
        "      \n",
        "        # average gradient\n",
        "        total_hidden_gradient_h2 = np.average(grad_h2, axis=0)\n",
        "        total_output_gradient_h1 = np.average(grad_h1, axis=0)\n",
        "        #print(grad_h1.shape)\n",
        "        #print(W_h1.shape)\n",
        "\n",
        "        # update weights\n",
        "        W_h2 += - alpha * total_hidden_gradient_h2\n",
        "        W_h1 += - alpha * total_output_gradient_h1[1:,:]\n",
        "        \n",
        "        \n",
        "        print('iter',i)\n",
        "        \n",
        "        print('cost',J)"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(10, 4)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(10, 4)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "230.63267595403136\n",
            "iter 0\n",
            "cost 6.967904327155814\n",
            "iter 1\n",
            "cost 6.253331561872703\n",
            "iter 2\n",
            "cost 5.634467242421168\n",
            "iter 3\n",
            "cost 5.108369592183296\n",
            "iter 4\n",
            "cost 4.666187962033291\n",
            "iter 5\n",
            "cost 4.296961012768637\n",
            "iter 6\n",
            "cost 3.98974555617948\n",
            "iter 7\n",
            "cost 3.734629803544964\n",
            "iter 8\n",
            "cost 3.5230915390583424\n",
            "iter 9\n",
            "cost 3.3480152269611914\n",
            "iter 10\n",
            "cost 3.2035564230386653\n",
            "iter 11\n",
            "cost 3.0849553444945914\n",
            "iter 12\n",
            "cost 2.9883491932902007\n",
            "iter 13\n",
            "cost 2.9106041049909894\n",
            "iter 14\n",
            "cost 2.8491730517914897\n",
            "iter 15\n",
            "cost 2.8019793217617157\n",
            "iter 16\n",
            "cost 2.767322537758351\n",
            "iter 17\n",
            "cost 2.743803475799404\n",
            "iter 18\n",
            "cost 2.730264101779923\n",
            "iter 19\n",
            "cost 2.725739725904436\n",
            "iter 20\n",
            "cost 2.7294207237281856\n",
            "iter 21\n",
            "cost 2.740621784240535\n",
            "iter 22\n",
            "cost 2.758757082003074\n",
            "iter 23\n",
            "cost 2.7833201266652754\n",
            "iter 24\n",
            "cost 2.8138673267548486\n",
            "iter 25\n",
            "cost 2.8500045269785708\n",
            "iter 26\n",
            "cost 2.891375950940296\n",
            "iter 27\n",
            "cost 2.9376551143270233\n",
            "iter 28\n",
            "cost 2.988537375565789\n",
            "iter 29\n",
            "cost 3.043733868339421\n",
            "iter 30\n",
            "cost 3.1029666183567115\n",
            "iter 31\n",
            "cost 3.165964689459526\n",
            "iter 32\n",
            "cost 3.2324612347200934\n",
            "iter 33\n",
            "cost 3.302191349218775\n",
            "iter 34\n",
            "cost 3.374890634833284\n",
            "iter 35\n",
            "cost 3.450294395433582\n",
            "iter 36\n",
            "cost 3.52813738494442\n",
            "iter 37\n",
            "cost 3.6081540321986774\n",
            "iter 38\n",
            "cost 3.69007906657129\n",
            "iter 39\n",
            "cost 3.7736484680867126\n",
            "iter 40\n",
            "cost 3.858600665868802\n",
            "iter 41\n",
            "cost 3.944677910076989\n",
            "iter 42\n",
            "cost 4.0316277452529805\n",
            "iter 43\n",
            "cost 4.119204517475856\n",
            "iter 44\n",
            "cost 4.207170853874056\n",
            "iter 45\n",
            "cost 4.295299060679012\n",
            "iter 46\n",
            "cost 4.383372394798621\n",
            "iter 47\n",
            "cost 4.471186173422414\n",
            "iter 48\n",
            "cost 4.5585486959847445\n",
            "iter 49\n",
            "cost 4.645281962454371\n",
            "iter 50\n",
            "cost 4.731222180978075\n",
            "iter 51\n",
            "cost 4.816220066044861\n",
            "iter 52\n",
            "cost 4.90014093530927\n",
            "iter 53\n",
            "cost 4.982864618866992\n",
            "iter 54\n",
            "cost 5.06428519905755\n",
            "iter 55\n",
            "cost 5.144310601804267\n",
            "iter 56\n",
            "cost 5.222862062185585\n",
            "iter 57\n",
            "cost 5.299873487507807\n",
            "iter 58\n",
            "cost 5.375290740792048\n",
            "iter 59\n",
            "cost 5.4490708664859255\n",
            "iter 60\n",
            "cost 5.5211812785514685\n",
            "iter 61\n",
            "cost 5.591598929041217\n",
            "iter 62\n",
            "cost 5.660309473012245\n",
            "iter 63\n",
            "cost 5.727306443276407\n",
            "iter 64\n",
            "cost 5.792590446152904\n",
            "iter 65\n",
            "cost 5.856168387159225\n",
            "iter 66\n",
            "cost 5.918052733508154\n",
            "iter 67\n",
            "cost 5.978260818410421\n",
            "iter 68\n",
            "cost 6.03681419053633\n",
            "iter 69\n",
            "cost 6.093738010571823\n",
            "iter 70\n",
            "cost 6.149060495611897\n",
            "iter 71\n",
            "cost 6.202812411155008\n",
            "iter 72\n",
            "cost 6.2550266096794775\n",
            "iter 73\n",
            "cost 6.30573761417688\n",
            "iter 74\n",
            "cost 6.354981244566446\n",
            "iter 75\n",
            "cost 6.402794284597121\n",
            "iter 76\n",
            "cost 6.449214186638962\n",
            "iter 77\n",
            "cost 6.494278811653854\n",
            "iter 78\n",
            "cost 6.538026201599095\n",
            "iter 79\n",
            "cost 6.580494381540605\n",
            "iter 80\n",
            "cost 6.621721188821621\n",
            "iter 81\n",
            "cost 6.661744126736141\n",
            "iter 82\n",
            "cost 6.700600240283875\n",
            "iter 83\n",
            "cost 6.738326011727416\n",
            "iter 84\n",
            "cost 6.774957273825552\n",
            "iter 85\n",
            "cost 6.810529138774346\n",
            "iter 86\n",
            "cost 6.845075941045293\n",
            "iter 87\n",
            "cost 6.878631192464674\n",
            "iter 88\n",
            "cost 6.911227548027488\n",
            "iter 89\n",
            "cost 6.942896781081896\n",
            "iter 90\n",
            "cost 6.973669766654323\n",
            "iter 91\n",
            "cost 7.003576471810924\n",
            "iter 92\n",
            "cost 7.032645952067633\n",
            "iter 93\n",
            "cost 7.060906352968292\n",
            "iter 94\n",
            "cost 7.088384916048787\n",
            "iter 95\n",
            "cost 7.11510798849466\n",
            "iter 96\n",
            "cost 7.141101035881188\n",
            "iter 97\n",
            "cost 7.166388657458266\n",
            "iter 98\n",
            "cost 7.190994603508848\n",
            "iter 99\n",
            "cost 7.214941794368913\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}